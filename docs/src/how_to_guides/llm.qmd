```@meta
CurrentModule = ConformalPrediction
```

```{julia}
#| echo: false
include("$(pwd())/docs/setup_docs.jl")
eval(setup_docs)
```

# How to Conformalize a Language Model

## Data

```{julia}
# Get labels:
df_labels = CSV.read("dev/artifacts/data/banking77/labels.csv", DataFrame, drop=[1])
labels = df_labels[:,1]

# Get data:
df_train = CSV.read("dev/artifacts/data/banking77/train.csv", DataFrame, drop=[1])
df_cal = CSV.read("dev/artifacts/data/banking77/calibration.csv", DataFrame, drop=[1])
df_test = CSV.read("dev/artifacts/data/banking77/test.csv", DataFrame, drop=[1])

# Preprocess data:
queries_train, y_train = collect(df_train.text), categorical(df_train.labels .+ 1)
queries_cal, y_cal = collect(df_cal.text), categorical(df_cal.labels .+ 1)
queries_test, y_test = collect(df_test.text), categorical(df_test.labels .+ 1)
```

## HuggingFace Model

```{julia}
tkr = hgf"mrm8488/distilroberta-finetuned-banking77:tokenizer"
mod = hgf"mrm8488/distilroberta-finetuned-banking77:ForSequenceClassification"
```


```{julia}
query = [
    "What is the base of the exchange rates?",
    "Exchange rates for the US dollar.",
]
a = encode(tkr, query)
b = mod.model(a)
c = mod.cls(b.hidden_state)
d = softmax(c.logit)
Flux.onecold(d)
```

## `MLJ` Model

```{julia}
struct IntentClassifier <: MLJBase.Probabilistic
    tkr::TextEncoders.AbstractTransformerTextEncoder
    mod::HuggingFace.HGFRobertaForSequenceClassification
end

function IntentClassifier(;
    tokenizer::TextEncoders.AbstractTransformerTextEncoder, 
    model::HuggingFace.HGFRobertaForSequenceClassification,
)
    IntentClassifier(tkr, mod)
end

function get_hidden_state(clf::IntentClassifier, query::Union{AbstractString, Vector{<:AbstractString}})
    token = encode(clf.tkr, query)
    hidden_state = clf.mod.model(token).hidden_state
    return hidden_state
end

# This doesn't actually retrain the model, but it retrieves the classifier object
function MLJBase.fit(clf::IntentClassifier, verbosity, X, y)
    cache=nothing
    report=nothing
    fitresult = (clf = clf.mod.cls, labels = levels(y))
    return fitresult, cache, report
end

function MLJBase.predict(clf::IntentClassifier, fitresult, Xnew)
    output = fitresult.clf(Xnew)
    p̂ = permutedims(softmax(output.logit))
    p̂ = UnivariateFinite(fitresult.labels,p̂,pool=missing)
    return p̂
end
```


```{julia}
clf = IntentClassifier(tkr, mod)
X_train = get_hidden_state(clf, queries_train)
fitresult, _, _ = fit(clf, 1, nothing, y_train)
p̂ = predict(clf, fitresult, X_train)
```