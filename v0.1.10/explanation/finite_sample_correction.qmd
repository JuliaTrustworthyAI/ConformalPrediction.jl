# Finite-sample Correction

```{julia}
#| echo: false
include("$(pwd())/docs/setup_docs.jl")
eval(setup_docs)
```

We follow the convention used in @angelopoulos2021gentle and @barber2021predictive to correct for the finite-sample bias of the empirical quantile. Specifically, we use the following definition of the $(1-\alpha)$ empirical quantile:

```math
\hat{q}_{n,\alpha}^{+}\{v\} = \frac{\lceil (n+1)(1-\alpha)\rceil}{n}
```

@barber2021predictive further define as the $\alpha$ empirical quantile:

```math
\hat{q}_{n,\alpha}^{-}\{v\} = \frac{\lfloor (n+1)\alpha \rfloor}{n} = - \hat{q}_{n,\alpha}^{+}\{-v\}
```

Below we test this equality numerically by generating a large number of random vectors and comparing the two quantiles. We then plot the density of the difference between the two quantiles. While the errors are small, they are not negligible for small $n$. In our computations, we use $\hat{q}_{n,\alpha}^{-}\{v\}$ exactly as it is defined above, rather than relying on $- \hat{q}_{n,\alpha}^{+}\{-v\}$.

```{julia}
#| output: true

using ConformalPrediction: qplus, qminus
nobs = [100, 1000, 10000]
n = 1000
alpha = 0.1
plts = []
Δ = Float32[]
for _nobs in nobs
    for i in 1:n
        v = rand(_nobs)
        δ = qminus(v, alpha) - (-qplus(-v, 1-alpha))
        push!(Δ, δ)
    end
    plt = density(Δ)
    vline!([mean(Δ)], color=:red, label="mean")
    push!(plts, plt)
end
plot(plts..., layout=(1,3), size=(900, 300), legend=:topleft, title=["nobs = 100" "nobs = 1000" "nobs = 10000"])
```

See also this related [discussion](https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl/discussions/17).

## References