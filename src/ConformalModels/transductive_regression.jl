using MLJ
using MLJBase

"A base type for Transductive Conformal Regressors."
abstract type TransductiveConformalRegressor <: TransductiveConformalModel end

# Naive
"""
The `NaiveRegressor` for conformal prediction is the simplest approach to conformal regression.
"""
mutable struct NaiveRegressor{Model <: Supervised} <: TransductiveConformalRegressor
    model::Model
    coverage::AbstractFloat
    scores::Union{Nothing,AbstractArray}
    heuristic::Function
end

function NaiveRegressor(model::Supervised; coverage::AbstractFloat=0.95, heuristic::Function=f(y,yÌ‚)=abs(y-yÌ‚))
    return NaiveRegressor(model, coverage, nothing, heuristic)
end

@doc raw"""
    MMI.fit(conf_model::NaiveRegressor, verbosity, X, y)

Wrapper function to fit the underlying MLJ model. For Inductive Conformal Prediction the underlying model is fitted on the *proper training set*. The `fitresult` is assigned to the model instance. Computation of nonconformity scores requires a separate calibration step involving a *calibration data set* (see [`calibrate!`](@ref)). 
"""
function MMI.fit(conf_model::NaiveRegressor, verbosity, X, y)
    
    # Training: 
    fitresult, cache, report = MMI.fit(conf_model.model, verbosity, MMI.reformat(conf_model.model, X, y)...)

    # Nonconformity Scores:
    yÌ‚ = MMI.predict(conf_model.model, fitresult, X)
    conf_model.scores = @.(conf_model.heuristic(y, yÌ‚))

    return (fitresult, cache, report)

end

# Prediction
@doc raw"""
    MMI.predict(conf_model::NaiveRegressor, fitresult, Xnew)

For the [`NaiveRegressor`](@ref) prediction intervals are computed as follows:

``
\begin{aligned}
\hat{C}_{n,\alpha}(X_{n+1}) &= \hat\mu(X_{n+1}) \pm \hat{q}_{n, \alpha}^{+} \{|Y_i - \hat\mu(X_i)| \}, \ i \in \mathcal{D}_{\text{train}}
\end{aligned}
``

The naive approach typically produces prediction regions that undercover due to overfitting.
"""
function MMI.predict(conf_model::NaiveRegressor, fitresult, Xnew)
    yÌ‚ = MMI.predict(conf_model.model, fitresult, MMI.reformat(conf_model.model, Xnew)...)
    v = conf_model.scores
    qÌ‚ = qplus(v, conf_model)
    yÌ‚ = map(x -> (x .- qÌ‚, x .+ qÌ‚), eachrow(yÌ‚))
    return yÌ‚
end

# Jackknife
"Constructor for `JackknifeRegressor`."
mutable struct JackknifeRegressor{Model <: Supervised} <: TransductiveConformalRegressor
    model::Model
    coverage::AbstractFloat
    scores::Union{Nothing,AbstractArray}
    heuristic::Function
end

function JackknifeRegressor(model::Supervised; coverage::AbstractFloat=0.95, heuristic::Function=f(y,yÌ‚)=abs(y-yÌ‚))
    return JackknifeRegressor(model, coverage, nothing, heuristic)
end

@doc raw"""
    MMI.fit(conf_model::JackknifeRegressor, verbosity, X, y)

Wrapper function to fit the underlying MLJ model. For Inductive Conformal Prediction the underlying model is fitted on the *proper training set*. The `fitresult` is assigned to the model instance. Computation of nonconformity scores requires a separate calibration step involving a *calibration data set* (see [`calibrate!`](@ref)). 
"""
function MMI.fit(conf_model::JackknifeRegressor, verbosity, X, y)
    
    # Training: 
    fitresult, cache, report = MMI.fit(conf_model.model, verbosity, MMI.reformat(conf_model.model, X, y)...)

    # Nonconformity Scores:
    T = size(y, 1)
    scores = []
    for t in 1:T
        loo_ids = 1:T .!= t
        yâ‚‹áµ¢ = y[loo_ids]                
        Xâ‚‹áµ¢ = MLJ.matrix(X)[loo_ids,:]
        yáµ¢ = y[t]
        Xáµ¢ = selectrows(X, t)
        Î¼Ì‚â‚‹áµ¢, = MMI.fit(conf_model.model, 0, MMI.reformat(conf_model.model, Xâ‚‹áµ¢, yâ‚‹áµ¢)...)
        yÌ‚áµ¢ = MMI.predict(conf_model.model, Î¼Ì‚â‚‹áµ¢, Xáµ¢)
        push!(scores,@.(conf_model.heuristic(yáµ¢, yÌ‚áµ¢))...)
    end
    conf_model.scores = scores

    return (fitresult, cache, report)
end

# Prediction
@doc raw"""
    MMI.predict(conf_model::JackknifeRegressor, fitresult, Xnew)

For the [`JackknifeRegressor`](@ref) prediction intervals are computed as follows,

``
\begin{aligned}
\hat{C}_{n,\alpha}(X_{n+1}) &= \hat\mu(X_{n+1}) \pm \hat{q}_{n, \alpha}^{+} \{|Y_i - \hat\mu_{-i}(X_i)|\}, \ i \in \mathcal{D}_{\text{train}}
\end{aligned}
``

where ``\hat\mu_{-i}`` denotes the model fitted on training data with ``i``th point removed. The jackknife procedure addresses the overfitting issue associated with the [`NaiveRegressor`](@ref).
"""
function MMI.predict(conf_model::JackknifeRegressor, fitresult, Xnew)
    yÌ‚ = MMI.predict(conf_model.model, fitresult, MMI.reformat(conf_model.model, Xnew)...)
    v = conf_model.scores
    qÌ‚ = qplus(v, conf_model)
    yÌ‚ = map(x -> (x .- qÌ‚, x .+ qÌ‚), eachrow(yÌ‚))
    return yÌ‚
end

# Jackknife+
"Constructor for `JackknifePlusRegressor`."
mutable struct JackknifePlusRegressor{Model <: Supervised} <: TransductiveConformalRegressor
    model::Model
    coverage::AbstractFloat
    scores::Union{Nothing,AbstractArray}
    heuristic::Function
end

function JackknifePlusRegressor(model::Supervised; coverage::AbstractFloat=0.95, heuristic::Function=f(y,yÌ‚)=abs(y-yÌ‚))
    return JackknifePlusRegressor(model, coverage, nothing, heuristic)
end

@doc raw"""
    MMI.fit(conf_model::JackknifeRegressor, verbosity, X, y)

Wrapper function to fit the underlying MLJ model. For Inductive Conformal Prediction the underlying model is fitted on the *proper training set*. The `fitresult` is assigned to the model instance. Computation of nonconformity scores requires a separate calibration step involving a *calibration data set* (see [`calibrate!`](@ref)). 
"""
function MMI.fit(conf_model::JackknifePlusRegressor, verbosity, X, y)
    
    # Training: 
    fitresult, cache, report = ([],[],[])

    # Nonconformity Scores:
    T = size(y, 1)
    scores = []
    for t in 1:T
        loo_ids = 1:T .!= t
        yâ‚‹áµ¢ = y[loo_ids]                
        Xâ‚‹áµ¢ = MLJ.matrix(X)[loo_ids,:]
        yáµ¢ = y[t]
        Xáµ¢ = selectrows(X, t)
        # Store LOO fitresult:
        Î¼Ì‚â‚‹áµ¢, cacheâ‚‹áµ¢, reportâ‚‹áµ¢ = MMI.fit(conf_model.model, 0, MMI.reformat(conf_model.model, Xâ‚‹áµ¢, yâ‚‹áµ¢)...)
        push!(fitresult, Î¼Ì‚â‚‹áµ¢)
        push!(cache, cacheâ‚‹áµ¢)
        push!(report, reportâ‚‹áµ¢)
        # Store LOO score:
        yÌ‚áµ¢ = MMI.predict(conf_model.model, Î¼Ì‚â‚‹áµ¢, Xáµ¢)
        push!(scores,@.(conf_model.heuristic(yáµ¢, yÌ‚áµ¢))...)
    end
    conf_model.scores = scores

    return (fitresult, cache, report)
end

# Prediction
@doc raw"""
    MMI.predict(conf_model::JJackknifePlusRegressor, fitresult, Xnew)

For the [`JackknifePlusRegressor`](@ref) prediction intervals are computed as follows,

``
\begin{aligned}
\hat{C}_{n,\alpha}(X_{n+1}) &= \left[ \hat{q}_{n, \alpha}^{-} \{\hat\mu_{-i}(X_{N+1}) - R_i^{\text{LOO}} \}, \hat{q}_{n, \alpha}^{+} \{\hat\mu_{-i}(X_{N+1}) + R_i^{\text{LOO}}\} \right] , & i \in \mathcal{D}_{\text{train}} \\
R_i^{\text{LOO}}&=|Y_i - \hat\mu_{-i}(X_i)|, & i \in \mathcal{D}_{\text{train}}
\end{aligned}
``

where ``\hat\mu_{-i}`` denotes the model fitted on training data with ``i``th point removed. The jackknife``+`` procedure is more stable than the [`JackknifeRegressor`](@ref).
"""
function MMI.predict(conf_model::JackknifePlusRegressor, fitresult, Xnew)
    # Get all LOO predictions for each Xnew:
    yÌ‚ = [MMI.predict(conf_model.model, Î¼Ì‚â‚‹áµ¢, MMI.reformat(conf_model.model, Xnew)...) for Î¼Ì‚â‚‹áµ¢ in fitresult] 
    # All LOO predictions across columns for each Xnew across rows:
    yÌ‚ = reduce(hcat, yÌ‚)
    # For each Xnew compute ( qÌ‚â»(Î¼Ì‚â‚‹áµ¢(xnew)-Ráµ¢á´¸á´¼á´¼) , qÌ‚âº(Î¼Ì‚â‚‹áµ¢(xnew)+Ráµ¢á´¸á´¼á´¼) ):
    yÌ‚ = map(yáµ¢ -> (-qplus(-yáµ¢ .+ conf_model.scores, conf_model), qplus(yáµ¢ .+ conf_model.scores, conf_model)), eachrow(yÌ‚))
    return yÌ‚
end


# Jackknife-minmax
"Constructor for `JackknifeMinMaxRegressor`."
mutable struct JackknifeMinMaxRegressor{Model <: Supervised} <: TransductiveConformalRegressor
    model::Model
    coverage::AbstractFloat
    scores::Union{Nothing,AbstractArray}
    heuristic::Function
end

function JackknifeMinMaxRegressor(model::Supervised; coverage::AbstractFloat=0.95, heuristic::Function=f(y,yÌ‚)=abs(y-yÌ‚))
    return JackknifeMinMaxRegressor(model, coverage, nothing, heuristic)
end

@doc raw"""
    MMI.fit(conf_model::JackknifeRegressor, verbosity, X, y)

Wrapper function to fit the underlying MLJ model. For Inductive Conformal Prediction the underlying model is fitted on the *proper training set*. The `fitresult` is assigned to the model instance. Computation of nonconformity scores requires a separate calibration step involving a *calibration data set* (see [`calibrate!`](@ref)). 
"""
function MMI.fit(conf_model::JackknifeMinMaxRegressor, verbosity, X, y)
    
    # Pre-allocate: 
    fitresult, cache, report = ([],[],[])

    # Training and Nonconformity Scores:
    T = size(y, 1)
    scores = []
    for t in 1:T
        loo_ids = 1:T .!= t
        yâ‚‹áµ¢ = y[loo_ids]                
        Xâ‚‹áµ¢ = MLJ.matrix(X)[loo_ids,:]
        yáµ¢ = y[t]
        Xáµ¢ = selectrows(X, t)
        # Store LOO fitresult:
        Î¼Ì‚â‚‹áµ¢, cacheâ‚‹áµ¢, reportâ‚‹áµ¢ = MMI.fit(conf_model.model, 0, MMI.reformat(conf_model.model, Xâ‚‹áµ¢, yâ‚‹áµ¢)...)
        push!(fitresult, Î¼Ì‚â‚‹áµ¢)
        push!(cache, cacheâ‚‹áµ¢)
        push!(report, reportâ‚‹áµ¢)
        # Store LOO score:
        yÌ‚áµ¢ = MMI.predict(conf_model.model, Î¼Ì‚â‚‹áµ¢, Xáµ¢)
        push!(scores,@.(conf_model.heuristic(yáµ¢, yÌ‚áµ¢))...)
    end
    conf_model.scores = scores

    return (fitresult, cache, report)
end

# Prediction
@doc raw"""
    MMI.predict(conf_model::JackknifeMinMaxRegressor, fitresult, Xnew)

For the [`JackknifeMinMaxRegressor`](@ref) prediction intervals are computed as follows,

``
\begin{aligned}
\hat{C}_{n,\alpha}(X_{n+1}) &= \left[ \hat{q}_{n, \alpha}^{-} \{\hat\mu_{-i}(X_{N+1}) - R_i^{\text{LOO}} \}, \hat{q}_{n, \alpha}^{+} \{\hat\mu_{-i}(X_{N+1}) + R_i^{\text{LOO}}\} \right] , & i \in \mathcal{D}_{\text{train}} \\
R_i^{\text{LOO}}&=|Y_i - \hat\mu_{-i}(X_i)|, & i \in \mathcal{D}_{\text{train}}
\end{aligned}
``

where ``\hat\mu_{-i}`` denotes the model fitted on training data with ``i``th point removed. The jackknife``+`` procedure is more stable than the [`JackknifeRegressor`](@ref).
"""
function MMI.predict(conf_model::JackknifeMinMaxRegressor, fitresult, Xnew)
    # Get all LOO predictions for each Xnew:
    yÌ‚ = [MMI.predict(conf_model.model, Î¼Ì‚â‚‹áµ¢, MMI.reformat(conf_model.model, Xnew)...) for Î¼Ì‚â‚‹áµ¢ in fitresult] 
    # All LOO predictions across columns for each Xnew across rows:
    yÌ‚ = reduce(hcat, yÌ‚)
    # Get all LOO residuals:
    v = conf_model.scores
    qÌ‚ = qplus(v, conf_model)
    # For each Xnew compute ( qÌ‚â»(Î¼Ì‚â‚‹áµ¢(xnew)-Ráµ¢á´¸á´¼á´¼) , qÌ‚âº(Î¼Ì‚â‚‹áµ¢(xnew)+Ráµ¢á´¸á´¼á´¼) ):
    yÌ‚ = map(yáµ¢ -> (minimum(yáµ¢ .- qÌ‚), maximum(yáµ¢ .+ qÌ‚)), eachrow(yÌ‚))
    return yÌ‚
end

# CV+
"Constructor for `CVPlusRegressor`."
mutable struct CVPlusRegressor{Model <: Supervised} <: TransductiveConformalRegressor
    model::Model
    coverage::AbstractFloat
    scores::Union{Nothing,AbstractArray}
    heuristic::Function
    cv::MLJ.CV
end

function CVPlusRegressor(
    model::Supervised; 
    coverage::AbstractFloat=0.95, heuristic::Function=f(y,yÌ‚)=abs(y-yÌ‚), cv::MLJ.CV=MLJ.CV()
)
    return CVPlusRegressor(model, coverage, nothing, heuristic, cv)
end

@doc raw"""
    MMI.fit(conf_model::CVPlusRegressor, verbosity, X, y)

Wrapper function to fit the underlying MLJ model. For Inductive Conformal Prediction the underlying model is fitted on the *proper training set*. The `fitresult` is assigned to the model instance. Computation of nonconformity scores requires a separate calibration step involving a *calibration data set* (see [`calibrate!`](@ref)). 
"""
function MMI.fit(conf_model::CVPlusRegressor, verbosity, X, y)

    # ğ¾-fold training:
    T = size(y, 1)
    cv_indices = MLJBase.train_test_pairs(conf_model.cv, 1:T)
    cv_fitted = map(cv_indices) do (train, test)
        ytrain = y[train]                
        Xtrain = MLJ.matrix(X)[train,:]
        Î¼Ì‚â‚–, cache, report = MMI.fit(conf_model.model, 0, MMI.reformat(conf_model.model, Xtrain, ytrain)...)
        Dict(:fitresult => Î¼Ì‚â‚–, :test => test, :cache => cache, :report => report)
    end

    # Pre-allocate: 
    fitresult, cache, report = ([],[],[])

    # Nonconformity Scores:
    scores = []
    for t in 1:T
        yáµ¢ = y[t]
        Xáµ¢ = selectrows(X, t)
        resultsáµ¢ = [(x[:fitresult], x[:cache], x[:report]) for x in cv_fitted if t in x[:test]]
        @assert length(resultsáµ¢) == 1 "Expected each individual to be contained in only one subset."
        Î¼Ì‚áµ¢, cacheáµ¢, reportáµ¢ = resultsáµ¢[1]
        # Store individual CV fitresults
        push!(fitresult, Î¼Ì‚áµ¢)
        push!(cache, cacheáµ¢)
        push!(report, reportáµ¢)
        # Store LOO score:
        yÌ‚áµ¢ = MMI.predict(conf_model.model, Î¼Ì‚áµ¢, Xáµ¢)
        push!(scores,@.(conf_model.heuristic(yáµ¢, yÌ‚áµ¢))...)
    end
    conf_model.scores = scores

    return (fitresult, cache, report)
end

# Prediction
@doc raw"""
    MMI.predict(conf_model::CVPlusRegressor, fitresult, Xnew)

For the [`CVPlusRegressor`](@ref) prediction intervals are computed as follows,

``
\begin{aligned}
\hat{C}_{n,\alpha}(X_{n+1}) &= \left[ \hat{q}_{n, \alpha}^{-} \{\hat\mu_{-i}(X_{N+1}) - R_i^{\text{LOO}} \}, \hat{q}_{n, \alpha}^{+} \{\hat\mu_{-i}(X_{N+1}) + R_i^{\text{LOO}}\} \right] , & i \in \mathcal{D}_{\text{train}} \\
R_i^{\text{LOO}}&=|Y_i - \hat\mu_{-i}(X_i)|, & i \in \mathcal{D}_{\text{train}}
\end{aligned}
``

where ``\hat\mu_{-i}`` denotes the model fitted on training data with ``i``th point removed. The jackknife``+`` procedure is more stable than the [`JackknifeRegressor`](@ref).
"""
function MMI.predict(conf_model::CVPlusRegressor, fitresult, Xnew)
    # Get all LOO predictions for each Xnew:
    yÌ‚ = [MMI.predict(conf_model.model, Î¼Ì‚â‚‹áµ¢, MMI.reformat(conf_model.model, Xnew)...) for Î¼Ì‚â‚‹áµ¢ in fitresult] 
    # All LOO predictions across columns for each Xnew across rows:
    yÌ‚ = reduce(hcat, yÌ‚)
    # For each Xnew compute ( qÌ‚â»(Î¼Ì‚â‚‹áµ¢(xnew)-Ráµ¢á´¸á´¼á´¼) , qÌ‚âº(Î¼Ì‚â‚‹áµ¢(xnew)+Ráµ¢á´¸á´¼á´¼) ):
    yÌ‚ = map(yáµ¢ -> (-qplus(-yáµ¢ .+ conf_model.scores, conf_model), qplus(yáµ¢ .+ conf_model.scores, conf_model)), eachrow(yÌ‚))
    return yÌ‚
end


# CV MinMax
"Constructor for `CVMinMaxRegressor`."
mutable struct CVMinMaxRegressor{Model <: Supervised} <: TransductiveConformalRegressor
    model::Model
    coverage::AbstractFloat
    scores::Union{Nothing,AbstractArray}
    heuristic::Function
    cv::MLJ.CV
end

function CVMinMaxRegressor(
    model::Supervised; 
    coverage::AbstractFloat=0.95, heuristic::Function=f(y,yÌ‚)=abs(y-yÌ‚), cv::MLJ.CV=MLJ.CV()
)
    return CVMinMaxRegressor(model, coverage, nothing, heuristic, cv)
end

@doc raw"""
    MMI.fit(conf_model::CVMinMaxRegressor, verbosity, X, y)

Wrapper function to fit the underlying MLJ model. For Inductive Conformal Prediction the underlying model is fitted on the *proper training set*. The `fitresult` is assigned to the model instance. Computation of nonconformity scores requires a separate calibration step involving a *calibration data set* (see [`calibrate!`](@ref)). 
"""
function MMI.fit(conf_model::CVMinMaxRegressor, verbosity, X, y)

    # ğ¾-fold training:
    T = size(y, 1)
    cv_indices = MLJBase.train_test_pairs(conf_model.cv, 1:T)
    cv_fitted = map(cv_indices) do (train, test)
        ytrain = y[train]                
        Xtrain = MLJ.matrix(X)[train,:]
        Î¼Ì‚â‚–, cache, report = MMI.fit(conf_model.model, 0, MMI.reformat(conf_model.model, Xtrain, ytrain)...)
        Dict(:fitresult => Î¼Ì‚â‚–, :test => test, :cache => cache, :report => report)
    end

    # Pre-allocate: 
    fitresult, cache, report = ([],[],[])

    # Nonconformity Scores:
    scores = []
    for t in 1:T
        yáµ¢ = y[t]
        Xáµ¢ = selectrows(X, t)
        resultsáµ¢ = [(x[:fitresult], x[:cache], x[:report]) for x in cv_fitted if t in x[:test]]
        @assert length(resultsáµ¢) == 1 "Expected each individual to be contained in only one subset."
        Î¼Ì‚áµ¢, cacheáµ¢, reportáµ¢ = resultsáµ¢[1]
        # Store individual CV fitresults
        push!(fitresult, Î¼Ì‚áµ¢)
        push!(cache, cacheáµ¢)
        push!(report, reportáµ¢)
        # Store LOO score:
        yÌ‚áµ¢ = MMI.predict(conf_model.model, Î¼Ì‚áµ¢, Xáµ¢)
        push!(scores,@.(conf_model.heuristic(yáµ¢, yÌ‚áµ¢))...)
    end
    conf_model.scores = scores

    return (fitresult, cache, report)
end


# Prediction
@doc raw"""
    MMI.predict(conf_model::CVMinMaxRegressor, fitresult, Xnew)

For the [`CVMinMaxRegressor`](@ref) prediction intervals are computed as follows,

``
\begin{aligned}
\hat{C}_{n,\alpha}(X_{n+1}) &= \left[ \hat{q}_{n, \alpha}^{-} \{\hat\mu_{-i}(X_{N+1}) - R_i^{\text{LOO}} \}, \hat{q}_{n, \alpha}^{+} \{\hat\mu_{-i}(X_{N+1}) + R_i^{\text{LOO}}\} \right] , & i \in \mathcal{D}_{\text{train}} \\
R_i^{\text{LOO}}&=|Y_i - \hat\mu_{-i}(X_i)|, & i \in \mathcal{D}_{\text{train}}
\end{aligned}
``

where ``\hat\mu_{-i}`` denotes the model fitted on training data with ``i``th point removed. The jackknife``+`` procedure is more stable than the [`JackknifeRegressor`](@ref).
"""
function MMI.predict(conf_model::CVMinMaxRegressor, fitresult, Xnew)
    # Get all LOO predictions for each Xnew:
    yÌ‚ = [MMI.predict(conf_model.model, Î¼Ì‚â‚‹áµ¢, MMI.reformat(conf_model.model, Xnew)...) for Î¼Ì‚â‚‹áµ¢ in fitresult] 
    # All LOO predictions across columns for each Xnew across rows:
    yÌ‚ = reduce(hcat, yÌ‚)
    # Get all LOO residuals:
    v = conf_model.scores
    qÌ‚ = qplus(v, conf_model)
    # For each Xnew compute ( qÌ‚â»(Î¼Ì‚â‚‹áµ¢(xnew)-Ráµ¢á´¸á´¼á´¼) , qÌ‚âº(Î¼Ì‚â‚‹áµ¢(xnew)+Ráµ¢á´¸á´¼á´¼) ):
    yÌ‚ = map(yáµ¢ -> (minimum(yáµ¢ .- qÌ‚), maximum(yáµ¢ .+ qÌ‚)), eachrow(yÌ‚))
    return yÌ‚
end