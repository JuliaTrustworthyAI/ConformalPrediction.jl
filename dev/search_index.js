var documenterSearchIndex = {"docs":
[{"location":"how_to_guides/#How-To-Guides","page":"Overview","title":"How-To Guides","text":"In this section you will find a series of how-to-guides that showcase specific use cases of Conformal Prediction.\n\nHow-to guides are directions that take the reader through the steps required to solve a real-world problem. How-to guides are goal-oriented.‚Äî Di√°taxis\n\nIn other words, you come here because you may have some particular problem in mind, would like to see how it can be solved using CP and then most likely head off again ü´°","category":"section"},{"location":"tutorials/regression/#Regression","page":"Regression","title":"Regression","text":"This tutorial presents and compares different approaches to Conformal Regression using a simple synthetic dataset. It is inspired by this MAPIE tutorial.","category":"section"},{"location":"tutorials/regression/#Data","page":"Regression","title":"Data","text":"We begin by generating some synthetic regression data below:\n\n# Regression data:\n\n# Inputs:\nN = 600\nxmax = 5.0\nusing Distributions\nd = Uniform(-xmax, xmax)\nX = rand(d, N)\nX = reshape(X, :, 1)\n\n# Outputs:\nnoise = 0.5\nfun(X) = X * sin(X)\nŒµ = randn(N) .* noise\ny = @.(fun(X)) + Œµ\ny = vec(y)\n\n# Partition:\nusing MLJ\ntrain, test = partition(eachindex(y), 0.4, 0.4, shuffle=true)\n\nusing Plots\nscatter(X, y, label=\"Observed\")\nxrange = range(-xmax,xmax,length=N)\nplot!(xrange, @.(fun(xrange)), lw=4, label=\"Ground truth\", ls=:dash, colour=:black)","category":"section"},{"location":"tutorials/regression/#Model","page":"Regression","title":"Model","text":"To model this data we will use polynomial regression. There is currently no out-of-the-box support for polynomial feature transformations in MLJ, but it is easy enough to add a little helper function for this. Note how we define a linear pipeline pipe here. Since pipelines in MLJ are just models, we can use the generated object as an input to conformal_model below.\n\nLinearRegressor = @load LinearRegressor pkg=MLJLinearModels\ndegree_polynomial = 10\npolynomial_features(X, degree::Int) = reduce(hcat, map(i -> X.^i, 1:degree))\npipe = (X -> MLJ.table(polynomial_features(MLJ.matrix(X), degree_polynomial))) |> LinearRegressor()","category":"section"},{"location":"tutorials/regression/#Conformal-Prediction","page":"Regression","title":"Conformal Prediction","text":"Next, we conformalize our polynomial regressor using every available approach (except the Naive approach):\n\nusing ConformalPrediction\nconformal_models = merge(values(available_models[:regression])...)\nresults = Dict()\nfor _mod in keys(conformal_models) \n    conf_model = conformal_model(pipe; method=_mod, coverage=0.95)\n    global mach = machine(conf_model, X, y)\n    MLJ.fit!(mach, rows=train)\n    results[_mod] = mach\nend\n\nFinally, let us look at the resulting conformal predictions in each case. The chart below shows the results: for the first 4 methods it displays the training data (dots) overlaid with the conformal prediction interval (shaded area). At first glance it is hard to spot any major differences between the different approaches. Next, we will look at how we can evaluate and benchmark these predictions.\n\nusing Plots\nzoom = -0.5\nxrange = range(-xmax+zoom,xmax-zoom,length=N)\nplt_list = []\n\nfor (_mod, mach) in first(results, n_charts)\n    plt = plot(mach.model, mach.fitresult, X, y, zoom=zoom, title=_mod)\n    plot!(plt, xrange, @.(fun(xrange)), lw=1, ls=:dash, colour=:black, label=\"Ground truth\")\n    push!(plt_list, plt)\nend\n\nplot(plt_list..., size=(800,500))\n\n(Image: Figure¬†1: Conformal prediction regions.)","category":"section"},{"location":"tutorials/regression/#Evaluation","page":"Regression","title":"Evaluation","text":"For evaluation of conformal predictors we follow Angelopoulos and Bates (2021) (Section 3). As a first step towards adaptiveness (adaptivity), the authors recommend to inspect the set size of conformal predictions. The chart below shows the interval width for the different methods along with the ground truth interval width:\n\nxrange = range(-xmax,xmax,length=N)\nplt = plot(xrange, ones(N) .* (1.96*2*noise), ls=:dash, colour=:black, label=\"Ground truth\", lw=2)\nfor (_mod, mach) in results\n    yÃÇ = predict(mach, reshape([x for x in xrange], :, 1))\n    y_size = set_size.(yÃÇ)\n    plot!(xrange, y_size, label=String(_mod))\nend\nplt\n\n(Image: Figure¬†2: Prediction interval width.)\n\nWe can also use specific metrics like empirical coverage and size-stratified coverage to check for correctness and adaptiveness, respectively (angelopoulus2021gentle?). To this end, the package provides custom measures that are compatible with MLJ.jl. In other words, we can evaluate model performance in true MLJ.jl fashion (see here).\n\nThe code below runs the evaluation with respect to both metrics, emp_coverage and ssc for a single conformal machine:\n\n_mod, mach = first(results)\n_eval = evaluate!(\n    mach,\n    operation=predict,\n    measure=[emp_coverage, ssc]\n)\ndisplay(_eval)\nprintln(\"Empirical coverage for $(_mod): $(round(_eval.measurement[1], digits=3))\")\nprintln(\"SSC for $(_mod): $(round(_eval.measurement[2], digits=3))\")\n\nPerformanceEvaluation object with these fields:\n  measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows\nExtract:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚îÇ measure                                      ‚îÇ operation ‚îÇ measurement ‚îÇ 1.9 ‚ãØ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚îÇ ConformalPrediction.emp_coverage             ‚îÇ predict   ‚îÇ 0.94        ‚îÇ 0.0 ‚ãØ\n‚îÇ ConformalPrediction.size_stratified_coverage ‚îÇ predict   ‚îÇ 0.94        ‚îÇ 0.0 ‚ãØ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                                                               2 columns omitted\n\nEmpirical coverage for jackknife_plus_ab: 0.94\nSSC for jackknife_plus_ab: 0.94\n\nNote that, in the regression case, stratified set sizes correspond to discretized interval widths.\n\nTo benchmark the different approaches, we evaluate them iteratively below. As expected, more conservative approaches like Jackknife-min‚ÄÜmax‚ÄÜ and CV-min‚ÄÜmax‚ÄÜ attain higher aggregate and conditional coverage. Note that size-stratified is not available for methods that produce constant intervals, like standard Jackknife.\n\nusing DataFrames\nbmk = DataFrame()\nfor (_mod, mach) in results\n    _eval = evaluate!(\n        mach,\n        resampling=CV(;nfolds=5),\n        operation=predict,\n        measure=[emp_coverage, ssc]\n    )\n    _bmk = DataFrame(\n        Dict(\n            :model => _mod,\n            :emp_coverage => _eval.measurement[1],\n            :ssc => _eval.measurement[2]\n        )\n    )\n    bmk = vcat(bmk, _bmk)\nend\n\nshow(sort(select!(bmk, [2,1,3]), 2, rev=true))\n\n9√ó3 DataFrame\n Row ‚îÇ model                     emp_coverage  ssc      \n     ‚îÇ Symbol                    Float64       Float64  \n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n   1 ‚îÇ jackknife_plus_ab_minmax      0.988333  0.980547\n   2 ‚îÇ cv_minmax                     0.96      0.910873\n   3 ‚îÇ simple_inductive              0.953333  0.953333\n   4 ‚îÇ jackknife_minmax              0.946667  0.869103\n   5 ‚îÇ cv_plus                       0.945     0.866549\n   6 ‚îÇ jackknife_plus_ab             0.941667  0.941667\n   7 ‚îÇ jackknife_plus                0.941667  0.871606\n   8 ‚îÇ jackknife                     0.941667  0.941667\n   9 ‚îÇ naive                         0.938333  0.938333","category":"section"},{"location":"tutorials/regression/#References","page":"Regression","title":"References","text":"Angelopoulos, Anastasios N., and Stephen Bates. 2021. ‚ÄúA Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification.‚Äù https://arxiv.org/abs/2107.07511.","category":"section"},{"location":"how_to_guides/mnist/#How-to-Conformalize-a-Deep-Image-Classifier","page":"How to Conformalize a Deep Image Classifier","title":"How to Conformalize a Deep Image Classifier","text":"Deep Learning is popular and ‚Äî for some tasks like image classification ‚Äî remarkably powerful. But it is also well-known that Deep Neural Networks (DNN) can be unstable (Goodfellow, Shlens, and Szegedy 2014) and poorly calibrated. Conformal Prediction can be used to mitigate these pitfalls. This how-to guide demonstrates how you can build an image classifier in Flux.jl and conformalize its predictions. For a formal treatment see A. Angelopoulos et al. (2022).","category":"section"},{"location":"how_to_guides/mnist/#The-Task-at-Hand","page":"How to Conformalize a Deep Image Classifier","title":"The Task at Hand","text":"The task at hand is to predict the labels of handwritten images of digits using the famous MNIST dataset (LeCun 1998). Importing this popular machine learning dataset in Julia is made remarkably easy through MLDatasets.jl:\n\nusing MLDatasets\nN = 1000\nXraw, yraw = MNIST(split=:train)[:]\nXraw = Xraw[:,:,1:N]\nyraw = yraw[1:N]\n\nThe chart below shows a few random samples from the training data:\n\nusing MLJ\nusing Images\nX = map(x -> convert2image(MNIST, x), eachslice(Xraw, dims=3))\ny = coerce(yraw, Multiclass)\n\nn_samples = 10\nmosaic(rand(X, n_samples)..., ncol=n_samples)\n\n(Image: Figure¬†1: Random samples from the MNIST dataset.)","category":"section"},{"location":"how_to_guides/mnist/#Building-the-Network","page":"How to Conformalize a Deep Image Classifier","title":"Building the Network","text":"To model the mapping from image inputs to labels will rely on a simple Multi-Layer Perceptron (MLP). A great Julia library for Deep Learning is Flux.jl. But wait ‚Ä¶ doesn‚Äôt ConformalPrediction.jl work with models trained in MLJ.jl? That‚Äôs right, but fortunately there exists a Flux.jl interface to MLJ.jl, namely MLJFlux.jl. The interface is still in its early stages, but already very powerful and easily accessible for anyone (like myself) who is used to building Neural Networks in Flux.jl.\n\nIn Flux.jl, you could build an MLP for this task as follows,\n\nusing Flux\n\nmlp = Chain(\n    Flux.flatten,\n    Dense(prod((28,28)), 32, relu),\n    Dense(32, 10)\n)\n\nwhere (28,28) is just the input dimension (28x28 pixel images). Since we have ten digits, our output dimension is ten.[1]\n\nWe can do the exact same thing in MLJFlux.jl as follows,\n\nusing MLJFlux\n\nbuilder = MLJFlux.@builder Chain(\n    Flux.flatten,\n    Dense(prod(n_in), 32, relu),\n    Dense(32, n_out)\n)\n\nwhere here we rely on the @builder macro to make the transition from Flux.jl to MLJ.jl as seamless as possible. Finally, MLJFlux.jl already comes with a number of helper functions to define plain-vanilla networks. In this case, we will use the ImageClassifier with our custom builder and cross-entropy loss:\n\nImageClassifier = @load ImageClassifier\nclf = ImageClassifier(\n    builder=builder,\n    epochs=10,\n    loss=Flux.crossentropy\n)\n\nThe generated instance clf is a model (in the MLJ.jl sense) so from this point on we can rely on standard MLJ.jl workflows. For example, we can wrap our model in data to create a machine and then evaluate it on a holdout set as follows:\n\nmach = machine(clf, X, y)\n\nevaluate!(\n    mach,\n    resampling=Holdout(rng=123, fraction_train=0.8),\n    operation=predict_mode,\n    measure=[accuracy]\n)\n\nThe accuracy of our very simple model is not amazing, but good enough for the purpose of this tutorial. For each image, our MLP returns a softmax output for each possible digit: 0,1,2,3,‚Ä¶,9. Since each individual softmax output is valued between zero and one, y(k)‚ÄÑ‚àà‚ÄÑ(0,1), this is commonly interpreted as a probability: y(k)‚ÄÑ‚âî‚ÄÑp(y=k|X). Edge cases ‚Äì that is values close to either zero or one ‚Äì indicate high predictive certainty. But this is only a heuristic notion of predictive uncertainty (A. N. Angelopoulos and Bates 2021). Next, we will turn this heuristic notion of uncertainty into a rigorous one using Conformal Prediction.","category":"section"},{"location":"how_to_guides/mnist/#Conformalizing-the-Network","page":"How to Conformalize a Deep Image Classifier","title":"Conformalizing the Network","text":"Since clf is a model, it is also compatible with our package: ConformalPrediction.jl. To conformalize our MLP, we therefore only need to call conformal_model(clf). Since the generated instance conf_model is also just a model, we can still rely on standard MLJ.jl workflows. Below we first wrap it in data and then fit it. Aaaand ‚Ä¶ we‚Äôre done! Let‚Äôs look at the results in the next section.\n\nusing ConformalPrediction\nconf_model = conformal_model(clf; method=:simple_inductive)\nmach = machine(conf_model, X, y)\nfit!(mach)","category":"section"},{"location":"how_to_guides/mnist/#Results","page":"How to Conformalize a Deep Image Classifier","title":"Results","text":"The charts below present the results. The first row displays highly certain predictions, now defined in the rigorous sense of Conformal Prediction: in each case, the conformal set (just beneath the image) includes only one label.\n\nThe following two rows display increasingly uncertain predictions of set size two and three, respectively. They demonstrate that CP is well equipped to deal with samples characterized by high aleatoric uncertainty: digits four (4), seven (7) and nine (9) share certain similarities. So do digits five (5) and six (6) as well as three (3) and eight (8). These may be hard to distinguish from each other even after seeing many examples (and even for a human). It is therefore unsurprising to see that these digits often end up together in conformal sets.\n\n(Image: Figure¬†2: Plot 1)\n\n(Image: Figure¬†3: Plot 2)\n\n(Image: Figure¬†4: Plot 3)\n\nConformalized predictions from an image classifier.","category":"section"},{"location":"how_to_guides/mnist/#Evaluation","page":"How to Conformalize a Deep Image Classifier","title":"Evaluation","text":"As always, we can also evaluate our conformal model in terms of coverage (correctness) and size-stratified coverage (adaptiveness).\n\n_eval = evaluate!(\n    mach,\n    resampling=Holdout(rng=123, fraction_train=0.8),\n    operation=predict,\n    measure=[emp_coverage, ssc]\n)\ndisplay(_eval)\nprintln(\"Empirical coverage: $(round(_eval.measurement[1], digits=3))\")\nprintln(\"SSC: $(round(_eval.measurement[2], digits=3))\")\n\nPerformanceEvaluation object with these fields:\n  measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows\nExtract:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚îÇ measure                                      ‚îÇ operation ‚îÇ measurement ‚îÇ per ‚ãØ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚îÇ ConformalPrediction.emp_coverage             ‚îÇ predict   ‚îÇ 0.96        ‚îÇ [0. ‚ãØ\n‚îÇ ConformalPrediction.size_stratified_coverage ‚îÇ predict   ‚îÇ 0.885       ‚îÇ [0. ‚ãØ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                                                                1 column omitted\n\nEmpirical coverage: 0.96\nSSC: 0.885\n\nUnsurprisingly, we can attain higher adaptivity (SSC) when using adaptive prediction sets:\n\nconf_model = conformal_model(clf; method=:adaptive_inductive)\nmach = machine(conf_model, X, y)\nfit!(mach)\n_eval = evaluate!(\n    mach,\n    resampling=Holdout(rng=123, fraction_train=0.8),\n    operation=predict,\n    measure=[emp_coverage, ssc]\n)\nresults[:adaptive_inductive] = mach\ndisplay(_eval)\nprintln(\"Empirical coverage: $(round(_eval.measurement[1], digits=3))\")\nprintln(\"SSC: $(round(_eval.measurement[2], digits=3))\")\n\nPerformanceEvaluation object with these fields:\n  measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows\nExtract:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚îÇ measure                                      ‚îÇ operation ‚îÇ measurement ‚îÇ per ‚ãØ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚îÇ ConformalPrediction.emp_coverage             ‚îÇ predict   ‚îÇ 1.0         ‚îÇ [1. ‚ãØ\n‚îÇ ConformalPrediction.size_stratified_coverage ‚îÇ predict   ‚îÇ 1.0         ‚îÇ [1. ‚ãØ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                                                                1 column omitted\n\nEmpirical coverage: 1.0\nSSC: 1.0\n\nWe can also have a look at the resulting set size for both approaches:\n\nplt_list = []\nfor (_mod, mach) in results\n    push!(plt_list, bar(mach.model, mach.fitresult, X; title=String(_mod)))\nend\nplot(plt_list..., size=(800,300))\n\n(Image: Figure¬†5: Prediction interval width.)","category":"section"},{"location":"how_to_guides/mnist/#References","page":"How to Conformalize a Deep Image Classifier","title":"References","text":"Angelopoulos, Anastasios N., and Stephen Bates. 2021. ‚ÄúA Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification.‚Äù https://arxiv.org/abs/2107.07511.\n\nAngelopoulos, Anastasios, Stephen Bates, Jitendra Malik, and Michael I. Jordan. 2022. ‚ÄúUncertainty Sets for Image Classifiers Using Conformal Prediction.‚Äù arXiv. https://arxiv.org/abs/2009.14193.\n\nGoodfellow, Ian J, Jonathon Shlens, and Christian Szegedy. 2014. ‚ÄúExplaining and Harnessing Adversarial Examples.‚Äù https://arxiv.org/abs/1412.6572.\n\nLeCun, Yann. 1998. ‚ÄúThe MNIST Database of Handwritten Digits.‚Äù\n\n[1] For a full tutorial on how to build an MNIST image classifier relying solely on Flux.jl, check out this tutorial.","category":"section"},{"location":"tutorials/#Tutorials","page":"Overview","title":"Tutorials","text":"In this section you will find a series of tutorials that should help you gain a basic understanding of Conformal Prediction and how to apply it in Julia using this package.\n\nTutorials are lessons that take the reader by the hand through a series of steps to complete a project of some kind. Tutorials are learning-oriented.‚Äî Di√°taxis\n\nIn other words, you come here because you are new to this topic and are looking for a first peek at the methodology and code ü´£","category":"section"},{"location":"contribute/#Contributor‚Äôs-Guide","page":"üõ† Contribute","title":"Contributor‚Äôs Guide","text":"","category":"section"},{"location":"contribute/#Contents","page":"üõ† Contribute","title":"Contents","text":"Pages = [\"contribute.md\"]\nDepth = 2","category":"section"},{"location":"contribute/#Contributing-to-ConformalPrediction.jl","page":"üõ† Contribute","title":"Contributing to ConformalPrediction.jl","text":"Contributions are welcome! Please follow the SciML ColPrac guide. To get started we recommend you have a look at the Explanation section in the docs. The subsection explaining the package architecture may be particularly useful. You may already have a specific idea about what you want to contribute, in which case please feel free to open an issue and pull request. If you don‚Äôt have anything specific in mind, the list of outstanding issues may be a good source of inspiration. If you decide to work on an outstanding issue, be sure to check its current status: if it‚Äôs ‚ÄúIn Progress‚Äù, check in with the developer who last worked on the issue to see how you may help.","category":"section"},{"location":"how_to_guides/timeseries/#How-to-Conformalize-a-Time-Series-Model","page":"How to Conformalize a Time Series Model","title":"How to Conformalize a Time Series Model","text":"Time series data is prevalent across various domains, such as finance, weather forecasting, energy, and supply chains. However, accurately quantifying uncertainty in time series predictions is often a complex task due to inherent temporal dependencies, non-stationarity, and noise in the data. In this context, Conformal Prediction offers a valuable solution by providing prediction intervals which offer a sound way to quantify uncertainty.\n\nThis how-to guide demonstrates how you can conformalize a time series model using Ensemble Batch Prediction Intervals (EnbPI) (Xu and Xie 2021). This method enables the updating of prediction intervals whenever new observations are available. This dynamic update process allows the method to adapt to changing conditions, accounting for the potential degradation of predictions or the increase in noise levels in the data.","category":"section"},{"location":"how_to_guides/timeseries/#The-Task-at-Hand","page":"How to Conformalize a Time Series Model","title":"The Task at Hand","text":"Inspired by MAPIE, we employ the Victoria electricity demand dataset. This dataset contains hourly electricity demand (in GW) for Victoria state in Australia, along with corresponding temperature data (in Celsius degrees).\n\nusing CSV, DataFrames\ndf = CSV.read(\"./dev/artifacts/electricity_demand.csv\", DataFrame)","category":"section"},{"location":"how_to_guides/timeseries/#Feature-engineering","page":"How to Conformalize a Time Series Model","title":"Feature engineering","text":"In this how-to guide, we only focus on date, time and lag features.","category":"section"},{"location":"how_to_guides/timeseries/#Date-and-Time-related-features","page":"How to Conformalize a Time Series Model","title":"Date and Time-related features","text":"We create temporal features out of the date and hour:\n\nusing Dates\ndf.Datetime = Dates.DateTime.(df.Datetime, \"yyyy-mm-dd HH:MM:SS\")\ndf.Weekofyear = Dates.week.(df.Datetime)\ndf.Weekday = Dates.dayofweek.(df.Datetime)\ndf.hour = Dates.hour.(df.Datetime) \n\nAdditionally, to simulate sudden changes caused by unforeseen events, such as blackouts or lockdowns, we deliberately reduce the electricity demand by 2GW from February 22nd onward.\n\ndf.Demand_updated = copy(df.Demand)\ncondition = df.Datetime .>= Date(\"2014-02-22\")\ndf[condition, :Demand_updated] .= df[condition, :Demand_updated] .- 2\n\nThat is how the data looks like after our manipulation\n\ncutoff_point = 200\nplot(df[cutoff_point:split_index, [:Datetime]].Datetime, df[cutoff_point:split_index, :].Demand ,\n label=\"training data\", color=:green, xlabel = \"Date\" , ylabel=\"Electricity demand(GW)\")\nplot!(df[split_index+1 : size(df,1), [:Datetime]].Datetime, df[split_index+1 : size(df,1), : ].Demand,\n label=\"test data\", color=:orange, xlabel = \"Date\" , ylabel=\"Electricity demand(GW)\")\nplot!(df[split_index+1 : size(df,1), [:Datetime]].Datetime, df[split_index+1 : size(df,1), : ].Demand_updated, label=\"updated test data\", color=:red, linewidth=1, framestyle=:box)\nplot!(legend=:outerbottom, legendcolumns=3)\nplot!(size=(850,400), left_margin = 5Plots.mm)","category":"section"},{"location":"how_to_guides/timeseries/#Lag-features","page":"How to Conformalize a Time Series Model","title":"Lag features","text":"using ShiftedArrays\nn_lags = 5\nfor i = 1:n_lags\n    DataFrames.transform!(df, \"Demand\" => (x -> ShiftedArrays.lag(x, i)) => \"lag_hour_$i\")\nend\n\ndf_dropped_missing = dropmissing(df)\ndf_dropped_missing","category":"section"},{"location":"how_to_guides/timeseries/#Train-test-split","page":"How to Conformalize a Time Series Model","title":"Train-test split","text":"As usual, we split the data into train and test sets. We use the first 90% of the data for training and the remaining 10% for testing.\n\nfeatures_cols = DataFrames.select(df_dropped_missing, Not([:Datetime, :Demand, :Demand_updated]))\nX = Matrix(features_cols)\ny = Matrix(df_dropped_missing[:, [:Demand_updated]])\nsplit_index = floor(Int, 0.9 * size(y , 1)) \nprintln(split_index)\nX_train = X[1:split_index, :]\ny_train = y[1:split_index, :]\nX_test = X[split_index+1 : size(y,1), :]\ny_test = y[split_index+1 : size(y,1), :]","category":"section"},{"location":"how_to_guides/timeseries/#Loading-model-using-MLJ-interface","page":"How to Conformalize a Time Series Model","title":"Loading model using MLJ interface","text":"As our baseline model, we use a boosted tree regressor:\n\nusing MLJ\nEvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees verbosity=0\nmodel = EvoTreeRegressor(nrounds =100, max_depth=10, rng=123)","category":"section"},{"location":"how_to_guides/timeseries/#Conformal-time-series","page":"How to Conformalize a Time Series Model","title":"Conformal time series","text":"Next, we conformalize the model using EnbPI. First, we will proceed without updating training set residuals to build prediction intervals. The result is shown in the following figure:\n\nusing ConformalPrediction\n\nconf_model = conformal_model(model; method=:time_series_ensemble_batch, coverage=0.95)\nmach = machine(conf_model, X_train, y_train)\ntrain = [1:split_index;]\nfit!(mach, rows=train)\n\ny_pred_interval = MLJ.predict(conf_model, mach.fitresult, X_test)\nlb = [ minimum(tuple_data) for tuple_data in y_pred_interval]\nub = [ maximum(tuple_data) for tuple_data in y_pred_interval]\ny_pred = [mean(tuple_data) for tuple_data in y_pred_interval]\n\n#| echo: false\n#| output: true\ncutoff_point = findfirst(df_dropped_missing.Datetime .== Date(\"2014-02-15\"))\nplot(df_dropped_missing[cutoff_point:split_index, [:Datetime]].Datetime, y_train[cutoff_point:split_index] ,\n label=\"train\", color=:green , xlabel = \"Date\" , ylabel=\"Electricity demand(GW)\", linewidth=1)\nplot!(df_dropped_missing[split_index+1 : size(y,1), [:Datetime]].Datetime,\n y_test, label=\"test\", color=:red)\nplot!(df_dropped_missing[split_index+1 : size(y,1), [:Datetime]].Datetime ,\n y_pred, label =\"prediction\", color=:blue)\nplot!(df_dropped_missing[split_index+1 : size(y,1), [:Datetime]].Datetime,\n lb, fillrange = ub, fillalpha = 0.2, label = \"prediction interval w/o EnbPI\",\n  color=:lake, linewidth=0, framestyle=:box)\nplot!(legend=:outerbottom, legendcolumns=4, legendfontsize=6)\nplot!(size=(850,400), left_margin = 5Plots.mm)\n\nWe can use partial_fit method in EnbPI implementation in ConformalPrediction in order to adjust prediction intervals to sudden change points on test sets that have not been seen by the model during training. In the below experiment, samplesize indicates the batch of new observations. You can decide if you want to update residuals by samplesize or update and remove first n residuals (shift_size = n). The latter will allow to remove early residuals that will not have a positive impact on the current observations.\n\nThe chart below compares the results to the previous experiment without updating residuals:\n\nsample_size = 30\nshift_size = 100\nlast_index = size(X_test , 1)\nlb_updated , ub_updated = ([], [])\nfor step in 1:sample_size:last_index\n    if last_index - step < sample_size\n        y_interval = MLJ.predict(conf_model, mach.fitresult, X_test[step:last_index , :])\n        partial_fit(mach.model , mach.fitresult, X_test[step:last_index , :], y_test[step:last_index , :], shift_size)\n    else\n        y_interval = MLJ.predict(conf_model, mach.fitresult, X_test[step:step+sample_size-1 , :])\n        partial_fit(mach.model , mach.fitresult, X_test[step:step+sample_size-1 , :], y_test[step:step+sample_size-1 , :], shift_size)        \n    end \n    lb_updated·µ¢= [ minimum(tuple_data) for tuple_data in y_interval]\n    push!(lb_updated,lb_updated·µ¢)\n    ub_updated·µ¢ = [ maximum(tuple_data) for tuple_data in y_interval]\n    push!(ub_updated, ub_updated·µ¢)\nend\nlb_updated = reduce(vcat, lb_updated)\nub_updated = reduce(vcat, ub_updated)\n\n#| echo: false\n#| output: true\nplot(df_dropped_missing[cutoff_point:split_index, [:Datetime]].Datetime, y_train[cutoff_point:split_index] ,\n label=\"train\", color=:green , xlabel = \"Date\" , ylabel=\"Electricity demand(GW)\", linewidth=1)\nplot!(df_dropped_missing[split_index+1 : size(y,1), [:Datetime]].Datetime, y_test,\n label=\"test\", color=:red)\nplot!(df_dropped_missing[split_index+1 : size(y,1), [:Datetime]].Datetime ,\n                                 y_pred, label =\"prediction\", color=:blue)\nplot!(df_dropped_missing[split_index+1 : size(y,1), [:Datetime]].Datetime,\n lb_updated, fillrange = ub_updated, fillalpha = 0.2, label = \"EnbPI\",\n  color=:lake, linewidth=0, framestyle=:box)\nplot!(legend=:outerbottom, legendcolumns=4)\nplot!(size=(850,400), left_margin = 5Plots.mm)","category":"section"},{"location":"how_to_guides/timeseries/#Results","page":"How to Conformalize a Time Series Model","title":"Results","text":"In time series problems, unexpected incidents can lead to sudden changes, and such scenarios are highly probable. As illustrated earlier, the model‚Äôs training data lacks information about these change points, making it unable to anticipate them. The top figure demonstrates that when residuals are not updated, the prediction intervals solely rely on the distribution of residuals from the training set. Consequently, these intervals fail to encompass the true observations after the change point, resulting in a sudden drop in coverage.\n\nHowever, by partially updating the residuals, the method becomes adept at capturing the increasing uncertainties in model predictions. It is important to note that the changes in uncertainty occur approximately one day after the change point. This delay is attributed to the requirement of having a sufficient number of new residuals to alter the quantiles obtained from the residual distribution.","category":"section"},{"location":"how_to_guides/timeseries/#References","page":"How to Conformalize a Time Series Model","title":"References","text":"Xu, Chen, and Yao Xie. 2021. ‚ÄúConformal Prediction Interval for Dynamic Time-Series.‚Äù In, 11559‚Äì69. PMLR. https://proceedings.mlr.press/v139/xu21h.html.","category":"section"},{"location":"explanation/architecture/#Package-Architecture","page":"Package Architecture","title":"Package Architecture","text":"The diagram below demonstrates the package architecture at the time of writing. This is still subject to change, so any thoughts and comments are very much welcome.\n\nThe goal is to make this package as compatible as possible with MLJ to tab into existing functionality. The basic idea is to subtype MLJ Supervised models and then use concrete types to implement different approaches to conformal prediction. For each of these concrete types the compulsory MMI.fit and MMI.predict methods need be implemented (see here).\n\n(Image: )","category":"section"},{"location":"explanation/architecture/#Abstract-Subtypes","page":"Package Architecture","title":"Abstract Subtypes","text":"Currently, I intend to work with three different abstract subtypes:\n\nConformalPrediction.ConformalInterval\nConformalPrediction.ConformalProbabilisticSet\nConformalPrediction.ConformalProbabilistic","category":"section"},{"location":"explanation/architecture/#fit-and-predict","page":"Package Architecture","title":"fit and predict","text":"The fit and predict methods are compulsory in order to prepare models for general use with MLJ. They also serve us to implement the logic underlying the various approaches to conformal prediction. To understand how this currently works, have a look at the ConformalPrediction.AdaptiveInductiveClassifier as an example: fit(conf_model::ConformalPrediction.AdaptiveInductiveClassifier, verbosity, X, y) and predict(conf_model::ConformalPrediction.AdaptiveInductiveClassifier, fitresult, Xnew).","category":"section"},{"location":"explanation/#Explanation","page":"Overview","title":"Explanation","text":"In this section you will find detailed explanations about the methodology and code.\n\nExplanation clarifies, deepens and broadens the reader‚Äôs understanding of a subject.‚Äî Di√°taxis\n\nIn other words, you come here because you are interested in understanding how all of this actually works ü§ì","category":"section"},{"location":"tutorials/classification/#Classification","page":"Classification","title":"Classification","text":"This tutorial is based in parts on this blog post.","category":"section"},{"location":"tutorials/classification/#Split-Conformal-Classification","page":"Classification","title":"Split Conformal Classification","text":"We consider a simple binary classification problem. Let (X(i),Y(i)),¬†i‚ÄÑ=‚ÄÑ1,‚ÄÜ...,‚ÄÜn denote our feature-label pairs and let Œº‚ÄÑ:‚ÄÑùí≥‚ÄÑ‚Ü¶‚ÄÑùí¥ denote the mapping from features to labels. For illustration purposes we will use the moons dataset üåô. Using MLJ.jl we first generate the data and split into into a training and test set:\n\nusing MLJ\nusing Random\nRandom.seed!(123)\n\n# Data:\nX, y = make_moons(500; noise=0.15)\nX = MLJ.table(convert.(Float32, MLJ.matrix(X)))\ntrain, test = partition(eachindex(y), 0.8, shuffle=true)\n\nHere we will use a specific case of CP called split conformal prediction which can then be summarized as follows:[1]\n\nPartition the training into a proper training set and a separate calibration set: ùíü_(n)‚ÄÑ=‚ÄÑùíü^(train)‚ÄÖ‚à™‚ÄÖùíü^(cali).\nTrain the machine learning model on the proper training set: ŒºÃÇ(i‚ÄÑ‚àà‚ÄÑùíü^(train))(X(i),Y_(i)).\nCompute nonconformity scores, ùíÆ, using the calibration data ùíü^(cali) and the fitted model ŒºÃÇ_(i‚ÄÑ‚àà‚ÄÑùíü^(train)).\nFor a user-specified desired coverage ratio (1‚àíŒ±) compute the corresponding quantile, qÃÇ, of the empirical distribution of nonconformity scores, ùíÆ.\nFor the given quantile and test sample X_(test), form the corresponding conformal prediction set:\n\nC(X_texttest)=ys(X_texttesty) le hatq\n\nThis is the default procedure used for classification and regression in ConformalPrediction.jl.\n\nNow let‚Äôs take this to our üåô data. To illustrate the package functionality we will demonstrate the envisioned workflow. We first define our atomic machine learning model following standard MLJ.jl conventions. Using ConformalPrediction.jl we then wrap our atomic model in a conformal model using the standard API call conformal_model(model::Supervised; kwargs...). To train and predict from our conformal model we can then rely on the conventional MLJ.jl procedure again. In particular, we wrap our conformal model in data (turning it into a machine) and then fit it to the training data. Finally, we use our machine to predict the label for a new test sample Xtest:\n\n# Model:\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\nmodel = KNNClassifier(;K=50) \n\n# Training:\nusing ConformalPrediction\nconf_model = conformal_model(model; coverage=.9)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)\n\n# Conformal Prediction:\nXtest = selectrows(X, test)\nytest = y[test]\nyÃÇ = predict(mach, Xtest)\nyÃÇ[1]\n\nimport NearestNeighborModels ‚úî\n\nUnivariateFinite{Multiclass{2}}(0=>0.94)\n\nThe final predictions are set-valued. While the softmax output remains unchanged for the SimpleInductiveClassifier, the size of the prediction set depends on the chosen coverage rate, (1‚àíŒ±).\n\nWhen specifying a coverage rate very close to one, the prediction set will typically include many (in some cases all) of the possible labels. Below, for example, both classes are included in the prediction set when setting the coverage rate equal to (1‚àíŒ±)=1.0. This is intuitive, since high coverage quite literally requires that the true label is covered by the prediction set with high probability.\n\nconf_model = conformal_model(model; coverage=coverage, method=:simple_inductive)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)\n\n# Conformal Prediction:\nXtest = (x1=[1],x2=[0])\npredict(mach, Xtest)[1]\n\nUnivariateFinite{Multiclass{2}}(0=>0.5, 1=>0.5)\n\nConversely, for low coverage rates, prediction sets can also be empty. For a choice of (1‚àíŒ±)=0.1, for example, the prediction set for our test sample is empty. This is a bit difficult to think about intuitively and I have not yet come across a satisfactory, intuitive interpretation.[2] When the prediction set is empty, the predict call currently returns missing:\n\nconf_model = conformal_model(model; coverage=coverage)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)\n\n# Conformal Prediction:\npredict(mach, Xtest)[1]\n\nmissing\n\ncov_ = .95\nconf_model = conformal_model(model; coverage=cov_)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)\nMarkdown.parse(\"\"\"\nThe following chart shows the resulting predicted probabilities for ``y=1`` (left) and set size (right) for a choice of ``(1-\\\\alpha)``=$cov_.\n\"\"\")\n\nThe following chart shows the resulting predicted probabilities for y‚ÄÑ=‚ÄÑ1 (left) and set size (right) for a choice of (1‚àíŒ±)=0.95.\n\nusing Plots\np_proba = contourf(mach.model, mach.fitresult, X, y)\np_set_size = contourf(mach.model, mach.fitresult, X, y; plot_set_size=true)\nplot(p_proba, p_set_size, size=(800,250))\n\n(Image: )\n\nThe animation below should provide some more intuition as to what exactly is happening here. It illustrates the effect of the chosen coverage rate on the predicted softmax output and the set size in the two-dimensional feature space. Contours are overlayed with the moon data points (including test data). The two samples highlighted in red, X‚ÇÅ and X‚ÇÇ, have been manually added for illustration purposes. Let‚Äôs look at these one by one.\n\nFirstly, note that X‚ÇÅ (red cross) falls into a region of the domain that is characterized by high predictive uncertainty. It sits right at the bottom-right corner of our class-zero moon üåú (orange), a region that is almost entirely enveloped by our class-one moon üåõ (green). For low coverage rates the prediction set for X‚ÇÅ is empty: on the left-hand side this is indicated by the missing contour for the softmax probability; on the right-hand side we can observe that the corresponding set size is indeed zero. For high coverage rates the prediction set includes both y‚ÄÑ=‚ÄÑ0 and y‚ÄÑ=‚ÄÑ1, indicative of the fact that the conformal classifier is uncertain about the true label.\n\nWith respect to X‚ÇÇ, we observe that while also sitting on the fringe of our class-zero moon, this sample populates a region that is not fully enveloped by data points from the opposite class. In this region, the underlying atomic classifier can be expected to be more certain about its predictions, but still not highly confident. How is this reflected by our corresponding conformal prediction sets?\n\nXtest_2 = (x1=[-0.5],x2=[0.25])\npÃÇ_2 = pdf(predict(mach, Xtest_2)[1], 0)\n\nWell, for low coverage rates (roughly ‚ÄÑ\\<‚ÄÑ0.9) the conformal prediction set does not include y‚ÄÑ=‚ÄÑ0: the set size is zero (right panel). Only for higher coverage rates do we have C(X‚ÇÇ)‚ÄÑ=‚ÄÑ{0}: the coverage rate is high enough to include y‚ÄÑ=‚ÄÑ0, but the corresponding softmax probability is still fairly low. For example, for (1‚àíŒ±)‚ÄÑ=‚ÄÑ0.95 we have pÃÇ(y=0|X‚ÇÇ)‚ÄÑ=‚ÄÑ0.72.\n\nThese two examples illustrate an interesting point: for regions characterized by high predictive uncertainty, conformal prediction sets are typically empty (for low coverage) or large (for high coverage). While set-valued predictions may be something to get used to, this notion is overall intuitive.\n\n# Setup\ncoverages = range(0.75,1.0,length=5)\nn = 100\nx1_range = range(extrema(X.x1)...,length=n)\nx2_range = range(extrema(X.x2)...,length=n)\n\nanim = @animate for coverage in coverages\n    conf_model = conformal_model(model; coverage=coverage)\n    mach = machine(conf_model, X, y)\n    fit!(mach, rows=train)\n    # Probabilities:\n    p1 = contourf(mach.model, mach.fitresult, X, y)\n    scatter!(p1, Xtest.x1, Xtest.x2, ms=6, c=:red, label=\"X‚ÇÅ\", shape=:cross, msw=6)\n    scatter!(p1, Xtest_2.x1, Xtest_2.x2, ms=6, c=:red, label=\"X‚ÇÇ\", shape=:diamond, msw=6)\n    p2 = contourf(mach.model, mach.fitresult, X, y; plot_set_size=true)\n    scatter!(p2, Xtest.x1, Xtest.x2, ms=6, c=:red, label=\"X‚ÇÅ\", shape=:cross, msw=6)\n    scatter!(p2, Xtest_2.x1, Xtest_2.x2, ms=6, c=:red, label=\"X‚ÇÇ\", shape=:diamond, msw=6)\n    plot(p1, p2, plot_title=\"(1-Œ±)=$(round(coverage,digits=2))\", size=(800,300))\nend\n\ngif(anim, joinpath(www_path,\"classification.gif\"), fps=1)\n\n(Image: )","category":"section"},{"location":"tutorials/classification/#Adaptive-Sets","page":"Classification","title":"Adaptive Sets","text":"Instead of using the simple approach, we can use adaptive prediction sets (Angelopoulos and Bates 2021):\n\nconf_model = conformal_model(model; coverage=cov_, method=:adaptive_inductive)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)\nresults[:adaptive_inductive] = mach\n\nusing Plots\np_proba = contourf(mach.model, mach.fitresult, X, y)\np_set_size = contourf(mach.model, mach.fitresult, X, y; plot_set_size=true)\nplot(p_proba, p_set_size, size=(800,250))","category":"section"},{"location":"tutorials/classification/#Evaluation","page":"Classification","title":"Evaluation","text":"For evaluation of conformal predictors we follow Angelopoulos and Bates (2021) (Section 3). As a first step towards adaptiveness (adaptivity), the authors recommend to inspect the set size of conformal predictions. The chart below shows the interval width for the different methods along with the ground truth interval width:\n\nplt_list = []\nfor (_mod, mach) in results\n    push!(plt_list, bar(mach.model, mach.fitresult, X; title=String(_mod)))\nend\nplot(plt_list..., size=(800,300))\n\n(Image: Figure¬†1: Prediction interval width.)\n\nWe can also use specific metrics like empirical coverage and size-stratified coverage to check for correctness and adaptiveness, respectively. To this end, the package provides custom measures that are compatible with MLJ.jl. In other words, we can evaluate model performance in true MLJ.jl fashion (see here).\n\nThe code below runs the evaluation with respect to both metrics, emp_coverage and ssc for a single conformal machine:\n\n_mod, mach = first(results)\n_eval = evaluate!(\n    mach,\n    operation=predict,\n    measure=[emp_coverage, ssc]\n)\n# display(_eval)\nprintln(\"Empirical coverage for $(_mod): $(round(_eval.measurement[1], digits=3))\")\nprintln(\"SSC for $(_mod): $(round(_eval.measurement[2], digits=3))\")\n\nEmpirical coverage for adaptive_inductive: 0.962\nSSC for adaptive_inductive: 0.962","category":"section"},{"location":"tutorials/classification/#References","page":"Classification","title":"References","text":"Angelopoulos, Anastasios N., and Stephen Bates. 2021. ‚ÄúA Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification.‚Äù https://arxiv.org/abs/2107.07511.\n\n[1] In other places split conformal prediction is sometimes referred to as inductive conformal prediction.\n\n[2] Any thoughts/comments welcome!","category":"section"},{"location":"tutorials/plotting/#Visualization-using-TaijaPlotting.jl","page":"Visualization using TaijaPlotting.jl","title":"Visualization using TaijaPlotting.jl","text":"This tutorial demonstrates how various custom plotting methods can be used to visually analyze conformal predictors.\n\nusing ConformalPrediction\nusing Plots, TaijaPlotting","category":"section"},{"location":"tutorials/plotting/#Regression","page":"Visualization using TaijaPlotting.jl","title":"Regression","text":"","category":"section"},{"location":"tutorials/plotting/#Visualizing-Prediction-Intervals","page":"Visualization using TaijaPlotting.jl","title":"Visualizing Prediction Intervals","text":"For conformal regressors, the TaijaPlotting.plot can be used to visualize the prediction intervals for given data points.","category":"section"},{"location":"tutorials/plotting/#Univariate-Input","page":"Visualization using TaijaPlotting.jl","title":"Univariate Input","text":"using MLJ\nX, y = make_regression(100, 1; noise=0.3)\n\nEvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees\nmodel = EvoTreeRegressor() \nconf_model = conformal_model(model)\nmach = machine(conf_model, X, y)\nfit!(mach)\n\nplot(mach.model, mach.fitresult, X, y; input_var=1)\n\n(Image: )","category":"section"},{"location":"tutorials/plotting/#Multivariate-Input","page":"Visualization using TaijaPlotting.jl","title":"Multivariate Input","text":"using MLJ\nX, y = @load_boston\nschema(X)\n\nEvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees\nmodel = EvoTreeRegressor() \nconf_model = conformal_model(model)\nmach = machine(conf_model, X, y)\nfit!(mach)\n\ninput_vars = [:Crim, :Age, :Tax]\nnvars = length(input_vars)\nplt_list = []\nfor input_var in input_vars\n    plt = plot(mach.model, mach.fitresult, X, y; input_var=input_var, title=input_var)\n    push!(plt_list, plt)\nend\nplot(plt_list..., layout=(1,nvars), size=(nvars*200, 200))\n\n(Image: )","category":"section"},{"location":"tutorials/plotting/#Visualizing-Set-Size","page":"Visualization using TaijaPlotting.jl","title":"Visualizing Set Size","text":"To visualize the set size distribution, the TaijaPlotting.bar can be used. For regression models, the prediction interval widths are stratified into discrete bins.\n\nbar(mach.model, mach.fitresult, X)\n\n(Image: )\n\nEvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees\nmodel = EvoTreeRegressor() \nconf_model = conformal_model(model, method=:jackknife_plus)\nmach = machine(conf_model, X, y)\nfit!(mach)\n\nbar(mach.model, mach.fitresult, X)\n\n(Image: )","category":"section"},{"location":"tutorials/plotting/#Classification","page":"Visualization using TaijaPlotting.jl","title":"Classification","text":"KNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\nmodel = KNNClassifier(;K=3)","category":"section"},{"location":"tutorials/plotting/#Visualizing-Predictions","page":"Visualization using TaijaPlotting.jl","title":"Visualizing Predictions","text":"","category":"section"},{"location":"tutorials/plotting/#Stacked-Area-Charts","page":"Visualization using TaijaPlotting.jl","title":"Stacked Area Charts","text":"Stacked area charts can be used to visualize prediction sets for any conformal classifier.\n\nusing MLJ\nn_input = 4\nX, y = make_blobs(100, n_input)\n\nconf_model = conformal_model(model)\nmach = machine(conf_model, X, y)\nfit!(mach)\n\nplt_list = []\nfor i in 1:n_input\n    plt = areaplot(mach.model, mach.fitresult, X, y; input_var=i, title=\"Input $i\")\n    push!(plt_list, plt)\nend\nplot(plt_list..., size=(220*n_input,200), layout=(1, n_input))\n\n(Image: )","category":"section"},{"location":"tutorials/plotting/#Contour-Plots-for-Two-Dimensional-Inputs","page":"Visualization using TaijaPlotting.jl","title":"Contour Plots for Two-Dimensional Inputs","text":"For conformal classifiers with exactly two input variables, the TaijaPlotting.contourf method can be used to visualize conformal predictions in the two-dimensional feature space.\n\nusing MLJ\nX, y = make_blobs(100, 2)\n\nconf_model = conformal_model(model)\nmach = machine(conf_model, X, y)\nfit!(mach)\n\np1 = contourf(mach.model, mach.fitresult, X, y)\np2 = contourf(mach.model, mach.fitresult, X, y; plot_set_size=true)\nplot(p1, p2, size=(700,300))\n\n(Image: )","category":"section"},{"location":"tutorials/plotting/#Visualizing-Set-Size-2","page":"Visualization using TaijaPlotting.jl","title":"Visualizing Set Size","text":"To visualize the set size distribution, the TaijaPlotting.bar can be used. Recall that for more adaptive predictors the distribution of set sizes is typically spread out more widely, which reflects that ‚Äúthe procedure is effectively distinguishing between easy and hard inputs‚Äù (Angelopoulos and Bates 2021). This is desirable: when for a given sample it is difficult to make predictions, this should be reflected in the set size (or interval width in the regression case). Since ‚Äòdifficult‚Äô lies on some spectrum that ranges from ‚Äòvery easy‚Äô to ‚Äòvery difficult‚Äô the set size should vary across the spectrum of ‚Äòempty set‚Äô to ‚Äòall labels included‚Äô.\n\nX, y = make_moons(500; noise=0.15)\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\nmodel = KNNClassifier(;K=50) \n\nconf_model = conformal_model(model)\nmach = machine(conf_model, X, y)\nfit!(mach)\n\np1 = contourf(mach.model, mach.fitresult, X, y; plot_set_size=true)\np2 = bar(mach.model, mach.fitresult, X)\nplot(p1, p2, size=(700,300))\n\n(Image: )\n\nconf_model = conformal_model(model, method=:adaptive_inductive)\nmach = machine(conf_model, X, y)\nfit!(mach)\n\np1 = contourf(mach.model, mach.fitresult, X, y; plot_set_size=true)\np2 = bar(mach.model, mach.fitresult, X)\nplot(p1, p2, size=(700,300))\n\n(Image: )\n\nAngelopoulos, Anastasios N., and Stephen Bates. 2021. ‚ÄúA Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification.‚Äù https://arxiv.org/abs/2107.07511.","category":"section"},{"location":"faq/#Frequently-Asked-Questions","page":"‚ùì FAQ","title":"Frequently Asked Questions","text":"In this section we attempt to provide some reflections on frequently asked questions about the package and implemented methodologies. If you have a particular question that is not listed here, please feel free to also open an issue. While can answer questions regarding the package with a certain degree of confidence, I do not pretend to have any definite answers to methodological questions, but merely reflections (see the disclaimer below).","category":"section"},{"location":"faq/#Package","page":"‚ùì FAQ","title":"Package","text":"","category":"section"},{"location":"faq/#Why-the-interface-to-MLJ.jl?","page":"‚ùì FAQ","title":"Why the interface to MLJ.jl?","text":"An important design choice. MLJ.jl is a one-stop shop for common machine learning models and pipelines in Julia. It‚Äôs growing fast and the development team is very accessible, friendly and enthusiastic. Conformal Prediction is a model-agnostic approach to uncertainty quantification, so it can be applied to any common (supervised) machine learning model. For these reasons I decided to interface this package to MLJ.jl. The idea is that any (supervised) MLJ.jl model can be conformalized using ConformalPrediction.jl. By leveraging existing MLJ.jl functionality for common tasks like training, prediction and model evaluation, this package is light-weight and scalable.","category":"section"},{"location":"faq/#Methodology","page":"‚ùì FAQ","title":"Methodology","text":"For methodological questions about Conformal Prediction, my best advice is to consult the literature on the topic. A good place to start is ‚ÄúA Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification‚Äù (Angelopoulos and Bates 2021): the tutorial is comprehensive, accessible and continuously updated. Below you will find a list of high-level questions and reflections.\n\nwarning: Disclaimer\n\n\n¬†¬†¬†¬†I want to emphasize that these are merely my own reflections. I provide these to the best of my knowledge and understanding of the topic, but please be aware that I am still on a learning journey myself. I have not read the entire literature on this topic (and won‚Äôt be able to in the future either). If you spot anything that doesn‚Äôt look right or sits at odds with something your read in the literature, please open an issue. Even better: if you want to add your own reflections and thoughts, feel free to open a pull request.","category":"section"},{"location":"faq/#What-is-Predictive-Uncertainty-Quantification?","page":"‚ùì FAQ","title":"What is Predictive Uncertainty Quantification?","text":"Predictive Uncertainty Quantification deals with quantifying the uncertainty around predictions for the output variable of a supervised model. It is a subset of Uncertainty Quantification, which can also relate to uncertainty around model parameters, for example. I will sometimes use both terms interchangeably, even though I shouldn‚Äôt (please bare with me, or if you‚Äôre bothered by a particular slip-up, open a PR).\n\nUncertainty of model parameters is a very important topic itself: we might be interested in understanding, for example, if the estimated effect Œ∏ of some input variable x on the output variable y is statistically significant. This typically hinges on being able to quantify the uncertainty around the parameter Œ∏. This package does not offer this sort of functionality. I have so far not come across any work on Conformal Inference that deals with parameter uncertainty, but I also haven‚Äôt properly looked for it.","category":"section"},{"location":"faq/#What-is-the-(marginal)-coverage-guarantee?","page":"‚ùì FAQ","title":"What is the (marginal) coverage guarantee?","text":"The (marginal) coverage guarantee states that:\n\n[‚Ä¶] the probability that the prediction set contains the correct label [for a fresh test point from the same distribution] is almost exactly 1‚ÄÖ‚àí‚ÄÖŒ±.‚Äî Angelopoulos and Bates (2021)\n\nSee Angelopoulos and Bates (2021) for a formal proof of this property or check out this section or Pluto.jl üéà notebook to convince yourself through a small empirical exercise. Note that this property relates to a special case of conformal prediction, namely Split Conformal Prediction (Angelopoulos and Bates 2021).","category":"section"},{"location":"faq/#What-does-marginal-mean-in-this-context?","page":"‚ùì FAQ","title":"What does marginal mean in this context?","text":"The property is ‚Äúmarginal‚Äù in the sense that the probability is averaged over the randomness in the data (Angelopoulos and Bates 2021). Depending on the size of the calibration set (context: Split Conformal Prediction), the realized coverage or estimated empirical coverage may deviate slightly from the user specified value 1‚ÄÖ‚àí‚ÄÖŒ±. To get a sense of this effect, you may want to check out this Pluto.jl üéà notebook: it allows you to adjust the calibration set size and check the resulting empirical coverage. See also Section 3 of Angelopoulos and Bates (2021).","category":"section"},{"location":"faq/#Is-CP-really-distribution-free?","page":"‚ùì FAQ","title":"Is CP really distribution-free?","text":"The marginal coverage property holds under the assumption that the input data is exchangeable, which is a minimal distributional assumption. So, in my view, the short answer to this question is ‚ÄúNo‚Äù. I believe that when people use the term ‚Äúdistribution-free‚Äù in this context, they mean that no prior assumptions are being made about the actual form or family of distribution(s) that generate the model parameters and data. If we define ‚Äúdistribution-free‚Äù in this sense, then the answer to me seems ‚ÄúYes‚Äù.","category":"section"},{"location":"faq/#What-happens-if-this-minimal-distributional-assumption-is-violated?","page":"‚ùì FAQ","title":"What happens if this minimal distributional assumption is violated?","text":"Then the marginal coverage property does not hold. See here for an example.","category":"section"},{"location":"faq/#What-are-set-valued-predictions?","page":"‚ùì FAQ","title":"What are set-valued predictions?","text":"This should be clearer after reading through some of the other tutorials and explanations. For conformal classifiers of type ConformalProbabilisticSet, predictions are set-valued: these conformal classifiers may return multiple labels, a single label or no labels at all. Larger prediction sets indicate higher predictive uncertainty: for sets of size greater than one the conformal predictor cannot with certainty narrow down its prediction down to a single label, so it returns all labels that meet the specified marginal coverage.","category":"section"},{"location":"faq/#How-do-I-interpret-the-distribution-of-set-size?","page":"‚ùì FAQ","title":"How do I interpret the distribution of set size?","text":"It can be useful to plot the distribution of set sizes in order to visually asses how adaptive a conformal predictor is. For more adaptive predictors the distribution of set sizes is typically spread out more widely, which reflects that ‚Äúthe procedure is effectively distinguishing between easy and hard inputs‚Äù (Angelopoulos and Bates 2021). This is desirable: when for a given sample it is difficult to make predictions, this should be reflected in the set size (or interval width in the regression case). Since ‚Äòdifficult‚Äô lies on some spectrum that ranges from ‚Äòvery easy‚Äô to ‚Äòvery difficult‚Äô the set size should very across the spectrum of ‚Äòempty set‚Äô to ‚Äòall labels included‚Äô.","category":"section"},{"location":"faq/#What-is-aleatoric-uncertainty?-What-is-epistemic-uncertainty?","page":"‚ùì FAQ","title":"What is aleatoric uncertainty? What is epistemic uncertainty?","text":"Loosely speaking: aleatoric uncertainty relates to uncertainty that cannot be ‚Äúlearned away‚Äù by observing more data (think points near the decision boundary); epistemic uncertainty relates to uncertainty that can be ‚Äúlearned away‚Äù by observing more data.","category":"section"},{"location":"faq/#References","page":"‚ùì FAQ","title":"References","text":"Angelopoulos, Anastasios N., and Stephen Bates. 2021. ‚ÄúA Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification.‚Äù https://arxiv.org/abs/2107.07511.","category":"section"},{"location":"reference/#Reference","page":"üßê Reference","title":"Reference","text":"In this reference you will find a detailed overview of the package API.\n\nReference guides are technical descriptions of the machinery and how to operate it. Reference material is information-oriented.‚Äî Di√°taxis\n\nIn other words, you come here because you want to take a very close look at the code üßê","category":"section"},{"location":"reference/#Content","page":"üßê Reference","title":"Content","text":"Pages = [\"_reference.md\"]","category":"section"},{"location":"reference/#Index","page":"üßê Reference","title":"Index","text":"","category":"section"},{"location":"reference/#Public-Interface","page":"üßê Reference","title":"Public Interface","text":"","category":"section"},{"location":"reference/#Internal-functions","page":"üßê Reference","title":"Internal functions","text":"","category":"section"},{"location":"reference/#ConformalPrediction.available_models","page":"üßê Reference","title":"ConformalPrediction.available_models","text":"A container listing all available methods for conformal prediction.\n\n\n\n\n\n","category":"constant"},{"location":"reference/#ConformalPrediction.tested_atomic_models","page":"üßê Reference","title":"ConformalPrediction.tested_atomic_models","text":"A container listing all atomic MLJ models that have been tested for use with this package.\n\n\n\n\n\n","category":"constant"},{"location":"reference/#ConformalPrediction.conformal_model-Tuple{MLJModelInterface.Supervised}","page":"üßê Reference","title":"ConformalPrediction.conformal_model","text":"conformal_model(model::Supervised; method::Union{Nothing, Symbol}=nothing, kwargs...)\n\nA simple wrapper function that turns a model::Supervised into a conformal model. It accepts an optional key argument that can be used to specify the desired method for conformal prediction as well as additinal kwargs... specific to the method.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.emp_coverage-Tuple{Any, Any}","page":"üßê Reference","title":"ConformalPrediction.emp_coverage","text":"emp_coverage(yÃÇ, y)\n\nComputes the empirical coverage for conformal predictions yÃÇ.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.ineff","page":"üßê Reference","title":"ConformalPrediction.ineff","text":"ineff(yÃÇ)\n\nComputes the inefficiency (average set size) for conformal predictions yÃÇ.\n\n\n\n\n\n","category":"function"},{"location":"reference/#ConformalPrediction.is_classifier-Tuple{MLJModelInterface.Supervised}","page":"üßê Reference","title":"ConformalPrediction.is_classifier","text":"is_classifier(model::Supervised)\n\nCheck if the model is a classification model or a regression model\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.partial_fit","page":"üßê Reference","title":"ConformalPrediction.partial_fit","text":"partial_fit(conf_model::TimeSeriesRegressorEnsembleBatch, fitresult, X, y, shift_size)\n\nFor the TimeSeriesRegressorEnsembleBatch Non-conformity scores are updated by the most recent data (X,y). shift_size determines how many points in Non-conformity scores will be discarded.\n\n\n\n\n\n","category":"function"},{"location":"reference/#ConformalPrediction.set_size-Tuple{Any}","page":"üßê Reference","title":"ConformalPrediction.set_size","text":"set_size(yÃÇ)\n\nHelper function that computes the set size for conformal predictions. \n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.size_stratified_coverage-Tuple{Any, Any}","page":"üßê Reference","title":"ConformalPrediction.size_stratified_coverage","text":"size_stratified_coverage(yÃÇ, y)\n\nComputes the size-stratified coverage for conformal predictions yÃÇ.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.split_data-Tuple{ConformalModel, Any, Any}","page":"üßê Reference","title":"ConformalPrediction.split_data","text":"split_data(conf_model::ConformalProbabilisticSet, indices::Base.OneTo{Int})\n\nSplits the data into a proper training and calibration set.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.fit-Tuple{ConformalPrediction.AdaptiveInductiveClassifier, Any, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.fit","text":"MMI.fit(conf_model::AdaptiveInductiveClassifier, verbosity, X, y)\n\nFor the AdaptiveInductiveClassifier nonconformity scores are computed by cumulatively summing the ranked scores of each label in descending order until reaching the true label Y_i:\n\nS_i^textCAL = s(X_iY_i) = sum_j=1^k  hatmu(X_i)_pi_j  textwhere   Y_i=pi_k  i in mathcalD_textcalibration\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.fit-Tuple{ConformalPrediction.BayesRegressor, Any, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.fit","text":"MMI.fit(conf_model::BayesRegressor, verbosity, X, y)\n\nFor the BayesRegressor nonconformity scores are computed as follows:\n\nS_i^textCAL = s(X_i Y_i) = h(hatmu(X_i) Y_i) = - P(Y_iX_i)  i in mathcalD_textcalibration where  P(Y_iX_i) denotes the posterior probability distribution of getting Y_i given X_i.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.fit-Tuple{ConformalPrediction.CVMinMaxRegressor, Any, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.fit","text":"MMI.fit(conf_model::CVMinMaxRegressor, verbosity, X, y)\n\nFor the CVMinMaxRegressor nonconformity scores are computed in the same way as for the CVPlusRegressor. Specifically, we have,\n\nS_i^textCV = s(X_i Y_i) = h(hatmu_-mathcalD_k(i)(X_i) Y_i)  i in mathcalD_texttrain\n\nwhere hatmu_-mathcalD_k(i)(X_i) denotes the CV prediction for X_i. In other words, for each CV fold k=1K and each training instance i=1n the model is trained on all training data excluding the fold containing i. The fitted model is then used to predict out-of-sample from X_i. The corresponding nonconformity score is then computed by applying a heuristic uncertainty measure h(cdot) to the fitted value hatmu_-mathcalD_k(i)(X_i) and the true value Y_i.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.fit-Tuple{ConformalPrediction.CVPlusRegressor, Any, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.fit","text":"MMI.fit(conf_model::CVPlusRegressor, verbosity, X, y)\n\nFor the CVPlusRegressor nonconformity scores are computed though cross-validation (CV) as follows,\n\nS_i^textCV = s(X_i Y_i) = h(hatmu_-mathcalD_k(i)(X_i) Y_i)  i in mathcalD_texttrain\n\nwhere hatmu_-mathcalD_k(i)(X_i) denotes the CV prediction for X_i. In other words, for each CV fold k=1K and each training instance i=1n the model is trained on all training data excluding the fold containing i. The fitted model is then used to predict out-of-sample from X_i. The corresponding nonconformity score is then computed by applying a heuristic uncertainty measure h(cdot) to the fitted value hatmu_-mathcalD_k(i)(X_i) and the true value Y_i.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.fit-Tuple{ConformalPrediction.ConformalQuantileRegressor, Any, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.fit","text":"MMI.fit(conf_model::ConformalQuantileRegressor, verbosity, X, y)\n\nFor the ConformalQuantileRegressor nonconformity scores are computed as follows:\n\nS_i^textCAL = s(X_i Y_i) = h(hatmu_alpha_lo(X_i) hatmu_alpha_hi(X_i)  Y_i)  i in mathcalD_textcalibration\n\nA typical choice for the heuristic function is h(hatmu_alpha_lo(X_i) hatmu_alpha_hi(X_i)  Y_i)= maxhatmu_alpha_low(X_i)-Y_i Y_i-hatmu_alpha_hi(X_i) where hatmu denotes the model fitted on training data mathcalD_texttrain and\\alpha{lo}, \\alpha{hi}`` lower and higher percentile.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.fit-Tuple{ConformalPrediction.JackknifeMinMaxRegressor, Any, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.fit","text":"MMI.fit(conf_model::JackknifeMinMaxRegressor, verbosity, X, y)\n\nFor the JackknifeMinMaxRegressor nonconformity scores are computed in the same way as for the JackknifeRegressor. Specifically, we have,\n\nS_i^textLOO = s(X_i Y_i) = h(hatmu_-i(X_i) Y_i)  i in mathcalD_texttrain\n\nwhere hatmu_-i(X_i) denotes the leave-one-out prediction for X_i. In other words, for each training instance i=1n the model is trained on all training data excluding i. The fitted model is then used to predict out-of-sample from X_i. The corresponding nonconformity score is then computed by applying a heuristic uncertainty measure h(cdot) to the fitted value hatmu_-i(X_i) and the true value Y_i.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.fit-Tuple{ConformalPrediction.JackknifePlusAbMinMaxRegressor, Any, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.fit","text":"MMI.fit(conf_model::JackknifePlusMinMaxAbRegressor, verbosity, X, y)\n\nFor the JackknifePlusAbMinMaxRegressor nonconformity scores are as,\n\nS_i^textJ+MinMax = s(X_i Y_i) = h(agg(hatmu_B_K(-i)(X_i)) Y_i)  i in mathcalD_texttrain\n\nwhere agg(hatmu_B_K(-i)(X_i)) denotes the aggregate predictions, typically mean or median, for each X_i (with K_-i the bootstraps not containing X_i). In other words, B models are trained on boostrapped sampling, the fitted models are then used to create aggregated prediction of out-of-sample X_i. The corresponding nonconformity score is then computed by applying a heuristic uncertainty measure h(cdot) to the fitted value agg(hatmu_B_K(-i)(X_i)) and the true value Y_i.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.fit-Tuple{ConformalPrediction.JackknifePlusAbRegressor, Any, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.fit","text":"MMI.fit(conf_model::JackknifePlusAbRegressor, verbosity, X, y)\n\nFor the JackknifePlusAbRegressor nonconformity scores are computed as\n\n S_i^textJ+ab = s(X_i Y_i) = h(agg(hatmu_B_K(-i)(X_i)) Y_i)  i in mathcalD_texttrain \n\nwhere agg(hatmu_B_K(-i)(X_i)) denotes the aggregate predictions, typically mean or median, for each X_i (with K_-i the bootstraps not containing X_i). In other words, B models are trained on boostrapped sampling, the fitted models are then used to create aggregated prediction of out-of-sample X_i. The corresponding nonconformity score is then computed by applying a heuristic uncertainty measure h(cdot) to the fitted value agg(hatmu_B_K(-i)(X_i)) and the true value Y_i.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.fit-Tuple{ConformalPrediction.JackknifePlusRegressor, Any, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.fit","text":"MMI.fit(conf_model::JackknifePlusRegressor, verbosity, X, y)\n\nFor the JackknifePlusRegressor nonconformity scores are computed in the same way as for the JackknifeRegressor. Specifically, we have,\n\nS_i^textLOO = s(X_i Y_i) = h(hatmu_-i(X_i) Y_i)  i in mathcalD_texttrain\n\nwhere hatmu_-i(X_i) denotes the leave-one-out prediction for X_i. In other words, for each training instance i=1n the model is trained on all training data excluding i. The fitted model is then used to predict out-of-sample from X_i. The corresponding nonconformity score is then computed by applying a heuristic uncertainty measure h(cdot) to the fitted value hatmu_-i(X_i) and the true value Y_i.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.fit-Tuple{ConformalPrediction.JackknifeRegressor, Any, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.fit","text":"MMI.fit(conf_model::JackknifeRegressor, verbosity, X, y)\n\nFor the JackknifeRegressor nonconformity scores are computed through a leave-one-out (LOO) procedure as follows,\n\nS_i^textLOO = s(X_i Y_i) = h(hatmu_-i(X_i) Y_i)  i in mathcalD_texttrain\n\nwhere hatmu_-i(X_i) denotes the leave-one-out prediction for X_i. In other words, for each training instance i=1n the model is trained on all training data excluding i. The fitted model is then used to predict out-of-sample from X_i. The corresponding nonconformity score is then computed by applying a heuristic uncertainty measure h(cdot) to the fitted value hatmu_-i(X_i) and the true value Y_i.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.fit-Tuple{ConformalPrediction.NaiveClassifier, Any, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.fit","text":"MMI.fit(conf_model::NaiveClassifier, verbosity, X, y)\n\nFor the NaiveClassifier nonconformity scores are computed in-sample as follows:\n\nS_i^textIS = s(X_i Y_i) = h(hatmu(X_i) Y_i)  i in mathcalD_textcalibration\n\nA typical choice for the heuristic function is h(hatmu(X_i) Y_i)=1-hatmu(X_i)_Y_i where hatmu(X_i)_Y_i denotes the softmax output of the true class and hatmu denotes the model fitted on training data mathcalD_texttrain.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.fit-Tuple{ConformalPrediction.NaiveRegressor, Any, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.fit","text":"MMI.fit(conf_model::NaiveRegressor, verbosity, X, y)\n\nFor the NaiveRegressor nonconformity scores are computed in-sample as follows:\n\nS_i^textIS = s(X_i Y_i) = h(hatmu(X_i) Y_i)  i in mathcalD_texttrain\n\nA typical choice for the heuristic function is h(hatmu(X_i)Y_i)=Y_i-hatmu(X_i) where hatmu denotes the model fitted on training data mathcalD_texttrain.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.fit-Tuple{ConformalPrediction.SimpleInductiveClassifier, Any, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.fit","text":"MMI.fit(conf_model::SimpleInductiveClassifier, verbosity, X, y)\n\nFor the SimpleInductiveClassifier nonconformity scores are computed as follows:\n\nS_i^textCAL = s(X_i Y_i) = h(hatmu(X_i) Y_i)  i in mathcalD_textcalibration\n\nA typical choice for the heuristic function is h(hatmu(X_i) Y_i)=1-hatmu(X_i)_Y_i where hatmu(X_i)_Y_i denotes the softmax output of the true class and hatmu denotes the model fitted on training data mathcalD_texttrain. The simple approach only takes the softmax probability of the true label into account.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.fit-Tuple{ConformalPrediction.SimpleInductiveRegressor, Any, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.fit","text":"MMI.fit(conf_model::SimpleInductiveRegressor, verbosity, X, y)\n\nFor the SimpleInductiveRegressor nonconformity scores are computed as follows:\n\nS_i^textCAL = s(X_i Y_i) = h(hatmu(X_i) Y_i)  i in mathcalD_textcalibration\n\nA typical choice for the heuristic function is h(hatmu(X_i)Y_i)=Y_i-hatmu(X_i) where hatmu denotes the model fitted on training data mathcalD_texttrain.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.fit-Tuple{ConformalPrediction.TimeSeriesRegressorEnsembleBatch, Any, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.fit","text":"MMI.fit(conf_model::TimeSeriesRegressorEnsembleBatch, verbosity, X, y)\n\nFor the TimeSeriesRegressorEnsembleBatch nonconformity scores are computed as\n\n S_i^textJ+ab = s(X_i Y_i) = h(agg(hatmu_B_K(-i)(X_i)) Y_i)  i in mathcalD_texttrain \n\nwhere agg(hatmu_B_K(-i)(X_i)) denotes the aggregate predictions, typically mean or median, for each X_i (with K_-i the bootstraps not containing X_i). In other words, B models are trained on boostrapped sampling, the fitted models are then used to create aggregated prediction of out-of-sample X_i. The corresponding nonconformity score is then computed by applying a heuristic uncertainty measure h(cdot) to the fitted value agg(hatmu_B_K(-i)(X_i)) and the true value Y_i.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{ConformalPrediction.AdaptiveInductiveClassifier, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.predict","text":"MMI.predict(conf_model::AdaptiveInductiveClassifier, fitresult, Xnew)\n\nFor the AdaptiveInductiveClassifier prediction sets are computed as follows,\n\nhatC_nalpha(X_n+1) = lefty s(X_n+1y) le hatq_n alpha^+ S_i^textCAL right  i in mathcalD_textcalibration\n\nwhere mathcalD_textcalibration denotes the designated calibration data.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{ConformalPrediction.BayesRegressor, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.predict","text":"MMI.predict(conf_model::BayesRegressor, fitresult, Xnew)\n\nFor the BayesRegressor prediction sets are computed as follows,\n\nhatC_nalpha(X_n+1) = lefty f(yX_n+1) le -hatq_ alpha S_i^textCAL right  i in mathcalD_textcalibration\n\nwhere mathcalD_textcalibration denotes the designated calibration data.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{ConformalPrediction.CVMinMaxRegressor, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.predict","text":"MMI.predict(conf_model::CVMinMaxRegressor, fitresult, Xnew)\n\nFor the CVMinMaxRegressor prediction intervals are computed as follows,\n\nhatC_nalpha(X_n+1) = left min_i=1n hatmu_-mathcalD_k(i)(X_n+1) -  hatq_n alpha^+ S_i^textCV  max_i=1n hatmu_-mathcalD_k(i)(X_n+1) + hatq_n alpha^+  S_i^textCV right  i in mathcalD_texttrain\n\nwhere hatmu_-mathcalD_k(i) denotes the model fitted on training data with subset mathcalD_k(i) that contains the i th point removed.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{ConformalPrediction.CVPlusRegressor, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.predict","text":"MMI.predict(conf_model::CVPlusRegressor, fitresult, Xnew)\n\nFor the CVPlusRegressor prediction intervals are computed in much same way as for the JackknifePlusRegressor. Specifically, we have,\n\nhatC_nalpha(X_n+1) = left hatq_n alpha^- hatmu_-mathcalD_k(i)(X_n+1) - S_i^textCV  hatq_n alpha^+ hatmu_-mathcalD_k(i)(X_n+1) + S_i^textCV right   i in mathcalD_texttrain\n\nwhere hatmu_-mathcalD_k(i) denotes the model fitted on training data with fold mathcalD_k(i) that contains the i th point removed.\n\nThe JackknifePlusRegressor is a special case of the CVPlusRegressor for which K=n.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{ConformalPrediction.ConformalQuantileRegressor, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.predict","text":"MMI.predict(conf_model::ConformalQuantileRegressor, fitresult, Xnew)\n\nFor the ConformalQuantileRegressor prediction intervals are computed as follows,\n\nhatC_nalpha(X_n+1) = hatmu_alpha_lo(X_n+1) - hatq_n alpha S_i^textCAL  hatmu_alpha_hi(X_n+1) + hatq_n alpha S_i^textCAL   i in mathcalD_textcalibration\n\nwhere mathcalD_textcalibration denotes the designated calibration data.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{ConformalPrediction.JackknifeMinMaxRegressor, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.predict","text":"MMI.predict(conf_model::JackknifeMinMaxRegressor, fitresult, Xnew)\n\nFor the JackknifeMinMaxRegressor prediction intervals are computed as follows,\n\nhatC_nalpha(X_n+1) = left min_i=1n hatmu_-i(X_n+1) -  hatq_n alpha^+ S_i^textLOO  max_i=1n hatmu_-i(X_n+1) + hatq_n alpha^+ S_i^textLOO right   i in mathcalD_texttrain\n\nwhere hatmu_-i denotes the model fitted on training data with ith point removed. The jackknife-minmax procedure is more conservative than the JackknifePlusRegressor.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{ConformalPrediction.JackknifePlusAbMinMaxRegressor, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.predict","text":"MMI.predict(conf_model::JackknifePlusAbMinMaxRegressor, fitresult, Xnew)\n\nFor the JackknifePlusAbMinMaxRegressor prediction intervals are computed as follows,\n\nhatC_nalpha^J+MinMax(X_n+1) = left min_i=1n hatmu_-i(X_n+1) -  hatq_n alpha^+ S_i^textJ+MinMax  max_i=1n hatmu_-i(X_n+1) + hatq_n alpha^+ S_i^textJ+MinMax right   i in mathcalD_texttrain\n\nwhere hatmu_-i denotes the model fitted on training data with ith point removed. The jackknife+ab-minmax procedure is more conservative than the JackknifePlusAbRegressor.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{ConformalPrediction.JackknifePlusAbRegressor, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.predict","text":"MMI.predict(conf_model::JackknifePlusAbRegressor, fitresult, Xnew)\n\nFor the JackknifePlusAbRegressor prediction intervals are computed as follows,\n\nhatC_nalpha B^J+ab(X_n+1) = left hatq_n alpha^- hatmu_agg(-i)(X_n+1) - S_i^textJ+ab  hatq_n alpha^+ hatmu_agg(-i)(X_n+1) + S_i^textJ+ab right  i in mathcalD_texttrain\n\nwhere hatmu_agg(-i) denotes the aggregated models hatmu_1  hatmu_B fitted on bootstrapped data (B) does not include the ith data point. The jackknife+ procedure is more stable than the JackknifeRegressor.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{ConformalPrediction.JackknifePlusRegressor, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.predict","text":"MMI.predict(conf_model::JackknifePlusRegressor, fitresult, Xnew)\n\nFor the JackknifePlusRegressor prediction intervals are computed as follows,\n\nhatC_nalpha(X_n+1) = left hatq_n alpha^- hatmu_-i(X_n+1) - S_i^textLOO  hatq_n alpha^+ hatmu_-i(X_n+1) + S_i^textLOO right  i in mathcalD_texttrain\n\nwhere hatmu_-i denotes the model fitted on training data with ith point removed. The jackknife+ procedure is more stable than the JackknifeRegressor.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{ConformalPrediction.JackknifeRegressor, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.predict","text":"MMI.predict(conf_model::JackknifeRegressor, fitresult, Xnew)\n\nFor the JackknifeRegressor prediction intervals are computed as follows,\n\nhatC_nalpha(X_n+1) = hatmu(X_n+1) pm hatq_n alpha^+ S_i^textLOO  i in mathcalD_texttrain\n\nwhere S_i^textLOO denotes the nonconformity that is generated as explained in fit(conf_model::JackknifeRegressor, verbosity, X, y). The jackknife procedure addresses the overfitting issue associated with the NaiveRegressor.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{ConformalPrediction.NaiveClassifier, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.predict","text":"MMI.predict(conf_model::NaiveClassifier, fitresult, Xnew)\n\nFor the NaiveClassifier prediction sets are computed as follows:\n\nhatC_nalpha(X_n+1) = lefty s(X_n+1y) le hatq_n alpha^+ S_i^textIS  right  i in mathcalD_texttrain\n\nThe naive approach typically produces prediction regions that undercover due to overfitting.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{ConformalPrediction.NaiveRegressor, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.predict","text":"MMI.predict(conf_model::NaiveRegressor, fitresult, Xnew)\n\nFor the NaiveRegressor prediction intervals are computed as follows:\n\nhatC_nalpha(X_n+1) = hatmu(X_n+1) pm hatq_n alpha^+ S_i^textIS   i in mathcalD_texttrain\n\nThe naive approach typically produces prediction regions that undercover due to overfitting.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{ConformalPrediction.SimpleInductiveClassifier, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.predict","text":"MMI.predict(conf_model::SimpleInductiveClassifier, fitresult, Xnew)\n\nFor the SimpleInductiveClassifier prediction sets are computed as follows,\n\nhatC_nalpha(X_n+1) = lefty s(X_n+1y) le hatq_n alpha^+ S_i^textCAL right  i in mathcalD_textcalibration\n\nwhere mathcalD_textcalibration denotes the designated calibration data.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{ConformalPrediction.SimpleInductiveRegressor, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.predict","text":"MMI.predict(conf_model::SimpleInductiveRegressor, fitresult, Xnew)\n\nFor the SimpleInductiveRegressor prediction intervals are computed as follows,\n\nhatC_nalpha(X_n+1) = hatmu(X_n+1) pm hatq_n alpha^+ S_i^textCAL   i in mathcalD_textcalibration\n\nwhere mathcalD_textcalibration denotes the designated calibration data.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MLJModelInterface.predict-Tuple{ConformalPrediction.TimeSeriesRegressorEnsembleBatch, Any, Any}","page":"üßê Reference","title":"MLJModelInterface.predict","text":"MMI.predict(conf_model::TimeSeriesRegressorEnsembleBatch, fitresult, Xnew)\n\nFor the TimeSeriesRegressorEnsembleBatch prediction intervals are computed as follows,\n\nhatC_nalpha B^J+ab(X_n+1) = left hatq_n alpha^- hatmu_agg(-i)(X_n+1) - S_i^textJ+ab  hatq_n alpha^+ hatmu_agg(-i)(X_n+1) + S_i^textJ+ab right  i in mathcalD_texttrain\n\nwhere hatmu_agg(-i) denotes the aggregated models hatmu_1  hatmu_B fitted on bootstrapped data (B) does not include the ith data point. The jackknife+ procedure is more stable than the JackknifeRegressor.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.AdaptiveInductiveClassifier","page":"üßê Reference","title":"ConformalPrediction.AdaptiveInductiveClassifier","text":"The AdaptiveInductiveClassifier is an improvement to the SimpleInductiveClassifier and the NaiveClassifier. Contrary to the NaiveClassifier it computes nonconformity scores using a designated calibration dataset like the SimpleInductiveClassifier. Contrary to the SimpleInductiveClassifier it utilizes the softmax output of all classes.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.BayesRegressor","page":"üßê Reference","title":"ConformalPrediction.BayesRegressor","text":"The BayesRegressor is the simplest approach to Inductive Conformalized Bayes. As explained in https://arxiv.org/abs/2107.07511, the  conformal score is  defined as the opposite of the probability of observing y given x : s= -P(YX). Once the treshold hatq is chosen, The credible interval is then  computed as the range of y values so that   C(x)= bigy  P(YX)  -hatq big\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.CVMinMaxRegressor","page":"üßê Reference","title":"ConformalPrediction.CVMinMaxRegressor","text":"Constructor for CVMinMaxRegressor.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.CVPlusRegressor","page":"üßê Reference","title":"ConformalPrediction.CVPlusRegressor","text":"Constructor for CVPlusRegressor.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.ConformalInterval","page":"üßê Reference","title":"ConformalPrediction.ConformalInterval","text":"An abstract base type for conformal models that produce interval-valued predictions. This includes most conformal regression models.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.ConformalProbabilistic","page":"üßê Reference","title":"ConformalPrediction.ConformalProbabilistic","text":"An abstract base type for conformal models that produce probabilistic predictions. This includes some conformal classifier like Venn-ABERS.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.ConformalProbabilisticSet","page":"üßê Reference","title":"ConformalPrediction.ConformalProbabilisticSet","text":"An abstract base type for conformal models that produce set-valued probabilistic predictions. This includes most conformal classification models.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.ConformalQuantileRegressor","page":"üßê Reference","title":"ConformalPrediction.ConformalQuantileRegressor","text":"Constructor for ConformalQuantileRegressor.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.JackknifeMinMaxRegressor","page":"üßê Reference","title":"ConformalPrediction.JackknifeMinMaxRegressor","text":"Constructor for JackknifeMinMaxRegressor.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.JackknifePlusAbMinMaxRegressor","page":"üßê Reference","title":"ConformalPrediction.JackknifePlusAbMinMaxRegressor","text":"Constructor for JackknifePlusAbMinMaxRegressor.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.JackknifePlusAbRegressor","page":"üßê Reference","title":"ConformalPrediction.JackknifePlusAbRegressor","text":"Constructor for JackknifePlusAbPlusRegressor.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.JackknifePlusRegressor","page":"üßê Reference","title":"ConformalPrediction.JackknifePlusRegressor","text":"Constructor for JackknifePlusRegressor.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.JackknifeRegressor","page":"üßê Reference","title":"ConformalPrediction.JackknifeRegressor","text":"Constructor for JackknifeRegressor.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.NaiveClassifier","page":"üßê Reference","title":"ConformalPrediction.NaiveClassifier","text":"The NaiveClassifier is the simplest approach to Inductive Conformal Classification. Contrary to the NaiveClassifier it computes nonconformity scores using a designated training dataset.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.NaiveRegressor","page":"üßê Reference","title":"ConformalPrediction.NaiveRegressor","text":"The NaiveRegressor for conformal prediction is the simplest approach to conformal regression.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.QuantileModel","page":"üßê Reference","title":"ConformalPrediction.QuantileModel","text":"Union type for quantile models.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.SimpleInductiveClassifier","page":"üßê Reference","title":"ConformalPrediction.SimpleInductiveClassifier","text":"The SimpleInductiveClassifier is the simplest approach to Inductive Conformal Classification. Contrary to the NaiveClassifier it computes nonconformity scores using a designated calibration dataset.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.SimpleInductiveRegressor","page":"üßê Reference","title":"ConformalPrediction.SimpleInductiveRegressor","text":"The SimpleInductiveRegressor is the simplest approach to Inductive Conformal Regression. Contrary to the NaiveRegressor it computes nonconformity scores using a designated calibration dataset.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction.TimeSeriesRegressorEnsembleBatch","page":"üßê Reference","title":"ConformalPrediction.TimeSeriesRegressorEnsembleBatch","text":"Constructor for TimeSeriesRegressorEnsembleBatch.\n\n\n\n\n\n","category":"type"},{"location":"reference/#ConformalPrediction._aggregate-Tuple{Any, Union{String, Symbol}}","page":"üßê Reference","title":"ConformalPrediction._aggregate","text":"_aggregate(y, aggregate::Union{Symbol,String})\n\nHelper function that performs aggregation across vector of predictions.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.absolute_error-Tuple{Any, Any}","page":"üßê Reference","title":"ConformalPrediction.absolute_error","text":"absolute_error(y,yÃÇ)\n\nComputes abs(y - yÃÇ) where yÃÇ is the predicted value.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.blockbootstrap-Tuple{Any, Any}","page":"üßê Reference","title":"ConformalPrediction.blockbootstrap","text":"blockbootstrap(time_series_data, block_szie)\n\nGenerate a sampling method, that block bootstraps the given data\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.compute_interval-Tuple{Any, Any, Any}","page":"üßê Reference","title":"ConformalPrediction.compute_interval","text":"compute_interval(fŒº, fvar, qÃÇ)\n\ncompute the credible interval for a treshold score of qÃÇ under the assumption that each data point pdf is a gaussian distribution with mean fŒº and variance fvar.     More precisely, assuming that f(x) is the pdf of a Gaussian, it computes the values of x so that f(x) > -q, or equivalently ln(f(x))> ln(-q). The end result is the interval [ Œº -  \\sigma\\sqrt{-2 *ln(-q \\sigma \\sqrt{2 \\pi} )},Œº +  \\sigma\\sqrt{-2 *ln(-q \\sigma \\sqrt{2 \\pi} )}]\n\ninputs:\n    - fŒº array of the mean values\n    - fvar array of the variance values\n    - qÃÇ the treshold.\n\nreturn:\n    -  hcat(lower_bound, upper_bound) where lower_bound and upper_bound are,respectively, the arrays of the lower bounds and upper bounds for each data point.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.conformal_bayes_score-Tuple{Any, Any, Any}","page":"üßê Reference","title":"ConformalPrediction.conformal_bayes_score","text":"ConformalBayes(y, fŒº, fvar)\n\ncomputes the conformal score as the negative of probability of observing a value y given a Gaussian distribution with mean fŒº and a variance œÉ^2 N(y|fŒº,œÉ^2). In other words, it computes  -1/(sqrt(œÉ 2œÄ)) * e^[-(y- fŒº)^2/(2*œÉ^2)]\n\ninputs:\n    - y  the true values of the calibration set.\n    - fŒº array of the mean values\n    - fvar array of the variance values\n\nreturn:\n    -  the probability of observing a value y given a mean fŒº and a variance fvar.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.is_classification-Tuple{Any}","page":"üßê Reference","title":"ConformalPrediction.is_classification","text":"is_classification(yÃÇ)\n\nHelper function that checks if conformal prediction yÃÇ comes from a conformal classification model.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.is_covered-Tuple{Any, Any}","page":"üßê Reference","title":"ConformalPrediction.is_covered","text":"is_covered(yÃÇ, y)\n\nHelper function to check if y is contained in conformal region. Based on whether conformal predictions yÃÇ are set- or interval-valued, different checks are executed.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.is_covered_interval-Tuple{Any, Any}","page":"üßê Reference","title":"ConformalPrediction.is_covered_interval","text":"is_covered_interval(yÃÇ, y)\n\nHelper function to check if y is contained in conformal interval.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.is_covered_set-Tuple{Any, Any}","page":"üßê Reference","title":"ConformalPrediction.is_covered_set","text":"is_covered_set(yÃÇ, y)\n\nHelper function to check if y is contained in conformal set.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.is_regression-Tuple{Any}","page":"üßê Reference","title":"ConformalPrediction.is_regression","text":"is_regression(yÃÇ)\n\nHelper function that checks if conformal prediction yÃÇ comes from a conformal regression model.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.minus_softmax-Tuple{Any, Any}","page":"üßê Reference","title":"ConformalPrediction.minus_softmax","text":"minus_softmax(y,yÃÇ)\n\nComputes 1.0 - yÃÇ where yÃÇ is the softmax output for a given class.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.qminus","page":"üßê Reference","title":"ConformalPrediction.qminus","text":"qminus(v::AbstractArray, coverage::AbstractFloat=0.9)\n\nImplements the hatq_nalpha^- finite-sample corrected quantile function as defined in Barber et al. (2020): https://arxiv.org/pdf/1905.02928.pdf. \n\n\n\n\n\n","category":"function"},{"location":"reference/#ConformalPrediction.qplus","page":"üßê Reference","title":"ConformalPrediction.qplus","text":"qplus(v::AbstractArray, coverage::AbstractFloat=0.9)\n\nImplements the hatq_nalpha^+ finite-sample corrected quantile function as defined in Barber et al. (2020): https://arxiv.org/pdf/1905.02928.pdf. \n\n\n\n\n\n","category":"function"},{"location":"reference/#ConformalPrediction.reformat_interval-Tuple{Any}","page":"üßê Reference","title":"ConformalPrediction.reformat_interval","text":"reformat_interval(yÃÇ)\n\nReformats conformal interval predictions.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.reformat_mlj_prediction-Tuple{Any}","page":"üßê Reference","title":"ConformalPrediction.reformat_mlj_prediction","text":"reformat_mlj_prediction(yÃÇ)\n\nA helper function that extracts only the output (predicted values) for whatever is returned from MMI.predict(model, fitresult, Xnew). This is currently used to avoid issues when calling MMI.predict(model, fitresult, Xnew) in pipelines.\n\n\n\n\n\n","category":"method"},{"location":"reference/#ConformalPrediction.score","page":"üßê Reference","title":"ConformalPrediction.score","text":"score(conf_model::SimpleInductiveClassifier, ::Type{<:Supervised}, fitresult, X, y::Union{Nothing,AbstractArray}=nothing)\n\nScore method for the SimpleInductiveClassifier dispatched for any <:Supervised model.\n\n\n\n\n\n","category":"function"},{"location":"reference/#ConformalPrediction.score-2","page":"üßê Reference","title":"ConformalPrediction.score","text":"score(conf_model::ConformalProbabilisticSet, fitresult, X, y=nothing)\n\nGeneric score method for the ConformalProbabilisticSet. It computes nonconformity scores using the heuristic function h and the softmax probabilities of the true class. Method is dispatched for different Conformal Probabilistic Sets and atomic models.\n\n\n\n\n\n","category":"function"},{"location":"reference/#ConformalPrediction.score-3","page":"üßê Reference","title":"ConformalPrediction.score","text":"score(conf_model::AdaptiveInductiveClassifier, ::Type{<:Supervised}, fitresult, X, y::Union{Nothing,AbstractArray}=nothing)\n\nScore method for the AdaptiveInductiveClassifier dispatched for any <:Supervised model.\n\n\n\n\n\n","category":"function"},{"location":"explanation/finite_sample_correction/#Finite-sample-Correction","page":"Finite-sample Correction","title":"Finite-sample Correction","text":"We follow the convention used in Angelopoulos and Bates (2021) and Barber et al. (2021) to correct for the finite-sample bias of the empirical quantile. Specifically, we use the following definition of the (1‚àíŒ±) empirical quantile:\n\nhatq_nalpha^+v = fraclceil (n+1)(1-alpha)rceiln\n\nBarber et al. (2021) further define as the Œ± empirical quantile:\n\nhatq_nalpha^-v = fraclfloor (n+1)alpha rfloorn = - hatq_nalpha^+-v\n\nBelow we test this equality numerically by generating a large number of random vectors and comparing the two quantiles. We then plot the density of the difference between the two quantiles. While the errors are small, they are not negligible for small n. In our computations, we use qÃÇ(n,‚ÄÜŒ±)‚Åª{v} exactly as it is defined above, rather than relying on ‚ÄÖ‚àí‚ÄÖqÃÇ(n,‚ÄÜŒ±)‚Å∫{‚ÄÖ‚àí‚ÄÖv}.\n\nusing ConformalPrediction: qplus, qminus\nnobs = [100, 1000, 10000]\nn = 1000\nalpha = 0.1\nplts = []\nŒî = Float32[]\nfor _nobs in nobs\n    for i in 1:n\n        v = rand(_nobs)\n        Œ¥ = qminus(v, alpha) - (-qplus(-v, 1-alpha))\n        push!(Œî, Œ¥)\n    end\n    plt = density(Œî)\n    vline!([mean(Œî)], color=:red, label=\"mean\")\n    push!(plts, plt)\nend\nplot(plts..., layout=(1,3), size=(900, 300), legend=:topleft, title=[\"nobs = 100\" \"nobs = 1000\" \"nobs = 10000\"])\n\n(Image: )\n\nSee also this related discussion.","category":"section"},{"location":"explanation/finite_sample_correction/#References","page":"Finite-sample Correction","title":"References","text":"Angelopoulos, Anastasios N., and Stephen Bates. 2021. ‚ÄúA Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification.‚Äù https://arxiv.org/abs/2107.07511.\n\nBarber, Rina Foygel, Emmanuel J. Cand√®s, Aaditya Ramdas, and Ryan J. Tibshirani. 2021. ‚ÄúPredictive Inference with the Jackknife+.‚Äù The Annals of Statistics 49 (1): 486‚Äì507. https://doi.org/10.1214/20-AOS1965.","category":"section"},{"location":"how_to_guides/llm/#How-to-Build-a-Conformal-Chatbot","page":"How to Conformalize a Large Language Model","title":"How to Build a Conformal Chatbot","text":"Large Language Models are all the buzz right now. They are used for a variety of tasks, including text classification, question answering, and text generation. In this tutorial, we will show how to conformalize a transformer language model for text classification. We will use the Banking77 dataset (Casanueva et al. 2020), which consists of 13,083 queries from 77 intents. On the model side, we will use the DistilRoBERTa model, which is a distilled version of RoBERTa (Liu et al. 2019) finetuned on the Banking77 dataset.","category":"section"},{"location":"how_to_guides/llm/#Data","page":"How to Conformalize a Large Language Model","title":"Data","text":"The data was downloaded from HuggingFace ü§ó (HF) and split into a proper training, calibration, and test set. All that‚Äôs left to do is to load the data and preprocess it. We add 1 to the labels to make them 1-indexed (sorry Pythonistas üòú)\n\n# Get labels:\ndf_labels = CSV.read(\"dev/artifacts/data/banking77/labels.csv\", DataFrame, drop=[1])\nlabels = df_labels[:,1]\n\n# Get data:\ndf_train = CSV.read(\"dev/artifacts/data/banking77/train.csv\", DataFrame, drop=[1])\ndf_cal = CSV.read(\"dev/artifacts/data/banking77/calibration.csv\", DataFrame, drop=[1])\ndf_full_train = vcat(df_train, df_cal)\ntrain_ratio = round(nrow(df_train)/nrow(df_full_train), digits=2)\ndf_test = CSV.read(\"dev/artifacts/data/banking77/test.csv\", DataFrame, drop=[1])\n\n# Preprocess data:\nqueries_train, y_train = collect(df_train.text), categorical(df_train.labels .+ 1)\nqueries_cal, y_cal = collect(df_cal.text), categorical(df_cal.labels .+ 1)\nqueries, y = collect(df_full_train.text), categorical(df_full_train.labels .+ 1)\nqueries_test, y_test = collect(df_test.text), categorical(df_test.labels .+ 1)","category":"section"},{"location":"how_to_guides/llm/#HuggingFace-Model","page":"How to Conformalize a Large Language Model","title":"HuggingFace Model","text":"The model can be loaded from HF straight into our running Julia session using the Transformers.jl package. Below we load the tokenizer tkr and the model mod. The tokenizer is used to convert the text into a sequence of integers, which is then fed into the model. The model outputs a hidden state, which is then fed into a classifier to get the logits for each class. Finally, the logits are then passed through a softmax function to get the corresponding predicted probabilities. Below we run a few queries through the model to see how it performs.\n\n# Load model from HF ü§ó:\ntkr = hgf\"mrm8488/distilroberta-finetuned-banking77:tokenizer\"\nmod = hgf\"mrm8488/distilroberta-finetuned-banking77:ForSequenceClassification\"\n\n# Test model:\nquery = [\n    \"What is the base of the exchange rates?\",\n    \"Why is my card not working?\",\n    \"My Apple Pay is not working, what should I do?\",\n]\na = encode(tkr, query)\nb = mod.model(a)\nc = mod.cls(b.hidden_state)\nd = softmax(c.logit)\n[labels[i] for i in Flux.onecold(d)]\n\n3-element Vector{String}:\n \"exchange_rate\"\n \"card_not_working\"\n \"apple_pay_or_google_pay\"","category":"section"},{"location":"how_to_guides/llm/#MLJ-Interface","page":"How to Conformalize a Large Language Model","title":"MLJ Interface","text":"Since our package is interfaced to MLJ.jl, we need to define a wrapper model that conforms to the MLJ interface. In order to add the model for general use, we would probably go through MLJFlux.jl, but for this tutorial, we will make our life easy and simply overload the MLJBase.fit and MLJBase.predict methods. Since the model from HF is already pre-trained and we are not interested in further fine-tuning, we will simply return the model object in the MLJBase.fit method. The MLJBase.predict method will then take the model object and the query and return the predicted probabilities. We also need to define the MLJBase.target_scitype and MLJBase.predict_mode methods. The former tells MLJ what the output type of the model is, and the latter can be used to retrieve the label with the highest predicted probability.\n\nstruct IntentClassifier <: MLJBase.Probabilistic\n    tkr::TextEncoders.AbstractTransformerTextEncoder\n    mod::HuggingFace.HGFRobertaForSequenceClassification\nend\n\nfunction IntentClassifier(;\n    tokenizer::TextEncoders.AbstractTransformerTextEncoder, \n    model::HuggingFace.HGFRobertaForSequenceClassification,\n)\n    IntentClassifier(tkr, mod)\nend\n\nfunction get_hidden_state(clf::IntentClassifier, query::Union{AbstractString, Vector{<:AbstractString}})\n    token = encode(clf.tkr, query)\n    hidden_state = clf.mod.model(token).hidden_state\n    return hidden_state\nend\n\n# This doesn't actually retrain the model, but it retrieves the classifier object\nfunction MLJBase.fit(clf::IntentClassifier, verbosity, X, y)\n    cache=nothing\n    report=nothing\n    fitresult = (clf = clf.mod.cls, labels = levels(y))\n    return fitresult, cache, report\nend\n\nfunction MLJBase.predict(clf::IntentClassifier, fitresult, Xnew)\n    output = fitresult.clf(get_hidden_state(clf, Xnew))\n    pÃÇ = UnivariateFinite(fitresult.labels,softmax(output.logit)',pool=missing)\n    return pÃÇ\nend\n\nMLJBase.target_scitype(clf::IntentClassifier) = AbstractVector{<:Finite}\n\nMLJBase.predict_mode(clf::IntentClassifier, fitresult, Xnew) = mode.(MLJBase.predict(clf, fitresult, Xnew))\n\nTo test that everything is working as expected, we fit the model and generated predictions for a subset of the test data:\n\nclf = IntentClassifier(tkr, mod)\ntop_n = 10\nfitresult, _, _ = MLJBase.fit(clf, 1, nothing, y_test[1:top_n])\n@time yÃÇ = MLJBase.predict(clf, fitresult, queries_test[1:top_n]);","category":"section"},{"location":"how_to_guides/llm/#Conformal-Chatbot","page":"How to Conformalize a Large Language Model","title":"Conformal Chatbot","text":"To turn the wrapped, pre-trained model into a conformal intent classifier, we can now rely on standard API calls. We first wrap our atomic model where we also specify the desired coverage rate and method. Since even simple forward passes are computationally expensive for our (small) LLM, we rely on Simple Inductive Conformal Classification.\n\n#| eval: false\n\nconf_model = conformal_model(clf; coverage=0.95, method=:simple_inductive, train_ratio=train_ratio)\nmach = machine(conf_model, queries, y)\n@time fit!(mach)\nSerialization.serialize(\"dev/artifacts/models/banking77/simple_inductive.jls\", mach)\n\nFinally, we use our conformal LLM to build a simple and yet powerful chatbot that runs directly in the Julia REPL. Without dwelling on the details too much, the conformal_chatbot works as follows:\n\nPrompt user to explain their intent.\nFeed user input through conformal LLM and present the output to the user.\nIf the conformal prediction sets includes more than one label, prompt the user to either refine their input or choose one of the options included in the set.\n\nmach = Serialization.deserialize(\"dev/artifacts/models/banking77/simple_inductive.jls\")\n\nfunction prediction_set(mach, query::String)\n    pÃÇ = MLJBase.predict(mach, query)[1]\n    probs = pdf.(pÃÇ, collect(1:77))\n    in_set = findall(probs .!= 0)\n    labels_in_set = labels[in_set]\n    probs_in_set = probs[in_set]\n    _order = sortperm(-probs_in_set)\n    plt = UnicodePlots.barplot(labels_in_set[_order], probs_in_set[_order], title=\"Possible Intents\")\n    return labels_in_set, plt\nend\n\nfunction conformal_chatbot()\n    println(\"üëã Hi, I'm a Julia, your conformal chatbot. I'm here to help you with your banking query. Ask me anything or type 'exit' to exit ...\\n\")\n    completed = false\n    queries = \"\"\n    while !completed\n        query = readline()\n        queries = queries * \",\" * query\n        labels, plt = prediction_set(mach, queries)\n        if length(labels) > 1\n            println(\"ü§î Hmmm ... I can think of several options here. If any of these applies, simply type the corresponding number (e.g. '1' for the first option). Otherwise, can you refine your question, please?\\n\")\n            println(plt)\n        else\n            println(\"ü•≥ I think you mean $(labels[1]). Correct?\")\n        end\n\n        # Exit:\n        if query == \"exit\"\n            println(\"üëã Bye!\")\n            break\n        end\n        if query ‚àà string.(collect(1:77))\n            println(\"üëç Great! You've chosen '$(labels[parse(Int64, query)])'. I'm glad I could help you. Have a nice day!\")\n            completed = true\n        end\n    end\nend\n\nBelow we show the output for two example queries. The first one is very ambiguous. As expected, the size of the prediction set is therefore large.\n\nambiguous_query = \"transfer mondey?\"\nprediction_set(mach, ambiguous_query)[2]\n\n                                                        Possible Intents              \n                                           ‚îå                                        ‚îê \n                   beneficiary_not_allowed ‚î§‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 0.150517   \n   balance_not_updated_after_bank_transfer ‚î§‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 0.111409           \n                     transfer_into_account ‚î§‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 0.0939535             \n        transfer_not_received_by_recipient ‚î§‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 0.091163               \n            top_up_by_bank_transfer_charge ‚î§‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 0.089306               \n                           failed_transfer ‚î§‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 0.0888322              \n                           transfer_timing ‚î§‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 0.0641952                   \n                      transfer_fee_charged ‚î§‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 0.0361131                         \n                          pending_transfer ‚î§‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 0.0270795                           \n                           receiving_money ‚î§‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 0.0252126                           \n                         declined_transfer ‚î§‚ñ†‚ñ†‚ñ† 0.0164443                             \n                           cancel_transfer ‚î§‚ñ†‚ñ†‚ñ† 0.0150444                             \n                                           ‚îî                                        ‚îò\n\nThe more refined version of the prompt yields a smaller prediction set: less ambiguous prompts result in lower predictive uncertainty.\n\nrefined_query = \"I tried to transfer money to my friend, but it failed.\"\nprediction_set(mach, refined_query)[2]\n\n                                                        Possible Intents              \n                                           ‚îå                                        ‚îê \n                           failed_transfer ‚î§‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 0.59042   \n                   beneficiary_not_allowed ‚î§‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 0.139806                          \n        transfer_not_received_by_recipient ‚î§‚ñ†‚ñ† 0.0449783                              \n   balance_not_updated_after_bank_transfer ‚î§‚ñ†‚ñ† 0.037894                               \n                         declined_transfer ‚î§‚ñ† 0.0232856                               \n                     transfer_into_account ‚î§‚ñ† 0.0108771                               \n                           cancel_transfer ‚î§ 0.00876369                               \n                                           ‚îî                                        ‚îò\n\nBelow we include a short demo video that shows the REPL-based chatbot in action.\n\n(Image: )","category":"section"},{"location":"how_to_guides/llm/#Final-Remarks","page":"How to Conformalize a Large Language Model","title":"Final Remarks","text":"This work was done in collaboration with colleagues at ING as part of the ING Analytics 2023 Experiment Week. Our team demonstrated that Conformal Prediction provides a powerful and principled alternative to top-K intent classification. We won the first prize by popular vote.","category":"section"},{"location":"how_to_guides/llm/#References","page":"How to Conformalize a Large Language Model","title":"References","text":"Casanueva, I√±igo, Tadas Temƒçinas, Daniela Gerz, Matthew Henderson, and Ivan Vuliƒá. 2020. ‚ÄúEfficient Intent Detection with Dual Sentence Encoders.‚Äù In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, 38‚Äì45. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.nlp4convai-1.5.\n\nLiu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. ‚ÄúRoBERTa: A Robustly Optimized BERT Pretraining Approach.‚Äù arXiv. https://doi.org/10.48550/arXiv.1907.11692.","category":"section"},{"location":"#ConformalPrediction","page":"üè† Home","title":"ConformalPrediction","text":"(Image: )\n\nDocumentation for ConformalPrediction.jl.\n\nConformalPrediction.jl is a package for Predictive Uncertainty Quantification (UQ) through Conformal Prediction (CP) in Julia. It is designed to work with supervised models trained in MLJ (Blaom et al. 2020). Conformal Prediction is easy-to-understand, easy-to-use and model-agnostic and it works under minimal distributional assumptions.","category":"section"},{"location":"#Quick-Tour","page":"üè† Home","title":"üèÉ Quick Tour","text":"First time here? Take a quick interactive tour to see what this package can do right on JuliaHub (To run the notebook, hit login and then edit).\n\nThis Pluto.jl üéà notebook won the 2nd Price in the JuliaCon 2023 Notebook Competition.","category":"section"},{"location":"#Local-Tour","page":"üè† Home","title":"Local Tour","text":"To run the tour locally, just clone this repo and start Pluto.jl as follows:\n\n] add Pluto\nusing Pluto\nPluto.run()\n\nAll notebooks are contained in docs/pluto.","category":"section"},{"location":"#Background","page":"üè† Home","title":"üìñ Background","text":"Don‚Äôt worry, we‚Äôre not about to deep-dive into methodology. But just to give you a high-level description of Conformal Prediction (CP) upfront:\n\nConformal prediction (a.k.a. conformal inference) is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions.‚Äî Angelopoulos and Bates (2021)\n\nIntuitively, CP works under the premise of turning heuristic notions of uncertainty into rigorous uncertainty estimates through repeated sampling or the use of dedicated calibration data.\n\n(Image: Conformal Prediction in action: prediction intervals at varying coverage rates. As coverage grows, so does the width of the prediction interval.)\n\nThe animation above is lifted from a small blog post that introduces Conformal Prediction and this package in the context of regression. It shows how the prediction interval and the test points that it covers varies in size as the user-specified coverage rate changes.","category":"section"},{"location":"#Installation","page":"üè† Home","title":"üö© Installation","text":"You can install the latest stable release from the general registry:\n\nusing Pkg\nPkg.add(\"ConformalPrediction\")\n\nThe development version can be installed as follows:\n\nusing Pkg\nPkg.add(url=\"https://github.com/juliatrustworthyai/ConformalPrediction.jl\")","category":"section"},{"location":"#Usage-Example","page":"üè† Home","title":"üîç Usage Example","text":"To illustrate the intended use of the package, let‚Äôs have a quick look at a simple regression problem. We first generate some synthetic data and then determine indices for our training and test data using MLJ:\n\nusing MLJ\n\n# Inputs:\nN = 600\nxmax = 3.0\nusing Distributions\nd = Uniform(-xmax, xmax)\nX = rand(d, N)\nX = reshape(X, :, 1)\n\n# Outputs:\nnoise = 0.5\nfun(X) = sin(X)\nŒµ = randn(N) .* noise\ny = @.(fun(X)) + Œµ\ny = vec(y)\n\n# Partition:\ntrain, test = partition(eachindex(y), 0.4, 0.4, shuffle=true)\n\nWe then import Symbolic Regressor (SymbolicRegression.jl) following the standard MLJ procedure.\n\nregressor = @load SRRegressor pkg=SymbolicRegression\nmodel = regressor(\n    niterations=50,\n    binary_operators=[+, -, *],\n    unary_operators=[sin],\n)\n\nTo turn our conventional model into a conformal model, we just need to declare it as such by using conformal_model wrapper function. The generated conformal model instance can wrapped in data to create a machine. Finally, we proceed by fitting the machine on training data using the generic fit! method:\n\nusing ConformalPrediction\nconf_model = conformal_model(model)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)\n\nPredictions can then be computed using the generic predict method. The code below produces predictions for the first n samples. Each tuple contains the lower and upper bound for the prediction interval.\n\nshow_first = 5\nXtest = selectrows(X, test)\nytest = y[test]\nyÃÇ = predict(mach, Xtest)\nyÃÇ[1:show_first]\n\n5-element Vector{Tuple{Float64, Float64}}:\n (-0.04087262272113379, 1.8635644669554758)\n (0.04647464096907805, 1.9509117306456876)\n (-0.24248802236397216, 1.6619490673126376)\n (-0.07841928163933476, 1.8260178080372749)\n (-0.02268628324126465, 1.881750806435345)\n\nFor simple models like this one, we can call a custom Plots recipe on our instance, fit result and data to generate the chart below:\n\nusing Plots\nzoom = 0\nplt = plot(mach.model, mach.fitresult, Xtest, ytest, lw=5, zoom=zoom, observed_lab=\"Test points\")\nxrange = range(-xmax+zoom,xmax-zoom,length=N)\nplot!(plt, xrange, @.(fun(xrange)), lw=2, ls=:dash, colour=:darkorange, label=\"Ground truth\")\n\n(Image: )\n\nWe can evaluate the conformal model using the standard MLJ workflow with a custom performance measure. You can use either emp_coverage for the overall empirical coverage (correctness) or ssc for the size-stratified coverage rate (adaptiveness).\n\n_eval = evaluate!(mach; measure=[emp_coverage, ssc], verbosity=0)\ndisplay(_eval)\nprintln(\"Empirical coverage: $(round(_eval.measurement[1], digits=3))\")\nprintln(\"SSC: $(round(_eval.measurement[2], digits=3))\")\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚îÇ measure                                      ‚îÇ operation ‚îÇ measurement ‚îÇ 1.9 ‚ãØ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚îÇ ConformalPrediction.emp_coverage             ‚îÇ predict   ‚îÇ 0.953       ‚îÇ 0.0 ‚ãØ\n‚îÇ ConformalPrediction.size_stratified_coverage ‚îÇ predict   ‚îÇ 0.953       ‚îÇ 0.0 ‚ãØ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                                                               2 columns omitted\n\nEmpirical coverage: 0.953\nSSC: 0.953","category":"section"},{"location":"#Read-on","page":"üè† Home","title":"üìö Read on","text":"If after reading the usage example above you are just left with more questions about the topic, that‚Äôs normal. Below we have have collected a number of further resources to help you get started with this package and the topic itself:\n\nBlog post introducing conformal classifiers: [Quarto], [TDS], [Forem].\nBlog post applying CP to a deep learning image classifier: [Quarto], [TDS], [Forem].\nThe package docs and in particular the FAQ.","category":"section"},{"location":"#External-Resources","page":"üè† Home","title":"External Resources","text":"A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification by Angelopoulos and Bates (2021) (pdf).\nPredictive inference with the jackknife+ by Barber et al. (2021) (pdf)\nAwesome Conformal Prediction repository by Valery Manokhin (repo).\nDocumentation for the Python package MAPIE.","category":"section"},{"location":"#Status","page":"üè† Home","title":"üîÅ Status","text":"This package is in its early stages of development and therefore still subject to changes to the core architecture and API.","category":"section"},{"location":"#Implemented-Methodologies","page":"üè† Home","title":"Implemented Methodologies","text":"The following CP approaches have been implemented:\n\nRegression:\n\nInductive\nNaive Transductive\nJackknife\nJackknife+\nJackknife-minmax\nCV+\nCV-minmax\n\nClassification:\n\nInductive\nNaive Transductive\nAdaptive Inductive\n\nThe package has been tested for the following supervised models offered by MLJ.\n\nRegression:\n\nkeys(tested_atomic_models[:regression])\n\nKeySet for a Dict{Symbol, Expr} with 5 entries. Keys:\n  :ridge\n  :lasso\n  :evo_tree\n  :nearest_neighbor\n  :linear\n\nClassification:\n\nkeys(tested_atomic_models[:classification])\n\nKeySet for a Dict{Symbol, Expr} with 3 entries. Keys:\n  :nearest_neighbor\n  :evo_tree\n  :logistic","category":"section"},{"location":"#Implemented-Evaluation-Metrics","page":"üè† Home","title":"Implemented Evaluation Metrics","text":"To evaluate conformal predictors we are typically interested in correctness and adaptiveness. The former can be evaluated by looking at the empirical coverage rate, while the latter can be assessed through metrics that address the conditional coverage (Angelopoulos and Bates 2021). To this end, the following metrics have been implemented:\n\nemp_coverage (empirical coverage)\nssc (size-stratified coverage)\n\nThere is also a simple Plots.jl recipe that can be used to inspect the set sizes. In the regression case, the interval width is stratified into discrete bins for this purpose:\n\nbar(mach.model, mach.fitresult, X)\n\n(Image: )","category":"section"},{"location":"#Contribute","page":"üè† Home","title":"üõ† Contribute","text":"Contributions are welcome! A good place to start is the list of outstanding issues. For more details, see also the Contributor‚Äôs Guide. Please follow the SciML ColPrac guide.","category":"section"},{"location":"#Thanks","page":"üè† Home","title":"üôè Thanks","text":"To build this package I have read and re-read both Angelopoulos and Bates (2021) and Barber et al. (2021). The Awesome Conformal Prediction repository (Manokhin, n.d.) has also been a fantastic place to get started. Thanks also to @aangelopoulos, @valeman and others for actively contributing to discussions on here. Quite a few people have also recently started using and contributing to the package for which I am very grateful. Finally, many thanks to Anthony Blaom (@ablaom) for many helpful discussions about how to interface this package to MLJ.jl.","category":"section"},{"location":"#References","page":"üè† Home","title":"üéì References","text":"Angelopoulos, Anastasios N., and Stephen Bates. 2021. ‚ÄúA Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification.‚Äù https://arxiv.org/abs/2107.07511.\n\nBarber, Rina Foygel, Emmanuel J. Cand√®s, Aaditya Ramdas, and Ryan J. Tibshirani. 2021. ‚ÄúPredictive Inference with the Jackknife+.‚Äù The Annals of Statistics 49 (1): 486‚Äì507. https://doi.org/10.1214/20-AOS1965.\n\nBlaom, Anthony D., Franz Kiraly, Thibaut Lienart, Yiannis Simillides, Diego Arenas, and Sebastian J. Vollmer. 2020. ‚ÄúMLJ: A Julia Package for Composable Machine Learning.‚Äù Journal of Open Source Software 5 (55): 2704. https://doi.org/10.21105/joss.02704.\n\nManokhin, Valery. n.d. ‚ÄúAwesome Conformal Prediction.‚Äù","category":"section"}]
}
