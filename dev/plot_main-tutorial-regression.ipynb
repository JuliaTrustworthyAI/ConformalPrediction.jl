{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Tutorial for tabular regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we compare the prediction intervals estimated by MAPIE on a\nsimple, one-dimensional, ground truth function\n$f(x) = x \\times \\sin(x)$.\n\nThroughout this tutorial, we will answer the following questions:\n\n- How well do the MAPIE strategies capture the aleatoric uncertainty\n  existing in the data?\n\n- How do the prediction intervals estimated by the resampling strategies\n  evolve for new *out-of-distribution* data ?\n\n- How do the prediction intervals vary between regressor models ?\n\nThroughout this tutorial, we estimate the prediction intervals first using\na polynomial function, and then using a boosting model, and a simple neural\nnetwork.\n\n**For practical problems, we advise using the faster CV+ or\nJackknife+-after-Bootstrap strategies.\nFor conservative prediction interval estimates, you can alternatively\nuse the CV-minmax strategies.**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport subprocess\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom mapie.metrics import regression_coverage_score\nfrom mapie.regression import MapieRegressor\nfrom mapie.quantile_regression import MapieQuantileRegressor\nfrom mapie.subsample import Subsample\nfrom sklearn.linear_model import LinearRegression, QuantileRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nwarnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Estimating the aleatoric uncertainty of homoscedastic noisy data\n\nLet's start by defining the $x \\times \\sin(x)$ function and another\nsimple function that generates one-dimensional data with normal noise\nuniformely in a given interval.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def x_sinx(x):\n    \"\"\"One-dimensional x*sin(x) function.\"\"\"\n    return x*np.sin(x)\n\n\ndef get_1d_data_with_constant_noise(funct, min_x, max_x, n_samples, noise):\n    \"\"\"\n    Generate 1D noisy data uniformely from the given function\n    and standard deviation for the noise.\n    \"\"\"\n    np.random.seed(59)\n    X_train = np.linspace(min_x, max_x, n_samples)\n    np.random.shuffle(X_train)\n    X_test = np.linspace(min_x, max_x, n_samples*5)\n    y_train, y_mesh, y_test = funct(X_train), funct(X_test), funct(X_test)\n    y_train += np.random.normal(0, noise, y_train.shape[0])\n    y_test += np.random.normal(0, noise, y_test.shape[0])\n    return (\n        X_train.reshape(-1, 1), y_train, X_test.reshape(-1, 1), y_test, y_mesh\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first generate noisy one-dimensional data uniformely on an interval.\nHere, the noise is considered as *homoscedastic*, since it remains constant\nover $x$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "min_x, max_x, n_samples, noise = -5, 5, 600, 0.5\nX_train, y_train, X_test, y_test, y_mesh = get_1d_data_with_constant_noise(\n    x_sinx, min_x, max_x, n_samples, noise\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize our noisy function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.scatter(X_train, y_train, color=\"C0\")\n_ = plt.plot(X_test, y_mesh, color=\"C1\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned previously, we fit our training data with a simple\npolynomial function. Here, we choose a degree equal to 10 so the function\nis able to perfectly fit $x \\times \\sin(x)$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "degree_polyn = 10\npolyn_model = Pipeline(\n    [\n        (\"poly\", PolynomialFeatures(degree=degree_polyn)),\n        (\"linear\", LinearRegression())\n    ]\n)\npolyn_model_quant = Pipeline(\n    [\n        (\"poly\", PolynomialFeatures(degree=degree_polyn)),\n        (\"linear\", QuantileRegressor(\n                solver=\"highs\",\n                alpha=0,\n        ))\n    ]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then estimate the prediction intervals for all the strategies very easily\nwith a\n`fit` and `predict` process. The prediction interval's lower and upper bounds\nare then saved in a DataFrame. Here, we set an alpha value of 0.05\nin order to obtain a 95% confidence for our prediction intervals.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "STRATEGIES = {\n    \"naive\": dict(method=\"naive\"),\n    \"jackknife\": dict(method=\"base\", cv=-1),\n    \"jackknife_plus\": dict(method=\"plus\", cv=-1),\n    \"jackknife_minmax\": dict(method=\"minmax\", cv=-1),\n    \"cv\": dict(method=\"base\", cv=10),\n    \"cv_plus\": dict(method=\"plus\", cv=10),\n    \"cv_minmax\": dict(method=\"minmax\", cv=10),\n    \"jackknife_plus_ab\": dict(method=\"plus\", cv=Subsample(n_resamplings=50)),\n    \"jackknife_minmax_ab\": dict(\n        method=\"minmax\", cv=Subsample(n_resamplings=50)\n    ),\n    \"conformalized_quantile_regression\": dict(\n        method=\"quantile\", cv=\"split\", alpha=0.05\n    )\n}\ny_pred, y_pis = {}, {}\nfor strategy, params in STRATEGIES.items():\n    if strategy == \"conformalized_quantile_regression\":\n        mapie = MapieQuantileRegressor(polyn_model_quant, **params)\n        mapie.fit(X_train, y_train, random_state=1)\n        y_pred[strategy], y_pis[strategy] = mapie.predict(X_test)\n    else:\n        mapie = MapieRegressor(polyn_model, **params)\n        mapie.fit(X_train, y_train)\n        y_pred[strategy], y_pis[strategy] = mapie.predict(X_test, alpha=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u2019s now compare the target confidence intervals with the predicted\nintervals obtained with the Jackknife+, Jackknife-minmax, CV+, CV-minmax,\nJackknife+-after-Boostrap, and conformalized quantile regression (CQR)\nstrategies. Note that for the Jackknife-after-Bootstrap method, we call the\n:class:`~mapie.subsample.Subsample` object that allows us to train\nbootstrapped models. Note also that the CQR method is called with\n:class:`~mapie.quantile_regression.MapieQuantileRegressor` with a\n\"split\" strategy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_1d_data(\n    X_train,\n    y_train,\n    X_test,\n    y_test,\n    y_sigma,\n    y_pred,\n    y_pred_low,\n    y_pred_up,\n    ax=None,\n    title=None\n):\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.fill_between(X_test, y_pred_low, y_pred_up, alpha=0.3)\n    ax.scatter(X_train, y_train, color=\"red\", alpha=0.3, label=\"Training data\")\n    ax.plot(X_test, y_test, color=\"gray\", label=\"True confidence intervals\")\n    ax.plot(X_test, y_test - y_sigma, color=\"gray\", ls=\"--\")\n    ax.plot(X_test, y_test + y_sigma, color=\"gray\", ls=\"--\")\n    ax.plot(\n        X_test, y_pred, color=\"blue\", alpha=0.5, label=\"Prediction intervals\"\n    )\n    if title is not None:\n        ax.set_title(title)\n    ax.legend()\n\n\nstrategies = [\n    \"jackknife_plus\",\n    \"jackknife_minmax\",\n    \"cv_plus\",\n    \"cv_minmax\",\n    \"jackknife_plus_ab\",\n    \"conformalized_quantile_regression\"\n]\nn_figs = len(strategies)\nfig, axs = plt.subplots(3, 2, figsize=(9, 13))\ncoords = [axs[0, 0], axs[0, 1], axs[1, 0], axs[1, 1], axs[2, 0], axs[2, 1]]\nfor strategy, coord in zip(strategies, coords):\n    plot_1d_data(\n        X_train.ravel(),\n        y_train.ravel(),\n        X_test.ravel(),\n        y_mesh.ravel(),\n        np.full((X_test.shape[0]), 1.96*noise).ravel(),\n        y_pred[strategy].ravel(),\n        y_pis[strategy][:, 0, 0].ravel(),\n        y_pis[strategy][:, 1, 0].ravel(),\n        ax=coord,\n        title=strategy\n    )\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At first glance, the four strategies give similar results and the\nprediction intervals are very close to the true confidence intervals.\nLet\u2019s confirm this by comparing the prediction interval widths over\n$x$ between all strategies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(9, 5))\nax.axhline(1.96*2*noise, ls=\"--\", color=\"k\", label=\"True width\")\nfor strategy in STRATEGIES:\n    ax.plot(\n        X_test,\n        y_pis[strategy][:, 1, 0] - y_pis[strategy][:, 0, 0],\n        label=strategy\n    )\nax.set_xlabel(\"x\")\nax.set_ylabel(\"Prediction Interval Width\")\nax.legend(fontsize=8)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, the prediction intervals estimated by the Naive method\nare slightly too narrow. The Jackknife, Jackknife+, CV, CV+, JaB, and J+aB\ngive\nsimilar widths that are very close to the true width. On the other hand,\nthe width estimated by Jackknife-minmax and CV-minmax are slightly too\nwide. Note that the widths given by the Naive, Jackknife, and CV strategies\nare constant because there is a single model used for prediction,\nperturbed models are ignored at prediction time.\n\nIt's interesting to observe that CQR strategy offers more varying width,\noften giving much higher but also lower interval width than other methods,\ntherefore,\nwith homoscedastic noise, CQR would not be the preferred method.\n\nLet\u2019s now compare the *effective* coverage, namely the fraction of test\npoints whose true values lie within the prediction intervals, given by\nthe different strategies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.DataFrame([\n    [\n        regression_coverage_score(\n            y_test, y_pis[strategy][:, 0, 0], y_pis[strategy][:, 1, 0]\n        ),\n        (\n            y_pis[strategy][:, 1, 0] - y_pis[strategy][:, 0, 0]\n        ).mean()\n    ] for strategy in STRATEGIES\n], index=STRATEGIES, columns=[\"Coverage\", \"Width average\"]).round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All strategies except the Naive one give effective coverage close to the\nexpected 0.95 value (recall that alpha = 0.05), confirming the theoretical\ngarantees.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Estimating the aleatoric uncertainty of heteroscedastic noisy data\n\nLet's define again the $x \\times \\sin(x)$ function and another simple\nfunction that generates one-dimensional data with normal noise uniformely\nin a given interval.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_1d_data_with_heteroscedastic_noise(\n    funct, min_x, max_x, n_samples, noise\n):\n    \"\"\"\n    Generate 1D noisy data uniformely from the given function\n    and standard deviation for the noise.\n    \"\"\"\n    np.random.seed(59)\n    X_train = np.linspace(min_x, max_x, n_samples)\n    np.random.shuffle(X_train)\n    X_test = np.linspace(min_x, max_x, n_samples*5)\n    y_train = (\n        funct(X_train) +\n        (np.random.normal(0, noise, len(X_train)) * X_train)\n    )\n    y_test = (\n        funct(X_test) +\n        (np.random.normal(0, noise, len(X_test)) * X_test)\n    )\n    y_mesh = funct(X_test)\n    return (\n        X_train.reshape(-1, 1), y_train, X_test.reshape(-1, 1), y_test, y_mesh\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first generate noisy one-dimensional data uniformely on an interval.\nHere, the noise is considered as *heteroscedastic*, since it will increase\nlinearly with $x$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "min_x, max_x, n_samples, noise = 0, 5, 300, 0.5\n(\n    X_train, y_train, X_test, y_test, y_mesh\n) = get_1d_data_with_heteroscedastic_noise(\n    x_sinx, min_x, max_x, n_samples, noise\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize our noisy function. As x increases, the data becomes more\nnoisy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.scatter(X_train, y_train, color=\"C0\")\nplt.plot(X_test, y_mesh, color=\"C1\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned previously, we fit our training data with a simple\npolynomial function. Here, we choose a degree equal to 10 so the function\nis able to perfectly fit $x \\times \\sin(x)$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "degree_polyn = 10\npolyn_model = Pipeline(\n    [\n        (\"poly\", PolynomialFeatures(degree=degree_polyn)),\n        (\"linear\", LinearRegression())\n    ]\n)\npolyn_model_quant = Pipeline(\n    [\n        (\"poly\", PolynomialFeatures(degree=degree_polyn)),\n        (\"linear\", QuantileRegressor(\n                solver=\"highs\",\n                alpha=0,\n        ))\n    ]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then estimate the prediction intervals for all the strategies very easily\nwith a\n`fit` and `predict` process. The prediction interval's lower and upper bounds\nare then saved in a DataFrame. Here, we set an alpha value of 0.05\nin order to obtain a 95% confidence for our prediction intervals.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "STRATEGIES = {\n    \"naive\": dict(method=\"naive\"),\n    \"jackknife\": dict(method=\"base\", cv=-1),\n    \"jackknife_plus\": dict(method=\"plus\", cv=-1),\n    \"jackknife_minmax\": dict(method=\"minmax\", cv=-1),\n    \"cv\": dict(method=\"base\", cv=10),\n    \"cv_plus\": dict(method=\"plus\", cv=10),\n    \"cv_minmax\": dict(method=\"minmax\", cv=10),\n    \"jackknife_plus_ab\": dict(method=\"plus\", cv=Subsample(n_resamplings=50)),\n    \"conformalized_quantile_regression\": dict(\n        method=\"quantile\", cv=\"split\", alpha=0.05\n    )\n}\ny_pred, y_pis = {}, {}\nfor strategy, params in STRATEGIES.items():\n    if strategy == \"conformalized_quantile_regression\":\n        mapie = MapieQuantileRegressor(polyn_model_quant, **params)\n        mapie.fit(X_train, y_train, random_state=1)\n        y_pred[strategy], y_pis[strategy] = mapie.predict(X_test)\n    else:\n        mapie = MapieRegressor(polyn_model, **params)\n        mapie.fit(X_train, y_train)\n        y_pred[strategy], y_pis[strategy] = mapie.predict(X_test, alpha=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once again, let\u2019s compare the target confidence intervals with prediction\nintervals obtained with the Jackknife+, Jackknife-minmax, CV+, CV-minmax,\nJackknife+-after-Boostrap, and CQR strategies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "strategies = [\n    \"jackknife_plus\",\n    \"jackknife_minmax\",\n    \"cv_plus\",\n    \"cv_minmax\",\n    \"jackknife_plus_ab\",\n    \"conformalized_quantile_regression\"\n]\nn_figs = len(strategies)\nfig, axs = plt.subplots(3, 2, figsize=(9, 13))\ncoords = [axs[0, 0], axs[0, 1], axs[1, 0], axs[1, 1], axs[2, 0], axs[2, 1]]\nfor strategy, coord in zip(strategies, coords):\n    plot_1d_data(\n        X_train.ravel(),\n        y_train.ravel(),\n        X_test.ravel(),\n        y_mesh.ravel(),\n        (1.96*noise*X_test).ravel(),\n        y_pred[strategy].ravel(),\n        y_pis[strategy][:, 0, 0].ravel(),\n        y_pis[strategy][:, 1, 0].ravel(),\n        ax=coord,\n        title=strategy\n    )\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can observe that all of the strategies except CQR seem to have similar\nconstant prediction intervals.\nOn the other hand, the CQR strategy offers a solution that adapts the\nprediction intervals to the local noise.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\nax.plot(X_test, 1.96*2*noise*X_test, ls=\"--\", color=\"k\", label=\"True width\")\nfor strategy in STRATEGIES:\n    ax.plot(\n        X_test,\n        y_pis[strategy][:, 1, 0] - y_pis[strategy][:, 0, 0],\n        label=strategy\n    )\nax.set_xlabel(\"x\")\nax.set_ylabel(\"Prediction Interval Width\")\nax.legend(fontsize=8)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One can observe that all the strategies behave in a similar way as in the\nfirst example shown previously. One exception is the CQR method which takes\ninto account the heteroscedasticity of the data. In this method we observe\nvery low interval widths at low values of $x$.\nThis is the only method that\neven slightly follows the true width, and therefore is the preferred method\nfor heteroscedastic data. Notice also that the true width is greater (lower)\nthan the predicted width from the other methods at $x \\gtrapprox 3$`\n($x \\leq 3$). This means that while the marginal coverage correct for\nthese methods, the conditional coverage is likely not guaranteed as we will\nobserve in the next figure.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_heteroscedastic_coverage(y_test, y_pis, STRATEGIES, bins):\n    recap = {}\n    for i in range(len(bins)-1):\n        bin1, bin2 = bins[i], bins[i+1]\n        name = f\"[{bin1}, {bin2}]\"\n        recap[name] = []\n        for strategy in STRATEGIES:\n            indices = np.where((X_test >= bins[i]) * (X_test <= bins[i+1]))\n            y_test_trunc = np.take(y_test, indices)\n            y_low_ = np.take(y_pis[strategy][:, 0, 0], indices)\n            y_high_ = np.take(y_pis[strategy][:, 1, 0], indices)\n            score_coverage = regression_coverage_score(\n                y_test_trunc[0], y_low_[0], y_high_[0]\n            )\n            recap[name].append(score_coverage)\n    recap_df = pd.DataFrame(recap, index=STRATEGIES)\n    return recap_df\n\n\nbins = [0, 1, 2, 3, 4, 5]\nheteroscedastic_coverage = get_heteroscedastic_coverage(\n    y_test, y_pis, STRATEGIES, bins\n)\n\n# fig = plt.figure()\nheteroscedastic_coverage.T.plot.bar(figsize=(12, 5), alpha=0.7)\nplt.axhline(0.95, ls=\"--\", color=\"k\")\nplt.ylabel(\"Conditional coverage\")\nplt.xlabel(\"x bins\")\nplt.xticks(rotation=0)\nplt.ylim(0.8, 1.0)\nplt.legend(fontsize=8, loc=[0, 0])\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u2019s now conclude by summarizing the *effective* coverage, namely the\nfraction of test\npoints whose true values lie within the prediction intervals, given by\nthe different strategies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.DataFrame([\n    [\n        regression_coverage_score(\n            y_test, y_pis[strategy][:, 0, 0], y_pis[strategy][:, 1, 0]\n        ),\n        (\n            y_pis[strategy][:, 1, 0] - y_pis[strategy][:, 0, 0]\n        ).mean()\n    ] for strategy in STRATEGIES\n], index=STRATEGIES, columns=[\"Coverage\", \"Width average\"]).round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All the strategies have the wanted coverage, however, we notice that the CQR\nstrategy has much lower interval width than all the other methods, therefore,\nwith heteroscedastic noise, CQR would be the preferred method.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Estimating the epistemic uncertainty of out-of-distribution data\n\nLet\u2019s now consider one-dimensional data without noise, but normally\ndistributed.\nThe goal is to explore how the prediction intervals evolve for new data\nthat lie outside the distribution of the training data in order to see how\nthe strategies can capture the *epistemic* uncertainty.\nFor a comparison of the epistemic and aleatoric uncertainties, please have\na look at this source:\nhttps://en.wikipedia.org/wiki/Uncertainty_quantification.\n\nLet's start by generating and showing the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_1d_data_with_normal_distrib(funct, mu, sigma, n_samples, noise):\n    \"\"\"\n    Generate noisy 1D data with normal distribution from given function\n    and noise standard deviation.\n    \"\"\"\n    np.random.seed(59)\n    X_train = np.random.normal(mu, sigma, n_samples)\n    X_test = np.arange(mu-4*sigma, mu+4*sigma, sigma/20.)\n    y_train, y_mesh, y_test = funct(X_train), funct(X_test), funct(X_test)\n    y_train += np.random.normal(0, noise, y_train.shape[0])\n    y_test += np.random.normal(0, noise, y_test.shape[0])\n    return (\n        X_train.reshape(-1, 1), y_train, X_test.reshape(-1, 1), y_test, y_mesh\n    )\n\n\nmu, sigma, n_samples, noise = 0, 2, 1000, 0.\nX_train, y_train, X_test, y_test, y_mesh = get_1d_data_with_normal_distrib(\n    x_sinx, mu, sigma, n_samples, noise\n)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.scatter(X_train, y_train, color=\"C0\")\n_ = plt.plot(X_test, y_test, color=\"C1\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As before, we estimate the prediction intervals using a polynomial\nfunction of degree 10 and show the results for the Jackknife+ and CV+\nstrategies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "polyn_model_quant = Pipeline(\n    [\n        (\"poly\", PolynomialFeatures(degree=degree_polyn)),\n        (\"linear\", QuantileRegressor(\n                solver=\"highs-ds\",\n                alpha=0,\n        ))\n    ]\n)\nSTRATEGIES = {\n    \"naive\": dict(method=\"naive\"),\n    \"jackknife\": dict(method=\"base\", cv=-1),\n    \"jackknife_plus\": dict(method=\"plus\", cv=-1),\n    \"jackknife_minmax\": dict(method=\"minmax\", cv=-1),\n    \"cv\": dict(method=\"base\", cv=10),\n    \"cv_plus\": dict(method=\"plus\", cv=10),\n    \"cv_minmax\": dict(method=\"minmax\", cv=10),\n    \"jackknife_plus_ab\": dict(method=\"plus\", cv=Subsample(n_resamplings=50)),\n    \"jackknife_minmax_ab\": dict(\n        method=\"minmax\", cv=Subsample(n_resamplings=50)\n    ),\n    \"conformalized_quantile_regression\": dict(\n        method=\"quantile\", cv=\"split\", alpha=0.05\n    )\n}\ny_pred, y_pis = {}, {}\nfor strategy, params in STRATEGIES.items():\n    if strategy == \"conformalized_quantile_regression\":\n        mapie = MapieQuantileRegressor(polyn_model_quant, **params)\n        mapie.fit(X_train, y_train, random_state=1)\n        y_pred[strategy], y_pis[strategy] = mapie.predict(X_test)\n    else:\n        mapie = MapieRegressor(polyn_model, **params)\n        mapie.fit(X_train, y_train)\n        y_pred[strategy], y_pis[strategy] = mapie.predict(X_test, alpha=0.05)\n\nstrategies = [\n    \"jackknife_plus\",\n    \"jackknife_minmax\",\n    \"cv_plus\",\n    \"cv_minmax\",\n    \"jackknife_plus_ab\",\n    \"conformalized_quantile_regression\"\n]\nn_figs = len(strategies)\nfig, axs = plt.subplots(3, 2, figsize=(9, 13))\ncoords = [axs[0, 0], axs[0, 1], axs[1, 0], axs[1, 1], axs[2, 0], axs[2, 1]]\nfor strategy, coord in zip(strategies, coords):\n    plot_1d_data(\n        X_train.ravel(),\n        y_train.ravel(),\n        X_test.ravel(),\n        y_mesh.ravel(),\n        1.96*noise,\n        y_pred[strategy].ravel(),\n        y_pis[strategy][:, 0, :].ravel(),\n        y_pis[strategy][:, 1, :].ravel(),\n        ax=coord,\n        title=strategy\n    )\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At first glance, our polynomial function does not give accurate\npredictions with respect to the true function when $|x| > 6$.\nThe prediction intervals estimated with the Jackknife+ do not seem to\nincrease. On the other hand, the CV and other related methods seem to capture\nsome uncertainty when $x > 6$.\n\nLet's now compare the prediction interval widths between all strategies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\nax.set_yscale(\"log\")\nfor strategy in STRATEGIES:\n    ax.plot(\n        X_test,\n        y_pis[strategy][:, 1, 0] - y_pis[strategy][:, 0, 0],\n        label=strategy\n    )\nax.set_xlabel(\"x\")\nax.set_ylabel(\"Prediction Interval Width\")\nax.legend(fontsize=8)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The prediction interval widths start to increase exponentially\nfor $|x| > 4$ for the CV+, CV-minmax, Jackknife-minmax, and quantile\nstrategies. On the other hand, the prediction intervals estimated by\nJackknife+ remain roughly constant until $|x| \\approx 5$ before\nincreasing.\nThe CQR strategy seems to perform well, however, on the extreme values\nof the data the quantile regression fails to give reliable results as it\noutputs\nnegative value for the prediction intervals. This occurs because the quantile\nregressor with quantile $1 - \\alpha/2$ gives higher values than the\nquantile regressor with quantile $\\alpha/2$. Note that a warning will\nbe issued when this occurs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.DataFrame([\n    [\n        regression_coverage_score(\n            y_test, y_pis[strategy][:, 0, 0], y_pis[strategy][:, 1, 0]\n        ),\n        (\n            y_pis[strategy][:, 1, 0] - y_pis[strategy][:, 0, 0]\n        ).mean()\n    ] for strategy in STRATEGIES\n], index=STRATEGIES, columns=[\"Coverage\", \"Width average\"]).round(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In conclusion, the Jackknife-minmax, CV+, CV-minmax, or Jackknife-minmax-ab\nstrategies are more\nconservative than the Jackknife+ strategy, and tend to result in more\nreliable coverages for *out-of-distribution* data. It is therefore\nadvised to use the three former strategies for predictions with new\nout-of-distribution data.\nNote however that there are no theoretical guarantees on the coverage level\nfor out-of-distribution data.\nHere it's important to note that the CQR strategy should not be taken into\naccount for width prediction, and it is abundantly clear from the negative\nwidth coverage that is observed in these results.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Estimating the uncertainty with different sklearn-compatible regressors\n\nMAPIE can be used with any kind of sklearn-compatible regressor. Here, we\nillustrate this by comparing the prediction intervals estimated by the CV+\nmethod using different models:\n\n* the same polynomial function as before.\n\n* a XGBoost model using the Scikit-learn API.\n\n* a simple neural network, a Multilayer Perceptron with three dense layers,\n  using the KerasRegressor wrapper.\n\nOnce again, let\u2019s use our noisy one-dimensional data obtained from a\nuniform distribution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subprocess.run(\"pip install scikeras\", shell=True)\nsubprocess.run(\"pip install tensorflow\", shell=True)\nsubprocess.run(\"pip install xgboost\", shell=True)\n\nfrom scikeras.wrappers import KerasRegressor  # noqa: E402\nfrom tensorflow.keras import Sequential  # noqa: E402\nfrom tensorflow.keras.layers import Dense  # noqa: E402\nfrom xgboost import XGBRegressor  # noqa: E402\n\n\nmin_x, max_x, n_samples, noise = -5, 5, 100, 0.5\nX_train, y_train, X_test, y_test, y_mesh = get_1d_data_with_constant_noise(\n    x_sinx, min_x, max_x, n_samples, noise\n)\n\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.plot(X_test, y_mesh, color=\"C1\")\n_ = plt.scatter(X_train, y_train)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's then define the models. The boosing model considers 100 shallow\ntrees with a max depth of 2 while the Multilayer Perceptron has two hidden\ndense layers with 20 neurons each followed by a relu activation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def mlp():\n    \"\"\"\n    Two-layer MLP model\n    \"\"\"\n    model = Sequential([\n        Dense(units=20, input_shape=(1,), activation=\"relu\"),\n        Dense(units=20, activation=\"relu\"),\n        Dense(units=1)\n    ])\n    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n    return model\n\n\npolyn_model = Pipeline(\n    [\n        (\"poly\", PolynomialFeatures(degree=degree_polyn)),\n        (\"linear\", LinearRegression())\n    ]\n)\n\nxgb_model = XGBRegressor(\n    max_depth=2,\n    n_estimators=100,\n    tree_method=\"hist\",\n    random_state=59,\n    learning_rate=0.1,\n    verbosity=0,\n    nthread=-1\n)\nmlp_model = KerasRegressor(\n    build_fn=mlp,\n    epochs=500,\n    verbose=0\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now use MAPIE to estimate the prediction intervals using the CV+\nmethod and compare their prediction interval.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "models = [polyn_model, xgb_model, mlp_model]\nmodel_names = [\"polyn\", \"xgb\", \"mlp\"]\nfor name, model in zip(model_names, models):\n    mapie = MapieRegressor(model, method=\"plus\", cv=10)\n    mapie.fit(X_train, y_train)\n    y_pred[name], y_pis[name] = mapie.predict(X_test, alpha=0.05)\n\nfig, axs = plt.subplots(1, 3, figsize=(20, 6))\nfor name, ax in zip(model_names, axs):\n    plot_1d_data(\n        X_train.ravel(),\n        y_train.ravel(),\n        X_test.ravel(),\n        y_mesh.ravel(),\n        1.96*noise,\n        y_pred[name].ravel(),\n        y_pis[name][:, 0, 0].ravel(),\n        y_pis[name][:, 1, 0].ravel(),\n        ax=ax,\n        title=name\n    )\nplt.show()\n\n\nfig, ax = plt.subplots(1, 1, figsize=(7, 5))\nfor name in model_names:\n    ax.plot(X_test, y_pis[name][:, 1, 0] - y_pis[name][:, 0, 0])\nax.axhline(1.96*2*noise, ls=\"--\", color=\"k\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"Prediction Interval Width\")\nax.legend(model_names + [\"True width\"], fontsize=8)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected with the CV+ method, the prediction intervals are a bit\nconservative since they are slightly wider than the true intervals.\nHowever, the CV+ method on the three models gives very promising results\nsince the prediction intervals closely follow the true intervals with\n$x$.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}