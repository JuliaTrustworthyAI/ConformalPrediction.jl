{
  "hash": "34358001c3f8363312cf01a952052809",
  "result": {
    "markdown": "```@meta\nCurrentModule = ConformalPrediction\n```\n\n# ConformalPrediction\n\nDocumentation for [ConformalPrediction.jl](https://github.com/pat-alt/ConformalPrediction.jl).\n\n\n\n`ConformalPrediction.jl` is a package for Uncertainty Quantification (UQ) through Conformal Prediction (CP) in Julia. It is designed to work with supervised models trained in [MLJ](https://alan-turing-institute.github.io/MLJ.jl/dev/) [@blaom2020mlj]. Conformal Prediction is distribution-free, easy-to-understand, easy-to-use and model-agnostic. \n\n# ğŸ“– Background\n\nConformal Prediction is a scalable frequentist approach to uncertainty quantification and coverage control. It promises to be an easy-to-understand, distribution-free and model-agnostic way to generate statistically rigorous uncertainty estimates. Interestingly, it can even be used to complement Bayesian methods.\n\nThe animation below is lifted from a small blog post that introduces the topic and the package ([[TDS](https://towardsdatascience.com/conformal-prediction-in-julia-351b81309e30)], [[Quarto](https://www.paltmeyer.com/blog/posts/conformal-prediction/#fig-anim)]). It shows conformal prediction sets for two different samples and changing coverage rates. Standard conformal classifiers produce set-valued predictions: for ambiguous samples these sets are typically large (for high coverage) or empty (for low coverage).\n\n![Conformal Prediction in action: Prediction sets for two different samples and changing coverage rates. As coverage grows, so does the size of the prediction sets.](https://raw.githubusercontent.com/pat-alt/blog/main/posts/conformal-prediction/www/medium.gif)\n\n## ğŸš© Installation \n\nYou can install the latest stable release from the general registry:\n\n```{.julia}\nusing Pkg\nPkg.add(\"ConformalPrediction\")\n```\n\nThe development version can be installed as follows:\n\n```{.julia}\nusing Pkg\nPkg.add(url=\"https://github.com/pat-alt/ConformalPrediction.jl\")\n```\n\n## ğŸ” Usage Example \n\nTo illustrate the intended use of the package, let's have a quick look at a simple regression problem. We first generate some synthetic data and then determine indices for our training and test data using [MLJ](https://alan-turing-institute.github.io/MLJ.jl/dev/):\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing MLJ\n\n# Inputs:\nN = 600\nxmax = 3.0\nusing Distributions\nd = Uniform(-xmax, xmax)\nX = rand(d, N)\nX = reshape(X, :, 1)\n\n# Outputs:\nnoise = 0.5\nfun(X) = X * sin(X)\nÎµ = randn(N) .* noise\ny = @.(fun(X)) + Îµ\ny = vec(y)\n\n# Partition:\ntrain, test = partition(eachindex(y), 0.4, 0.4, shuffle=true)\n```\n:::\n\n\nWe then import a decision-tree based regressor ([`EvoTrees.jl`](https://github.com/Evovest/EvoTrees.jl)) following the standard [MLJ](https://alan-turing-institute.github.io/MLJ.jl/dev/) procedure.\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nEvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees\nmodel = EvoTreeRegressor(rounds=100) \n```\n:::\n\n\nTo turn our conventional model into a conformal model, we just need to declare it as such by using `conformal_model` wrapper function. The generated conformal model instance can wrapped in data to create a *machine*. Finally, we proceed by fitting the machine on training data using the generic `fit!` method:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nusing ConformalPrediction\nconf_model = conformal_model(model; method=:jackknife_plus)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)\n```\n:::\n\n\nPredictions can then be computed using the generic `predict` method. The code below produces predictions for the first `n` samples. Each tuple contains the lower and upper bound for the prediction interval. \n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nshow_first = 5\nXtest = selectrows(X, test)\nytest = y[test]\nyÌ‚ = predict(mach, Xtest)\nyÌ‚[1:show_first]\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n5-element Vector{Tuple{Float64, Float64}}:\n (0.5113539995719073, 2.7791173590180245)\n (0.15501260477711076, 2.491986075800726)\n (-0.32783606947941524, 1.9302674946467009)\n (-0.13732511816023366, 2.141708832043786)\n (0.5089900787456267, 2.7771571126470387)\n```\n:::\n:::\n\n\nFor simple models like this one, we can call `Plots.plot` on our instance, fit result and data to generate the chart below:\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\nusing Plots\nzoom = -0.5\nplt = plot(mach.model, mach.fitresult, Xtest, ytest, zoom=zoom, observed_lab=\"Test points\")\nxrange = range(-xmax+zoom,xmax-zoom,length=N)\nplot!(plt, xrange, @.(fun(xrange)), lw=1, ls=:dash, colour=:black, label=\"Ground truth\")\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n![](index_files/figure-commonmark/cell-7-output-1.svg){}\n:::\n:::\n\n\nWe can evaluate the conformal model using the standard [MLJ](https://alan-turing-institute.github.io/MLJ.jl/dev/) workflow with a custom performance measure. You can use either `emp_coverage` for the overall empirical coverage (correctness) or `ssc` for the size-stratified coverage rate (adaptiveness).\n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\n_eval = evaluate!(mach; measure=[emp_coverage, ssc], verbosity=0)\ndisplay(_eval)\nprintln(\"Empirical coverage: $(round(_eval.measurement[1], digits=3))\")\nprintln(\"SSC: $(round(_eval.measurement[2], digits=3))\")\n```\n\n::: {.cell-output .cell-output-display}\n```\nPerformanceEvaluation object with these fields:\n  measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows\nExtract:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€\nâ”‚ measure                                                   â”‚ operation â”‚ meas â‹¯\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€\nâ”‚ emp_coverage (generic function with 1 method)             â”‚ predict   â”‚ 0.94 â‹¯\nâ”‚ size_stratified_coverage (generic function with 1 method) â”‚ predict   â”‚ 0.76 â‹¯\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€\n                                                               3 columns omitted\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEmpirical coverage: 0.947\nSSC: 0.762\n```\n:::\n:::\n\n\n## ğŸ” Status \n\nThis package is in its early stages of development and therefore still subject to changes to the core architecture and API. \n\n### Implemented Methodologies\n\nThe following CP approaches have been implemented:\n\n**Regression**:\n\n- Inductive \n- Naive Transductive \n- Jackknife \n- Jackknife+ \n- Jackknife-minmax\n- CV+\n- CV-minmax\n\n**Classification**:\n\n- Inductive (LABEL [@sadinle2019least])\n- Naive Transductive \n- Adaptive Inductive\n\nThe package has been tested for the following supervised models offered by [MLJ](https://alan-turing-institute.github.io/MLJ.jl/dev/).\n\n**Regression**:\n\n::: {.cell execution_count=8}\n``` {.julia .cell-code}\nkeys(tested_atomic_models[:regression])\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```\nKeySet for a Dict{Symbol, Expr} with 5 entries. Keys:\n  :nearest_neighbor\n  :evo_tree\n  :light_gbm\n  :linear\n  :decision_tree\n```\n:::\n:::\n\n\n**Classification**:\n\n::: {.cell execution_count=9}\n``` {.julia .cell-code}\nkeys(tested_atomic_models[:classification])\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\nKeySet for a Dict{Symbol, Expr} with 5 entries. Keys:\n  :nearest_neighbor\n  :evo_tree\n  :light_gbm\n  :decision_tree\n  :logistic\n```\n:::\n:::\n\n\n### Implemented Evaluation Metrics\n\nTo evaluate conformal predictors we are typically interested in correctness and adaptiveness. The former can be evaluated by looking at the empirical coverage rate, while the latter can be assessed through metrics that address the conditional coverage [@angelopoulos2021gentle]. To this end, the following metrics have been implemented:\n\n- `emp_coverage` (empirical coverage)\n- `ssc` (size-stratified coverage)\n\nThere is also a simple `Plots.jl` recipe that can be used to inspect the set sizes. In the regression case, the interval width is stratified into discrete bins for this purpose:\n\n::: {.cell execution_count=10}\n``` {.julia .cell-code}\nbar(mach.model, mach.fitresult, X)\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n![](index_files/figure-commonmark/cell-11-output-1.svg){}\n:::\n:::\n\n\n## ğŸ›  Contribute \n\nContributions are welcome! A good place to start is the [list](https://github.com/pat-alt/ConformalPrediction.jl/issues) of outstanding issues. For more details, see also the [Contributor's Guide](https://www.paltmeyer.com/ConformalPrediction.jl/dev/contribute/). Please follow the [SciML ColPrac guide](https://github.com/SciML/ColPrac).\n\n## ğŸ™ Thanks\n\nTo build this package we have made heavy use of this amazing [tutorial](https://arxiv.org/abs/2107.07511) [@angelopoulos2021gentle] and also this research [paper](https://arxiv.org/abs/1905.02928). The Awesome Conformal Prediction [repository](https://github.com/valeman/awesome-conformal-prediction) [@manokhin2022awesome] has also been a fantastic place to get started. Special thanks also to [\\@aangelopoulos](https://github.com/aangelopoulos), [\\@valeman](https://github.com/valeman) and others for actively contributing to discussions on here. \n\n## ğŸ“ References \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": []
  }
}