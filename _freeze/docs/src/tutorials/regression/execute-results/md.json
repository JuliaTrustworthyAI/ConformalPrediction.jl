{
  "hash": "14f4800514a7c6ce15170af1070c0531",
  "result": {
    "markdown": "---\ntitle: Regression\n---\n\n\n\n\n\n\nThis tutorial presents and compares different approaches to Conformal Regression using a simple synthetic dataset. It is inspired by this MAPIE [tutorial](https://mapie.readthedocs.io/en/latest/examples_regression/4-tutorials/plot_main-tutorial-regression.html#).\n\n## Data\n\nWe begin by generating some synthetic regression data below:\n\n::: {#fig-data .cell execution_count=2}\n``` {.julia .cell-code}\n# Regression data:\n\n# Inputs:\nN = 600\nxmax = 5.0\nusing Distributions\nd = Uniform(-xmax, xmax)\nX = rand(d, N)\nX = reshape(X, :, 1)\n\n# Outputs:\nnoise = 0.5\nfun(X) = X * sin(X)\nε = randn(N) .* noise\ny = @.(fun(X)) + ε\ny = vec(y)\n\n# Partition:\nusing MLJ\ntrain, test = partition(eachindex(y), 0.4, 0.4, shuffle=true)\n\nusing Plots\nscatter(X, y, label=\"Observed\")\nxrange = range(-xmax,xmax,length=N)\nplot!(xrange, @.(fun(xrange)), lw=4, label=\"Ground truth\", ls=:dash, colour=:black)\n```\n:::\n\n\n## Model\n\nTo model this data we will use polynomial regression. There is currently no out-of-the-box support for polynomial feature transformations in `MLJ`, but it is easy enough to add a little helper function for this. Note how we define a linear pipeline `pipe` here. Since pipelines in `MLJ` are just models, we can use the generated object as an input to `conformal_model` below.\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nLinearRegressor = @load LinearRegressor pkg=MLJLinearModels\ndegree_polynomial = 10\npolynomial_features(X, degree::Int) = reduce(hcat, map(i -> X.^i, 1:degree))\npipe = (X -> MLJ.table(polynomial_features(MLJ.matrix(X), degree_polynomial))) |> LinearRegressor()\n```\n:::\n\n\n## Conformal Prediction\n\nNext, we conformalize our polynomial regressor using every available approach (except the Naive approach):\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nusing ConformalPrediction\nconformal_models = merge(values(available_models[:regression])...)\nresults = Dict()\nfor _mod in keys(conformal_models) \n    conf_model = conformal_model(pipe; method=_mod, coverage=0.95)\n    global mach = machine(conf_model, X, y)\n    MLJ.fit!(mach, rows=train)\n    results[_mod] = mach\nend\n```\n:::\n\n\n::: {.cell execution_count=5}\n\n::: {.cell-output .cell-output-display execution_count=6}\nFinally, let us look at the resulting conformal predictions in each case. The chart below shows the results: for the first 4 methods it displays the training data (dots) overlaid with the conformal prediction interval (shaded area). At first glance it is hard to spot any major differences between the different approaches. Next, we will look at how we can evaluate and benchmark these predictions.\n\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\nusing Plots\nzoom = -0.5\nxrange = range(-xmax+zoom,xmax-zoom,length=N)\nplt_list = []\n\nfor (_mod, mach) in first(results, n_charts)\n    plt = plot(mach.model, mach.fitresult, X, y, zoom=zoom, title=_mod)\n    plot!(plt, xrange, @.(fun(xrange)), lw=1, ls=:dash, colour=:black, label=\"Ground truth\")\n    push!(plt_list, plt)\nend\n\nplot(plt_list..., size=(800,500))\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n![Conformal prediction regions.](regression_files/figure-commonmark/fig-cp-output-1.svg){#fig-cp}\n:::\n:::\n\n\n## Evaluation\n\nFor evaluation of conformal predictors we follow @angelopoulos2021gentle (Section 3). As a first step towards adaptiveness (adaptivity), the authors recommend to inspect the set size of conformal predictions. The chart below shows the interval width for the different methods along with the ground truth interval width:\n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\nxrange = range(-xmax,xmax,length=N)\nplt = plot(xrange, ones(N) .* (1.96*2*noise), ls=:dash, colour=:black, label=\"Ground truth\", lw=2)\nfor (_mod, mach) in results\n    ŷ = predict(mach, reshape([x for x in xrange], :, 1))\n    y_size = set_size.(ŷ)\n    plot!(xrange, y_size, label=String(_mod))\nend\nplt\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n![Prediction interval width.](regression_files/figure-commonmark/fig-setsize-output-1.svg){#fig-setsize}\n:::\n:::\n\n\nWe can also use specific metrics like **empirical coverage** and **size-stratified coverage** to check for correctness and adaptiveness, respectively [@angelopoulus2021gentle]. To this end, the package provides custom measures that are compatible with `MLJ.jl`. In other words, we can evaluate model performance in true `MLJ.jl` fashion (see [here](https://alan-turing-institute.github.io/MLJ.jl/dev/evaluating_model_performance/)). \n\nThe code below runs the evaluation with respect to both metrics, `emp_coverage` and `ssc` for a single conformal machine: \n\n::: {.cell execution_count=8}\n``` {.julia .cell-code}\n_mod, mach = first(results)\n_eval = evaluate!(\n    mach,\n    operation=predict,\n    measure=[emp_coverage, ssc]\n)\ndisplay(_eval)\nprintln(\"Empirical coverage for $(_mod): $(round(_eval.measurement[1], digits=3))\")\nprintln(\"SSC for $(_mod): $(round(_eval.measurement[2], digits=3))\")\n```\n\n::: {.cell-output .cell-output-display}\n```\nPerformanceEvaluation object with these fields:\n  measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows\nExtract:\n┌──────────────────────────────────────────────┬───────────┬─────────────┬──────\n│ measure                                      │ operation │ measurement │ 1.9 ⋯\n├──────────────────────────────────────────────┼───────────┼─────────────┼──────\n│ ConformalPrediction.emp_coverage             │ predict   │ 0.94        │ 0.0 ⋯\n│ ConformalPrediction.size_stratified_coverage │ predict   │ 0.94        │ 0.0 ⋯\n└──────────────────────────────────────────────┴───────────┴─────────────┴──────\n                                                               2 columns omitted\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEmpirical coverage for jackknife_plus_ab: 0.94\nSSC for jackknife_plus_ab: 0.94\n```\n:::\n:::\n\n\nNote that, in the regression case, stratified set sizes correspond to discretized interval widths. \n\nTo benchmark the different approaches, we evaluate them iteratively below. As expected, more conservative approaches like Jackknife-$\\min\\max$ and CV-$\\min\\max$ attain higher aggregate and conditional coverage. Note that size-stratified is not available for methods that produce constant intervals, like standard Jackknife.\n\n::: {.cell execution_count=9}\n``` {.julia .cell-code}\nusing DataFrames\nbmk = DataFrame()\nfor (_mod, mach) in results\n    _eval = evaluate!(\n        mach,\n        resampling=CV(;nfolds=5),\n        operation=predict,\n        measure=[emp_coverage, ssc]\n    )\n    _bmk = DataFrame(\n        Dict(\n            :model => _mod,\n            :emp_coverage => _eval.measurement[1],\n            :ssc => _eval.measurement[2]\n        )\n    )\n    bmk = vcat(bmk, _bmk)\nend\n\nshow(sort(select!(bmk, [2,1,3]), 2, rev=true))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n9×3 DataFrame\n Row │ model                     emp_coverage  ssc      \n     │ Symbol                    Float64       Float64  \n─────┼──────────────────────────────────────────────────\n   1 │ jackknife_plus_ab_minmax      0.988333  0.980547\n   2 │ cv_minmax                     0.96      0.910873\n   3 │ simple_inductive              0.953333  0.953333\n   4 │ jackknife_minmax              0.946667  0.869103\n   5 │ cv_plus                       0.945     0.866549\n   6 │ jackknife_plus_ab             0.941667  0.941667\n   7 │ jackknife_plus                0.941667  0.871606\n   8 │ jackknife                     0.941667  0.941667\n   9 │ naive                         0.938333  0.938333\n```\n:::\n:::\n\n\n## References\n\n",
    "supporting": [
      "regression_files/figure-commonmark"
    ],
    "filters": []
  }
}