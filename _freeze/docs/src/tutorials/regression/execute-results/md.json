{
  "hash": "5eb8e22be47f36070d5894401462c6dd",
  "result": {
    "markdown": "# Regression\n\n```@meta\nCurrentModule = ConformalPrediction\n```\n\n\n\nThis tutorial mostly replicates this [tutorial](https://mapie.readthedocs.io/en/latest/examples_regression/4-tutorials/plot_main-tutorial-regression.html#) from MAPIE.\n\n## Data\n\nWe begin by generating some synthetic regression data below:\n\n::: {#fig-data .cell execution_count=2}\n``` {.julia .cell-code}\n# Regression data:\n\n# Inputs:\nN = 600\nxmax = 3.0\nusing Distributions\nd = Uniform(-xmax, xmax)\nX = rand(d, N)\nX = reshape(X, :, 1)\n\n# Outputs:\nnoise = 0.5\nfun(X) = X * sin(X)\nε = randn(N) .* noise\ny = @.(fun(X)) + ε\ny = vec(y)\n\n# Partition:\nusing MLJ\ntrain, test = partition(eachindex(y), 0.4, 0.4, shuffle=true)\n\nusing Plots\nscatter(X, y, label=\"Observed\")\nxrange = range(-xmax,xmax,length=N)\nplot!(xrange, @.(fun(xrange)), lw=4, label=\"Ground truth\", ls=:dash, colour=:black)\n```\n:::\n\n\n## Model\n\nTo model this data we will use polynomial regression. There is currently no out-of-the-box support for polynomial feature transformations in `MLJ`, but it is easy enough to add a little helper function for this. Note how we define a linear pipeline `pipe` here. Since pipelines in `MLJ` are just models, we can use the generated object as an input to `conformal_model` below.\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nLinearRegressor = @load LinearRegressor pkg=MLJLinearModels\ndegree_polynomial = 10\npolynomial_features(X, degree::Int) = reduce(hcat, map(i -> X.^i, 1:degree))\npipe = (X -> MLJ.table(polynomial_features(MLJ.matrix(X), degree_polynomial))) |> LinearRegressor()\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nEvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees\n# pipe = EvoTreeRegressor(rounds=100) \n```\n:::\n\n\n## Conformal Prediction\n\nNext, we conformalize our polynomial regressor using every available approach (except the Naive approach):\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nusing ConformalPrediction\nconformal_models = merge(values(available_models[:regression])...)\ndelete!(conformal_models, :naive)\n# delete!(conformal_models, :jackknife)\nresults = Dict()\nfor _mod in keys(conformal_models) \n    conf_model = conformal_model(pipe; method=_mod, coverage=0.95)\n    global mach = machine(conf_model, X, y)\n    fit!(mach, rows=train)\n    results[_mod] = mach\nend\n```\n:::\n\n\nFinally, let us look at the resulting conformal predictions in each case.\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\nusing Plots\nzoom = -0.5\nxrange = range(-xmax+zoom,xmax-zoom,length=N)\nplt_list = []\n\nfor (_mod, mach) in results\n    plt = plot(mach.model, mach.fitresult, X, y, zoom=zoom, title=_mod)\n    plot!(plt, xrange, @.(fun(xrange)), lw=1, ls=:dash, colour=:black, label=\"Ground truth\")\n    push!(plt_list, plt)\nend\n\nplot(plt_list..., size=(1600,1000))\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n![Conformal prediction regions.](regression_files/figure-commonmark/fig-cp-output-1.svg){#fig-cp}\n:::\n:::\n\n\n",
    "supporting": [
      "regression_files/figure-commonmark"
    ],
    "filters": []
  }
}