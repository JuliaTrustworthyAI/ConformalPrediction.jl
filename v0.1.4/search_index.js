var documenterSearchIndex = {"docs":
[{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"ConformalPrediction.jl is a package for Uncertainty Quantification (UQ) through Conformal Prediction (CP) in Julia. It is designed to work with supervised models trained in MLJ Blaom et al. (2020). Conformal Prediction is distribution-free, easy-to-understand, easy-to-use and model-agnostic.","category":"page"},{"location":"intro/#Background","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"","category":"section"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"Conformal Prediction is a scalable frequentist approach to uncertainty quantification and coverage control. It promises to be an easy-to-understand, distribution-free and model-agnostic way to generate statistically rigorous uncertainty estimates. Interestingly, it can even be used to complement Bayesian methods.","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"The animation below is lifted from a small blog post that introduces the topic and the package ([TDS], [Quarto]). It shows conformal prediction sets for two different samples and changing coverage rates. Standard conformal classifiers produce set-valued predictions: for ambiguous samples these sets are typically large (for high coverage) or empty (for low coverage).","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"(Image: Conformal Prediction in action: Prediction sets for two different samples and changing coverage rates. As coverage grows, so does the size of the prediction sets.)","category":"page"},{"location":"intro/#Installation","page":"ğŸ“– Background","title":"ğŸš© Installation","text":"","category":"section"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"You can install the latest stable release from the general registry:","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"using Pkg\nPkg.add(\"ConformalPrediction\")","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"The development version can be installed as follows:","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"using Pkg\nPkg.add(url=\"https://github.com/pat-alt/ConformalPrediction.jl\")","category":"page"},{"location":"intro/#Status","page":"ğŸ“– Background","title":"ğŸ” Status","text":"","category":"section"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"This package is in its early stages of development and therefore still subject to changes to the core architecture and API. The following CP approaches have been implemented in the development version:","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"Regression:","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"Inductive\nNaive Transductive\nJackknife\nJackknife+\nJackknife-minmax\nCV+\nCV-minmax","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"Classification:","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"Inductive (LABEL (Sadinle, Lei, and Wasserman 2019))\nAdaptive Inductive","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"The package has been tested for the following supervised models offered by MLJ.","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"Regression:","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"using ConformalPrediction\nkeys(tested_atomic_models[:regression])","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"KeySet for a Dict{Symbol, Expr} with 4 entries. Keys:\n  :nearest_neighbor\n  :evo_tree\n  :light_gbm\n  :decision_tree","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"Classification:","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"keys(tested_atomic_models[:classification])","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"KeySet for a Dict{Symbol, Expr} with 4 entries. Keys:\n  :nearest_neighbor\n  :evo_tree\n  :light_gbm\n  :decision_tree","category":"page"},{"location":"intro/#Usage-Example","page":"ğŸ“– Background","title":"ğŸ” Usage Example","text":"","category":"section"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"To illustrate the intended use of the package, letâ€™s have a quick look at a simple regression problem. Using MLJ we first generate some synthetic data and then determine indices for our training, calibration and test data:","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"using MLJ\nX, y = MLJ.make_regression(1000, 2)\ntrain, test = partition(eachindex(y), 0.4, 0.4)","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"We then import a decision tree (EvoTrees.jl) following the standard MLJ procedure.","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"EvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees\nmodel = EvoTreeRegressor() ","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"To turn our conventional model into a conformal model, we just need to declare it as such by using conformal_model wrapper function. The generated conformal model instance can wrapped in data to create a machine. Finally, we proceed by fitting the machine on training data using the generic fit! method:","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"using ConformalPrediction\nconf_model = conformal_model(model)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"Predictions can then be computed using the generic predict method. The code below produces predictions for the first n samples. Each tuple contains the lower and upper bound for the prediction interval.","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"n = 5\nXtest = selectrows(X, first(test,n))\nytest = y[first(test,n)]\npredict(mach, Xtest)","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚                                                          â”‚\nâ”‚      (1)   (-0.9864061984981062, 2.2503222170961554)     â”‚\nâ”‚      (2)   (-0.7192196826151477, 2.5175087329791137)     â”‚\nâ”‚      (3)   (-0.33838267507136344, 2.898345740522898)     â”‚\nâ”‚      (4)   (-2.838413186252051, 0.39831522934221053)     â”‚\nâ”‚      (5)   (-0.7192196826151477, 2.5175087329791137)     â”‚\nâ”‚                                                          â”‚\nâ”‚                                                          â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5 items â”€â”€â”€â•¯","category":"page"},{"location":"intro/#Contribute","page":"ğŸ“– Background","title":"ğŸ›  Contribute","text":"","category":"section"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"Contributions are welcome! Please follow the SciML ColPrac guide.","category":"page"},{"location":"intro/#References","page":"ğŸ“– Background","title":"ğŸ“ References","text":"","category":"section"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"Blaom, Anthony D., Franz Kiraly, Thibaut Lienart, Yiannis Simillides, Diego Arenas, and Sebastian J. Vollmer. 2020. â€œMLJ: A Julia Package for Composable Machine Learning.â€ Journal of Open Source Software 5 (55): 2704. https://doi.org/10.21105/joss.02704.","category":"page"},{"location":"intro/","page":"ğŸ“– Background","title":"ğŸ“– Background","text":"Sadinle, Mauricio, Jing Lei, and Larry Wasserman. 2019. â€œLeast Ambiguous Set-Valued Classifiers with Bounded Error Levels.â€ Journal of the American Statistical Association 114 (525): 223â€“34.","category":"page"},{"location":"contribute/#Contributorâ€™s-Guide","page":"ğŸ›  Contribute","title":"Contributorâ€™s Guide","text":"","category":"section"},{"location":"contribute/","page":"ğŸ›  Contribute","title":"ğŸ›  Contribute","text":"CurrentModule = ConformalPrediction","category":"page"},{"location":"contribute/#Contents","page":"ğŸ›  Contribute","title":"Contents","text":"","category":"section"},{"location":"contribute/","page":"ğŸ›  Contribute","title":"ğŸ›  Contribute","text":"Pages = [\"contribute.md\"]\nDepth = 2","category":"page"},{"location":"contribute/#Contributing-to-ConformalPrediction.jl","page":"ğŸ›  Contribute","title":"Contributing to ConformalPrediction.jl","text":"","category":"section"},{"location":"contribute/","page":"ğŸ›  Contribute","title":"ğŸ›  Contribute","text":"Contributions are welcome! Please follow the SciML ColPrac guide.","category":"page"},{"location":"contribute/#Architecture","page":"ğŸ›  Contribute","title":"Architecture","text":"","category":"section"},{"location":"contribute/","page":"ğŸ›  Contribute","title":"ğŸ›  Contribute","text":"The diagram below demonstrates the package architecture at the time of writing. This is still subject to change, so any thoughts and comments are very much welcome.","category":"page"},{"location":"contribute/","page":"ğŸ›  Contribute","title":"ğŸ›  Contribute","text":"The goal is to make this package as compatible as possible with MLJ to tab into existing functionality. The basic idea is to subtype MLJ Supervised models and then use concrete types to implement different approaches to conformal prediction. For each of these concrete types the compulsory MMI.fit and MMI.predict methods need be implemented (see here).","category":"page"},{"location":"contribute/","page":"ğŸ›  Contribute","title":"ğŸ›  Contribute","text":"(Image: )","category":"page"},{"location":"contribute/#Abstract-Suptypes","page":"ğŸ›  Contribute","title":"Abstract Suptypes","text":"","category":"section"},{"location":"contribute/","page":"ğŸ›  Contribute","title":"ğŸ›  Contribute","text":"Currently I intend to work with three different abstract subtypes:","category":"page"},{"location":"contribute/","page":"ğŸ›  Contribute","title":"ğŸ›  Contribute","text":"ConformalInterval\nConformalSet\nConformalProbabilistic","category":"page"},{"location":"contribute/#ConformalPrediction.ConformalInterval","page":"ğŸ›  Contribute","title":"ConformalPrediction.ConformalInterval","text":"An abstract base type for conformal models that produce interval-valued predictions. This includes most conformal regression models.\n\n\n\n\n\n","category":"type"},{"location":"contribute/#ConformalPrediction.ConformalProbabilistic","page":"ğŸ›  Contribute","title":"ConformalPrediction.ConformalProbabilistic","text":"An abstract base type for conformal models that produce probabilistic predictions. This includes some conformal classifier like Venn-ABERS.\n\n\n\n\n\n","category":"type"},{"location":"contribute/#fit-and-predict","page":"ğŸ›  Contribute","title":"fit and predict","text":"","category":"section"},{"location":"contribute/","page":"ğŸ›  Contribute","title":"ğŸ›  Contribute","text":"The fit and predict methods are compulsory in order to prepare models for general use with MLJ. They also serve us to implement the logic underlying the various approaches to conformal prediction.","category":"page"},{"location":"contribute/","page":"ğŸ›  Contribute","title":"ğŸ›  Contribute","text":"To understand how this currently works, letâ€™s look at the AdaptiveInductiveClassifier as an example. Below are the two docstrings documenting both methods. Hovering over the bottom-right corner will reveal buttons that take you to the source code.","category":"page"},{"location":"contribute/","page":"ğŸ›  Contribute","title":"ğŸ›  Contribute","text":"fit(conf_model::AdaptiveInductiveClassifier, verbosity, X, y)","category":"page"},{"location":"contribute/#MLJModelInterface.fit-Tuple{ConformalPrediction.AdaptiveInductiveClassifier, Any, Any, Any}","page":"ğŸ›  Contribute","title":"MLJModelInterface.fit","text":"MMI.fit(conf_model::AdaptiveInductiveClassifier, verbosity, X, y)\n\nFor the AdaptiveInductiveClassifier nonconformity scores are computed by cumulatively summing the ranked scores of each label in descending order until reaching the true label Y_i:\n\nS_i^textCAL = s(X_iY_i) = sum_j=1^k  hatmu(X_i)_pi_j  textwhere   Y_i=pi_k  i in mathcalD_textcalibration\n\n\n\n\n\n","category":"method"},{"location":"contribute/","page":"ğŸ›  Contribute","title":"ğŸ›  Contribute","text":"predict(conf_model::AdaptiveInductiveClassifier, fitresult, Xnew)","category":"page"},{"location":"contribute/#MLJModelInterface.predict-Tuple{ConformalPrediction.AdaptiveInductiveClassifier, Any, Any}","page":"ğŸ›  Contribute","title":"MLJModelInterface.predict","text":"MMI.predict(conf_model::AdaptiveInductiveClassifier, fitresult, Xnew)\n\nFor the AdaptiveInductiveClassifier prediction sets are computed as follows,\n\nhatC_nalpha(X_n+1) = lefty s(X_n+1y) le hatq_n alpha^+ S_i^textCAL right  i in mathcalD_textcalibration\n\nwhere mathcalD_textcalibration denotes the designated calibration data.\n\n\n\n\n\n","category":"method"},{"location":"classification/#Classification","page":"Classification","title":"Classification","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"CurrentModule = ConformalPrediction","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"This tutorial is based in parts on this blog post.","category":"page"},{"location":"classification/#Split-Conformal-Classification","page":"Classification","title":"Split Conformal Classification","text":"","category":"section"},{"location":"classification/","page":"Classification","title":"Classification","text":"We consider a simple binary classification problem. Let (X(i),Y(i)),Â iâ€„=â€„1,â€†...,â€†n denote our feature-label pairs and let Î¼â€„:â€„ğ’³â€„â†¦â€„ğ’´ denote the mapping from features to labels. For illustration purposes we will use the moons dataset ğŸŒ™. Using MLJ.jl we first generate the data and split into into a training and test set:","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"using MLJ\nusing Random\nRandom.seed!(123)\n\n# Data:\nX, y = make_moons(500; noise=0.15)\ntrain, test = partition(eachindex(y), 0.8, shuffle=true)","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"Here we will use a specific case of CP called split conformal prediction which can then be summarized as follows:[1]","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"Partition the training into a proper training set and a separate calibration set: ğ’Ÿ_(n)â€„=â€„ğ’Ÿ^(train)â€…âˆªâ€…ğ’Ÿ^(cali).\nTrain the machine learning model on the proper training set: Î¼Ì‚(iâ€„âˆˆâ€„ğ’Ÿ^(train))(X(i),Y_(i)).\nCompute nonconformity scores, ğ’®, using the calibration data ğ’Ÿ^(cali) and the fitted model Î¼Ì‚_(iâ€„âˆˆâ€„ğ’Ÿ^(train)).\nFor a user-specified desired coverage ratio (1âˆ’Î±) compute the corresponding quantile, qÌ‚, of the empirical distribution of nonconformity scores, ğ’®.\nFor the given quantile and test sample X_(test), form the corresponding conformal prediction set:","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"C(X(test))â€„=â€„{yâ€„:â€„s(X(test),y)â€„â‰¤â€„qÌ‚}â€Šâ€â€(1)","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"This is the default procedure used for classification and regression in ConformalPrediction.jl.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"Now letâ€™s take this to our ğŸŒ™ data. To illustrate the package functionality we will demonstrate the envisioned workflow. We first define our atomic machine learning model following standard MLJ.jl conventions. Using ConformalPrediction.jl we then wrap our atomic model in a conformal model using the standard API call conformal_model(model::Supervised; kwargs...). To train and predict from our conformal model we can then rely on the conventional MLJ.jl procedure again. In particular, we wrap our conformal model in data (turning it into a machine) and then fit it on the training set. Finally, we use our machine to predict the label for a new test sample Xtest:","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"# Model:\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\nmodel = KNNClassifier(;K=50) \n\n# Training:\nusing ConformalPrediction\nconf_model = conformal_model(model; coverage=.9)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)\n\n# Conformal Prediction:\nXtest = selectrows(X, first(test))\nytest = y[first(test)]\npredict(mach, Xtest)[1]","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"import NearestNeighborModels âœ”\n\n           UnivariateFinite{Multiclass{2}}      \n     â”Œ                                        â” \n   0 â”¤â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â–  0.94   \n     â””                                        â”˜","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"The final predictions are set-valued. While the softmax output remains unchanged for the SimpleInductiveClassifier, the size of the prediction set depends on the chosen coverage rate, (1âˆ’Î±).","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"When specifying a coverage rate very close to one, the prediction set will typically include many (in some cases all) of the possible labels. Below, for example, both classes are included in the prediction set when setting the coverage rate equal to (1âˆ’Î±)=1.0. This is intuitive, since high coverage quite literally requires that the true label is covered by the prediction set with high probability.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"conf_model = conformal_model(model; coverage=coverage)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)\n\n# Conformal Prediction:\nXtest = (x1=[1],x2=[0])\npredict(mach, Xtest)[1]","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"           UnivariateFinite{Multiclass{2}}      \n     â”Œ                                        â” \n   0 â”¤â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â–  0.5   \n   1 â”¤â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â–  0.5   \n     â””                                        â”˜","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"Conversely, for low coverage rates, prediction sets can also be empty. For a choice of (1âˆ’Î±)=0.1, for example, the prediction set for our test sample is empty. This is a bit difficult to think about intuitively and I have not yet come across a satisfactory, intuitive interpretation.[2] When the prediction set is empty, the predict call currently returns missing:","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"conf_model = conformal_model(model; coverage=coverage)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)\n\n# Conformal Prediction:\npredict(mach, Xtest)[1]","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"missing","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"cov_ = .9\nconf_model = conformal_model(model; coverage=cov_)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)\nMarkdown.parse(\"\"\"\nThe following chart shows the resulting predicted probabilities for ``y=1`` (left) and set size (right) for a choice of ``(1-\\\\alpha)``=$cov_.\n\"\"\")","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"The following chart shows the resulting predicted probabilities for yâ€„=â€„1 (left) and set size (right) for a choice of (1âˆ’Î±)=0.9.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"using Plots\np_proba = plot(mach.model, mach.fitresult, X, y)\np_set_size = plot(mach.model, mach.fitresult, X, y; plot_set_size=true)\nplot(p_proba, p_set_size, size=(800,250))","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"(Image: )","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"The animation below should provide some more intuition as to what exactly is happening here. It illustrates the effect of the chosen coverage rate on the predicted softmax output and the set size in the two-dimensional feature space. Contours are overlayed with the moon data points (including test data). The two samples highlighted in red, Xâ‚ and Xâ‚‚, have been manually added for illustration purposes. Letâ€™s look at these one by one.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"Firstly, note that Xâ‚ (red cross) falls into a region of the domain that is characterized by high predictive uncertainty. It sits right at the bottom-right corner of our class-zero moon ğŸŒœ (orange), a region that is almost entirely enveloped by our class-one moon ğŸŒ› (green). For low coverage rates the prediction set for Xâ‚ is empty: on the left-hand side this is indicated by the missing contour for the softmax probability; on the right-hand side we can observe that the corresponding set size is indeed zero. For high coverage rates the prediction set includes both yâ€„=â€„0 and yâ€„=â€„1, indicative of the fact that the conformal classifier is uncertain about the true label.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"With respect to Xâ‚‚, we observe that while also sitting on the fringe of our class-zero moon, this sample populates a region that is not fully enveloped by data points from the opposite class. In this region, the underlying atomic classifier can be expected to be more certain about its predictions, but still not highly confident. How is this reflected by our corresponding conformal prediction sets?","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"Xtest_2 = (x1=[-0.5],x2=[0.25])\npÌ‚_2 = pdf(predict(mach, Xtest_2)[1], 0)","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"Well, for low coverage rates (roughly â€„\\<â€„0.9) the conformal prediction set does not include yâ€„=â€„0: the set size is zero (right panel). Only for higher coverage rates do we have C(Xâ‚‚)â€„=â€„{0}: the coverage rate is high enough to include yâ€„=â€„0, but the corresponding softmax probability is still fairly low. For example, for (1âˆ’Î±)â€„=â€„0.9 we have pÌ‚(y=0|Xâ‚‚)â€„=â€„0.72.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"These two examples illustrate an interesting point: for regions characterized by high predictive uncertainty, conformal prediction sets are typically empty (for low coverage) or large (for high coverage). While set-valued predictions may be something to get used to, this notion is overall intuitive.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"# Setup\ncoverages = range(0.75,1.0,length=5)\nn = 100\nx1_range = range(extrema(X.x1)...,length=n)\nx2_range = range(extrema(X.x2)...,length=n)\n\nanim = @animate for coverage in coverages\n    conf_model = conformal_model(model; coverage=coverage)\n    mach = machine(conf_model, X, y)\n    fit!(mach, rows=train)\n    # Probabilities:\n    p1 = plot(mach.model, mach.fitresult, X, y)\n    scatter!(p1, Xtest.x1, Xtest.x2, ms=6, c=:red, label=\"Xâ‚\", shape=:cross, msw=6)\n    scatter!(p1, Xtest_2.x1, Xtest_2.x2, ms=6, c=:red, label=\"Xâ‚‚\", shape=:diamond, msw=6)\n    p2 = plot(mach.model, mach.fitresult, X, y; plot_set_size=true)\n    scatter!(p2, Xtest.x1, Xtest.x2, ms=6, c=:red, label=\"Xâ‚\", shape=:cross, msw=6)\n    scatter!(p2, Xtest_2.x1, Xtest_2.x2, ms=6, c=:red, label=\"Xâ‚‚\", shape=:diamond, msw=6)\n    plot(p1, p2, plot_title=\"(1-Î±)=$(round(coverage,digits=2))\", size=(800,300))\nend\n\ngif(anim, joinpath(www_path,\"classification.gif\"), fps=1)","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"The effect of the coverage rate on the conformal prediction set. Softmax probabilities are shown on the left. The size of the prediction set is shown on the right.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"(Image: )","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"[1] In other places split conformal prediction is sometimes referred to as inductive conformal prediction.","category":"page"},{"location":"classification/","page":"Classification","title":"Classification","text":"[2] Any thoughts/comments welcome!","category":"page"},{"location":"reference/","page":"ğŸ“– Reference","title":"ğŸ“– Reference","text":"CurrentModule = ConformalPrediction","category":"page"},{"location":"reference/#Content","page":"ğŸ“– Reference","title":"Content","text":"","category":"section"},{"location":"reference/","page":"ğŸ“– Reference","title":"ğŸ“– Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/#Index","page":"ğŸ“– Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"ğŸ“– Reference","title":"ğŸ“– Reference","text":"","category":"page"},{"location":"reference/#Public-Interface","page":"ğŸ“– Reference","title":"Public Interface","text":"","category":"section"},{"location":"reference/","page":"ğŸ“– Reference","title":"ğŸ“– Reference","text":"Modules = [\n    ConformalPrediction,\n    ConformalPrediction.ConformalModels\n]\nPrivate = false","category":"page"},{"location":"reference/#Internal-functions","page":"ğŸ“– Reference","title":"Internal functions","text":"","category":"section"},{"location":"reference/","page":"ğŸ“– Reference","title":"ğŸ“– Reference","text":"Modules = [\n    ConformalPrediction,\n    ConformalPrediction.ConformalModels\n]\nPublic = false","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"CurrentModule = ConformalPrediction","category":"page"},{"location":"#ConformalPrediction","page":"ğŸ  Home","title":"ConformalPrediction","text":"","category":"section"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"Documentation for ConformalPrediction.jl.","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"ConformalPrediction.jl is a package for Uncertainty Quantification (UQ) through Conformal Prediction (CP) in Julia. It is designed to work with supervised models trained in MLJ Blaom et al. (2020). Conformal Prediction is distribution-free, easy-to-understand, easy-to-use and model-agnostic.","category":"page"},{"location":"#Background","page":"ğŸ  Home","title":"ğŸ“– Background","text":"","category":"section"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"Conformal Prediction is a scalable frequentist approach to uncertainty quantification and coverage control. It promises to be an easy-to-understand, distribution-free and model-agnostic way to generate statistically rigorous uncertainty estimates. Interestingly, it can even be used to complement Bayesian methods.","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"The animation below is lifted from a small blog post that introduces the topic and the package ([TDS], [Quarto]). It shows conformal prediction sets for two different samples and changing coverage rates. Standard conformal classifiers produce set-valued predictions: for ambiguous samples these sets are typically large (for high coverage) or empty (for low coverage).","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"(Image: Conformal Prediction in action: Prediction sets for two different samples and changing coverage rates. As coverage grows, so does the size of the prediction sets.)","category":"page"},{"location":"#Installation","page":"ğŸ  Home","title":"ğŸš© Installation","text":"","category":"section"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"You can install the latest stable release from the general registry:","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"using Pkg\nPkg.add(\"ConformalPrediction\")","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"The development version can be installed as follows:","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"using Pkg\nPkg.add(url=\"https://github.com/pat-alt/ConformalPrediction.jl\")","category":"page"},{"location":"#Status","page":"ğŸ  Home","title":"ğŸ” Status","text":"","category":"section"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"This package is in its early stages of development and therefore still subject to changes to the core architecture and API. The following CP approaches have been implemented in the development version:","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"Regression:","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"Inductive\nNaive Transductive\nJackknife\nJackknife+\nJackknife-minmax\nCV+\nCV-minmax","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"Classification:","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"Inductive (LABEL (Sadinle, Lei, and Wasserman 2019))\nAdaptive Inductive","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"The package has been tested for the following supervised models offered by MLJ.","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"Regression:","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"using ConformalPrediction\nkeys(tested_atomic_models[:regression])","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"KeySet for a Dict{Symbol, Expr} with 5 entries. Keys:\n  :nearest_neighbor\n  :evo_tree\n  :light_gbm\n  :linear\n  :decision_tree","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"Classification:","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"keys(tested_atomic_models[:classification])","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"KeySet for a Dict{Symbol, Expr} with 5 entries. Keys:\n  :nearest_neighbor\n  :evo_tree\n  :light_gbm\n  :decision_tree\n  :logistic","category":"page"},{"location":"#Usage-Example","page":"ğŸ  Home","title":"ğŸ” Usage Example","text":"","category":"section"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"To illustrate the intended use of the package, letâ€™s have a quick look at a simple regression problem. Using MLJ we first generate some synthetic data and then determine indices for our training, calibration and test data:","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"using MLJ\n\n# Inputs:\nN = 600\nxmax = 3.0\nusing Distributions\nd = Uniform(-xmax, xmax)\nX = rand(d, N)\nX = reshape(X, :, 1)\n\n# Outputs:\nnoise = 0.5\nfun(X) = X * sin(X)\nÎµ = randn(N) .* noise\ny = @.(fun(X)) + Îµ\ny = vec(y)\n\n# Partition:\ntrain, test = partition(eachindex(y), 0.4, 0.4, shuffle=true)","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"We then import a decision tree (EvoTrees.jl) following the standard MLJ procedure.","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"EvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees\nmodel = EvoTreeRegressor(rounds=100) ","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"To turn our conventional model into a conformal model, we just need to declare it as such by using conformal_model wrapper function. The generated conformal model instance can wrapped in data to create a machine. Finally, we proceed by fitting the machine on training data using the generic fit! method:","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"using ConformalPrediction\nconf_model = conformal_model(model; method=:jackknife_plus)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"Predictions can then be computed using the generic predict method. The code below produces predictions for the first n samples. Each tuple contains the lower and upper bound for the prediction interval. The chart below visualizes the results.","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"show_first = 5\nXtest = selectrows(X, test)\nytest = y[test]\nyÌ‚ = predict(mach, Xtest)\nyÌ‚[1:show_first]","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\nâ”‚                                                           â”‚\nâ”‚      (1)   (0.43551526547848046, 2.6332057068681665)      â”‚\nâ”‚      (2)   (0.2549311529001698, 2.51442447285207)         â”‚\nâ”‚      (3)   (0.013595617761448903, 2.2354822319945615)     â”‚\nâ”‚      (4)   (0.4575537084523473, 2.677558088410297)        â”‚\nâ”‚      (5)   (0.5163658290099347, 2.6967957385843824)       â”‚\nâ”‚                                                           â”‚\nâ”‚                                                           â”‚\nâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5 items â”€â”€â”€â•¯","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"(Image: )","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"We can evaluate the conformal model using the standard MLJ workflow with a custom performance measure (either emp_coverage for the overall empirical coverage or ssc for the size-stratified coverage rate).","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"_eval = evaluate!(mach; measure=[emp_coverage, ssc], verbosity=0)\nprintln(\"Empirical coverage: $(round(_eval.measurement[1], digits=3))\")\nprintln(\"SSC: $(round(_eval.measurement[2], digits=3))\")","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"Empirical coverage: 0.95\nSSC: 0.815","category":"page"},{"location":"#Contribute","page":"ğŸ  Home","title":"ğŸ›  Contribute","text":"","category":"section"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"Contributions are welcome! Please follow the SciML ColPrac guide.","category":"page"},{"location":"#References","page":"ğŸ  Home","title":"ğŸ“ References","text":"","category":"section"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"Blaom, Anthony D., Franz Kiraly, Thibaut Lienart, Yiannis Simillides, Diego Arenas, and Sebastian J. Vollmer. 2020. â€œMLJ: A Julia Package for Composable Machine Learning.â€ Journal of Open Source Software 5 (55): 2704. https://doi.org/10.21105/joss.02704.","category":"page"},{"location":"","page":"ğŸ  Home","title":"ğŸ  Home","text":"Sadinle, Mauricio, Jing Lei, and Larry Wasserman. 2019. â€œLeast Ambiguous Set-Valued Classifiers with Bounded Error Levels.â€ Journal of the American Statistical Association 114 (525): 223â€“34.","category":"page"},{"location":"regression/#Regression","page":"Regression","title":"Regression","text":"","category":"section"},{"location":"regression/","page":"Regression","title":"Regression","text":"CurrentModule = ConformalPrediction","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"This tutorial mostly replicates this tutorial from MAPIE.","category":"page"},{"location":"regression/#Data","page":"Regression","title":"Data","text":"","category":"section"},{"location":"regression/","page":"Regression","title":"Regression","text":"We begin by generating some synthetic regression data below:","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"# Regression data:\n\n# Inputs:\nN = 600\nxmax = 3.0\nusing Distributions\nd = Uniform(-xmax, xmax)\nX = rand(d, N)\nX = reshape(X, :, 1)\n\n# Outputs:\nnoise = 0.5\nfun(X) = X * sin(X)\nÎµ = randn(N) .* noise\ny = @.(fun(X)) + Îµ\nusing MLJ\ntrain, test = partition(eachindex(y), 0.4, 0.4, shuffle=true)\n\nusing Plots\nscatter(X, y, label=\"Observed\")\nxrange = range(-xmax,xmax,length=N)\nplot!(xrange, @.(fun(xrange)), lw=4, label=\"Ground truth\", ls=:dash, colour=:black)","category":"page"},{"location":"regression/#Model","page":"Regression","title":"Model","text":"","category":"section"},{"location":"regression/","page":"Regression","title":"Regression","text":"To model this data we will use polynomial regression. There is currently no out-of-the-box support for polynomial feature transformations in MLJ, but it is easy enough to add a little helper function for this. Note how we define a linear pipeline pipe here. Since pipelines in MLJ are just models, we can use the generated object as an input to conformal_model below.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\ndegree_polynomial = 10\npolynomial_features(X, degree::Int) = reduce(hcat, map(i -> X.^i, 1:degree))\npipe = (X -> MLJ.table(polynomial_features(MLJ.matrix(X), degree_polynomial))) |> LinearRegressor()","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"Next, we conformalize our polynomial regressor using every available approach (except the Naive approach):","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"using ConformalPrediction\nconformal_models = merge(values(available_models[:regression])...)\ndelete!(conformal_models, :naive)\n# delete!(conformal_models, :jackknife)\nresults = Dict()\nfor _mod in keys(conformal_models) \n    conf_model = conformal_model(pipe; method=_mod, coverage=0.95)\n    mach = machine(conf_model, X, y)\n    fit!(mach, rows=train)\n    results[_mod] = mach\nend","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"Finally, let us look at the resulting conformal predictions in each case.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"using Plots\nzoom = -3\nxrange = range(-xmax+zoom,xmax-zoom,length=N)\nplt_list = []\n\nfor (_mod, mach) in results\n    plt = plot(mach.model, mach.fitresult, X, y, zoom=zoom, title=_mod)\n    plot!(plt, xrange, @.(fun(xrange)), lw=1, ls=:dash, colour=:black, label=\"Ground truth\")\n    push!(plt_list, plt)\nend\n\nplot(plt_list..., size=(1600,1000))","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"(Image: FigureÂ 1: Conformal prediction regions.)","category":"page"}]
}
